---
title: "aaaAzure Service Fabric 災害復原 |Microsoft 文件"
description: "Azure Service Fabric 提供 hello 功能所需 toodeal 用於所有類型的損毀。 這篇文章描述 hello 類型可能會發生的嚴重損壞情況以及如何與其 toodeal。"
services: service-fabric
documentationcenter: .net
author: masnider
manager: timlt
editor: 
ms.assetid: ab49c4b9-74a8-4907-b75b-8d2ee84c6d90
ms.service: service-fabric
ms.devlang: dotNet
ms.topic: article
ms.tgt_pltfrm: NA
ms.workload: NA
ms.date: 08/18/2017
ms.author: masnider
ms.openlocfilehash: 04b8348fb63e8a1c76a8f722c4c8255b339908e2
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 10/06/2017
---
# <a name="disaster-recovery-in-azure-service-fabric"></a><span data-ttu-id="d3a8b-104">Azure Service Fabric 中的災害復原</span><span class="sxs-lookup"><span data-stu-id="d3a8b-104">Disaster recovery in Azure Service Fabric</span></span>
<span data-ttu-id="d3a8b-105">提供高可用性的關鍵在於確保服務能夠承受所有不同類型的故障。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-105">A critical part of delivering high-availability is ensuring that services can survive all different types of failures.</span></span> <span data-ttu-id="d3a8b-106">這對於預料之外且無法控制的故障而言特別重要。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-106">This is especially important for failures that are unplanned and outside of your control.</span></span> <span data-ttu-id="d3a8b-107">本文說明一些常見的故障模式，如果沒有未正確建立模型和管理，可能會造成嚴重損壞。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-107">This article describes some common failure modes that could be disasters if not modeled and managed correctly.</span></span> <span data-ttu-id="d3a8b-108">它也會探討緩和措施和動作 tootake 如果仍發生損毀。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-108">It also discuss mitigations and actions tootake if a disaster happened anyway.</span></span> <span data-ttu-id="d3a8b-109">hello 目標是 toolimit，或在發生失敗，已規劃或否則會發生時，排除 hello 的停機時間或資料遺失的風險。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-109">hello goal is toolimit or eliminate hello risk of downtime or data loss when they occur failures, planned or otherwise, occur.</span></span>

## <a name="avoiding-disaster"></a><span data-ttu-id="d3a8b-110">避免災害</span><span class="sxs-lookup"><span data-stu-id="d3a8b-110">Avoiding disaster</span></span>
<span data-ttu-id="d3a8b-111">Service Fabric 的主要目標是 toohelp 您的環境和您服務的方式，常見的失敗類型不是您的嚴重損壞時，您建立的模型。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-111">Service Fabric's primary goal is toohelp you model both your environment and your services in such a way that common failure types are not disasters.</span></span> 

<span data-ttu-id="d3a8b-112">一般情況下，災害/故障情節有兩種情況：</span><span class="sxs-lookup"><span data-stu-id="d3a8b-112">In general there are two types of disaster/failure scenarios:</span></span>

1. <span data-ttu-id="d3a8b-113">硬體或軟體錯誤</span><span class="sxs-lookup"><span data-stu-id="d3a8b-113">Hardware or software faults</span></span>
2. <span data-ttu-id="d3a8b-114">操作錯誤</span><span class="sxs-lookup"><span data-stu-id="d3a8b-114">Operational faults</span></span>

### <a name="hardware-and-software-faults"></a><span data-ttu-id="d3a8b-115">硬體和軟體錯誤</span><span class="sxs-lookup"><span data-stu-id="d3a8b-115">Hardware and software faults</span></span>
<span data-ttu-id="d3a8b-116">硬體和軟體錯誤是無法預期的。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-116">Hardware and software faults are unpredictable.</span></span> <span data-ttu-id="d3a8b-117">最簡單方式 toosurvive 錯誤 hello 執行更多份跨越硬體或軟體錯誤界限 hello 服務。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-117">hello easiest way toosurvive faults is running more copies of hello service  spanned across hardware or software fault boundaries.</span></span> <span data-ttu-id="d3a8b-118">比方說，如果只有一個特定的機器上執行您的服務，然後 hello 確定一部電腦失敗是該服務損毀。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-118">For example, if your service is running only on one particular machine, then hello failure of that one machine is a disaster for that service.</span></span> <span data-ttu-id="d3a8b-119">hello 簡單的方式 tooavoid 此嚴重損壞是指 tooensure hello 服務實際執行多部電腦上。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-119">hello simple way tooavoid this disaster is tooensure that hello service is actually running on multiple machines.</span></span> <span data-ttu-id="d3a8b-120">測試是另一部電腦的必要 tooensure hello 失敗不會中斷執行服務的 hello。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-120">Testing is also necessary tooensure hello failure of one machine doesn't disrupt hello running service.</span></span> <span data-ttu-id="d3a8b-121">容量規劃，可確保替代執行個體可由其他位置並減少容量不會多載 hello 剩餘服務。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-121">Capacity planning ensures a replacement instance can be created elsewhere and that reduction in capacity doesn't overload hello remaining services.</span></span> <span data-ttu-id="d3a8b-122">hello 相同的模式運作，不論您正在嘗試的項目 tooavoid hello 失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-122">hello same pattern works regardless of what you're trying tooavoid hello failure of.</span></span> <span data-ttu-id="d3a8b-123">例如，</span><span class="sxs-lookup"><span data-stu-id="d3a8b-123">For example.</span></span> <span data-ttu-id="d3a8b-124">如果您擔心 SAN hello 失敗，您會執行跨多個 San。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-124">if you're concerned about hello failure of a SAN, you run across multiple SANs.</span></span> <span data-ttu-id="d3a8b-125">如果您擔心伺服器機架的 hello 遺失，您會跨多個機架上執行。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-125">If you're concerned about hello loss of a rack of servers, you run across multiple racks.</span></span> <span data-ttu-id="d3a8b-126">如果您擔心 hello 遺失的資料中心，您的服務應執行分散到多個 Azure 區域或資料中心。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-126">If you're worried about hello loss of datacenters, your service should run across multiple Azure regions or datacenters.</span></span> 

<span data-ttu-id="d3a8b-127">這種類型的合併模式中執行時，您還是同時失敗，但單一和多個失敗的特定類型的主體 toosome 類型 (例如： 單一的 VM 或網路連結失敗) 會自動處理 （並因此不會再 「 災害 」）。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-127">When running in this type of spanned mode, you're still subject toosome types of simultaneous failures, but single and even multiple failures of a particular type (ex: a single VM or network link failing) are automatically handled (and so no longer a "disaster").</span></span> <span data-ttu-id="d3a8b-128">Service Fabric 提供許多擴充 hello 叢集，並處理重新將失敗的節點和服務的機制。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-128">Service Fabric provides many mechanisms for expanding hello cluster and handles bringing failed nodes and services back.</span></span> <span data-ttu-id="d3a8b-129">服務網狀架構也可讓您的服務的許多執行個體從執行中順序 tooavoid 這些類型的未預期的失敗開啟到真正的災害。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-129">Service Fabric also allows running many instances of your services in order tooavoid these types of unplanned failures from turning into real disasters.</span></span>

<span data-ttu-id="d3a8b-130">可能有透過失敗執行部署夠大的 toospan 為何不可行的原因。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-130">There may be reasons why running a deployment large enough toospan over failures is not feasible.</span></span> <span data-ttu-id="d3a8b-131">例如，可能需要更多硬體資源比您不願意 toopay 的相對 toohello 有故障的機會。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-131">For example, it may take more hardware resources than you're not willing toopay for relative toohello chance of failure.</span></span> <span data-ttu-id="d3a8b-132">處理分散式應用程式時，可能是跨地理位置距離的其他通訊躍點或狀態複寫成本造成無法接受的延遲。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-132">When dealing with distributed applications, it could be that additional communication hops or state replication costs across geographic distances causes unacceptable latency.</span></span> <span data-ttu-id="d3a8b-133">每個應用程式繪製的此線條有所不同。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-133">Where this line is drawn differs for each application.</span></span> <span data-ttu-id="d3a8b-134">軟體錯誤具體來說，hello 錯誤可能是 hello 服務嘗試 tooscale。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-134">For software faults specifically, hello fault could be in hello service that you are trying tooscale.</span></span> <span data-ttu-id="d3a8b-135">在此情況下更多的複製，不會造成 hello 損毀，因為 hello 失敗狀況會跨所有 hello 執行個體相互關聯。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-135">In this case more copies don't prevent hello disaster, since hello failure condition is correlated across all hello instances.</span></span>

### <a name="operational-faults"></a><span data-ttu-id="d3a8b-136">操作錯誤</span><span class="sxs-lookup"><span data-stu-id="d3a8b-136">Operational faults</span></span>
<span data-ttu-id="d3a8b-137">即使您的服務跨越 hello 地球與許多多餘項目，它仍可能會遇到而言事件。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-137">Even if your service is spanned across hello globe with many redundancies, it can still experience disastrous events.</span></span> <span data-ttu-id="d3a8b-138">例如，如果有人不小心重新設定 hello hello 服務的 dns 名稱或將它完全刪除。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-138">For example, if someone accidentally reconfigures hello dns name for hello service, or deletes it outright.</span></span> <span data-ttu-id="d3a8b-139">舉例來說，假設您的 Service Fabric 具狀態服務，而有人不小心刪除了該服務。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-139">As an example, let's say you had a stateful Service Fabric service, and someone deleted that service accidentally.</span></span> <span data-ttu-id="d3a8b-140">除非有其他風險降低，該服務所有 hello 狀態，請立即消失。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-140">Unless there's some other mitigation, that service and all of hello state it had is now gone.</span></span> <span data-ttu-id="d3a8b-141">這些類型的操作災害 (「糟糕」情況) 恢復所需的緩解和步驟與一般未預期故障不同。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-141">These types of operational disasters ("oops") require different mitigations and steps for recovery than regular unplanned failures.</span></span> 

<span data-ttu-id="d3a8b-142">hello 最佳方式 tooavoid 這些類型的操作錯誤為</span><span class="sxs-lookup"><span data-stu-id="d3a8b-142">hello best ways tooavoid these types of operational faults are to</span></span>
1. <span data-ttu-id="d3a8b-143">限制作業存取 toohello 環境</span><span class="sxs-lookup"><span data-stu-id="d3a8b-143">restrict operational access toohello environment</span></span>
2. <span data-ttu-id="d3a8b-144">嚴格稽核危險的作業</span><span class="sxs-lookup"><span data-stu-id="d3a8b-144">strictly audit dangerous operations</span></span>
3. <span data-ttu-id="d3a8b-145">強制執行自動化，避免手動或移出頻外變更，而且之前制定這些驗證針對 hello 實際環境的特定變更</span><span class="sxs-lookup"><span data-stu-id="d3a8b-145">impose automation, prevent manual or out of band changes, and validate specific changes against hello actual environment before enacting them</span></span>
4. <span data-ttu-id="d3a8b-146">確認破壞性操作是否為「軟性」。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-146">ensure that destructive operations are "soft".</span></span> <span data-ttu-id="d3a8b-147">軟性操作不會立即生效，或是可以在一段時間內復原</span><span class="sxs-lookup"><span data-stu-id="d3a8b-147">Soft operations don't take effect immediately or can be undone within some time window</span></span>

<span data-ttu-id="d3a8b-148">服務網狀架構會提供一些機制 tooprevent 操作錯誤，例如提供[以角色為基礎](service-fabric-cluster-security-roles.md)存取控制針對叢集操作。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-148">Service Fabric provides some mechanisms tooprevent operational faults, such as providing [role-based](service-fabric-cluster-security-roles.md) access control for cluster operations.</span></span> <span data-ttu-id="d3a8b-149">不過，大部分的操作錯誤需要投入組織化的心力以及其他系統。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-149">However, most of these operational faults require organizational efforts and other systems.</span></span> <span data-ttu-id="d3a8b-150">Service Fabric 提供一些機制以因應操作錯誤，最值得注意的是具狀態服務的備份與還原。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-150">Service Fabric does provide some mechanism for surviving operational faults, most notably backup and restore for stateful services.</span></span>

## <a name="managing-failures"></a><span data-ttu-id="d3a8b-151">管理故障</span><span class="sxs-lookup"><span data-stu-id="d3a8b-151">Managing failures</span></span>
<span data-ttu-id="d3a8b-152">Service Fabric hello 目標幾乎都是自動管理的失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-152">hello goal of Service Fabric is almost always automatic management of failures.</span></span> <span data-ttu-id="d3a8b-153">不過，若要 toohandle 某些類型的錯誤，服務必須有額外的程式碼。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-153">However, in order toohandle some types of failures, services must have additional code.</span></span> <span data-ttu-id="d3a8b-154">因為安全和營運永續性的考量，_不_應該自動解決其他類型的故障。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-154">Other types of failures should _not_ be automatically addressed because of safety and business continuity reasons.</span></span> 

### <a name="handling-single-failures"></a><span data-ttu-id="d3a8b-155">處理單一故障</span><span class="sxs-lookup"><span data-stu-id="d3a8b-155">Handling single failures</span></span>
<span data-ttu-id="d3a8b-156">單一機器可能鑒於各種原因而故障。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-156">Single machines can fail for all sorts of reasons.</span></span> <span data-ttu-id="d3a8b-157">其中一些是硬體因素，例如電源供應器和網路硬體故障。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-157">Some of these are hardware causes, like power supplies and networking hardware failures.</span></span> <span data-ttu-id="d3a8b-158">其他是軟體故障，</span><span class="sxs-lookup"><span data-stu-id="d3a8b-158">Other failures are in software.</span></span> <span data-ttu-id="d3a8b-159">這些包含失敗的實際作業系統 hello 和 hello 服務本身。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-159">These include failures of hello actual operating system and hello service itself.</span></span> <span data-ttu-id="d3a8b-160">服務網狀架構會自動偵測這些失敗類型，包括萬一 hello 電腦無法與其他機器因為 toonetwork 問題隔離。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-160">Service Fabric automatically detects these types of failures, including cases where hello machine becomes isolated from other machines due toonetwork issues.</span></span>

<span data-ttu-id="d3a8b-161">Hello 不論類型為何的服務，執行單一執行個體產生該服務的停機時間如果因為任何原因失敗 hello 程式碼的單一複本。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-161">Regardless of hello type of service, running a single instance results in downtime for that service if that single copy of hello code fails for any reason.</span></span> 

<span data-ttu-id="d3a8b-162">在順序 toohandle 任何單一失敗，hello 最簡單的事，您可以是您的服務在一個以上的節點執行預設的 tooensure。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-162">In order toohandle any single failure, hello simplest thing you can do is tooensure that your services run on more than one node by default.</span></span> <span data-ttu-id="d3a8b-163">針對無狀態服務，這可藉由讓 `InstanceCount` 大於 1 來實現。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-163">For stateless services, this can be accomplished by having an `InstanceCount` greater than 1.</span></span> <span data-ttu-id="d3a8b-164">可設定狀態服務，hello 最小建議的做法是永遠`TargetReplicaSetSize`和`MinReplicaSetSize`為至少 3。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-164">For stateful services, hello minimum recommendation is always a `TargetReplicaSetSize` and `MinReplicaSetSize` of at least 3.</span></span> <span data-ttu-id="d3a8b-165">執行服務程式碼的多個複本可確保您的服務可自動處理任何單一失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-165">Running more copies of your service code ensures that your service can handle any single failure automatically.</span></span> 

### <a name="handling-coordinated-failures"></a><span data-ttu-id="d3a8b-166">處理協調失敗</span><span class="sxs-lookup"><span data-stu-id="d3a8b-166">Handling coordinated failures</span></span>
<span data-ttu-id="d3a8b-167">協調的失敗可能會發生因為 tooeither 計劃或非計劃的基礎結構失敗和變更，或是已規劃的軟體變更叢集中。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-167">Coordinated failures can happen in a cluster due tooeither planned or unplanned infrastructure failures and changes, or planned software changes.</span></span> <span data-ttu-id="d3a8b-168">Service Fabric 會將發生協調失敗的基礎結構區域建立模型，做為「容錯網域」。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-168">Service Fabric models infrastructure zones that experience coordinated failures as Fault Domains.</span></span> <span data-ttu-id="d3a8b-169">發生協調軟體變更的區域會建立模型做為「升級網域」。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-169">Areas that will experience coordinated software changes are modeled as Upgrade Domains.</span></span> <span data-ttu-id="d3a8b-170">關於容錯網域和升級網域的更多資訊，請參閱[此文件](service-fabric-cluster-resource-manager-cluster-description.md)，文中會說明叢集的拓撲和定義。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-170">More information about fault and upgrade domains is in [this document](service-fabric-cluster-resource-manager-cluster-description.md) that describes cluster topology and definition.</span></span>

<span data-ttu-id="d3a8b-171">在規劃應在何處執行服務時，Service Fabric 依預設會考慮容錯網域和升級網域。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-171">By default Service Fabric considers fault and upgrade domains when planning where your services should run.</span></span> <span data-ttu-id="d3a8b-172">根據預設，Service Fabric 會嘗試 tooensure 規劃或未規劃的變更，就可能發生，如果您的服務仍可讓您的服務執行跨數個容錯網域和升級網域。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-172">By default, Service Fabric tries tooensure that your services run across several fault and upgrade domains so if planned or unplanned changes happen your services remain available.</span></span> 

<span data-ttu-id="d3a8b-173">例如，假設失敗的電力來源，同時會造成機器 toofail 機架。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-173">For example, let's say that failure of a power source causes a rack of machines toofail simultaneously.</span></span> <span data-ttu-id="d3a8b-174">使用中容錯網域失敗時所執行的許多機器 hello 遺失的 hello 服務的多個複本會轉換成只是另一個範例中的指定服務的單一失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-174">With multiple copies of hello service running hello loss of many machines in fault domain failure turns into just another example of single failure for a given service.</span></span> <span data-ttu-id="d3a8b-175">這就是為什麼管理的容錯網域是重大 tooensuring 高可用性，您的服務。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-175">This is why managing fault domains is critical tooensuring high availability of your services.</span></span> <span data-ttu-id="d3a8b-176">在 Azure 中執行 Service Fabric 時，會自動管理容錯網域。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-176">When running Service Fabric in Azure, fault domains are managed automatically.</span></span> <span data-ttu-id="d3a8b-177">在其他環境中可能不會自動管理。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-177">In other environments they may not be.</span></span> <span data-ttu-id="d3a8b-178">如果您建置在內部部署叢集，可確定 toomap 並正確規劃您的容錯網域版面配置。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-178">If you're building your own clusters on premises, be sure toomap and plan your fault domain layout correctly.</span></span>

<span data-ttu-id="d3a8b-179">升級網域可用於模型化軟體正在發生 toobe 升級 hello 相同區域的時間。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-179">Upgrade Domains are useful for modeling areas where software is going toobe upgraded at hello same time.</span></span> <span data-ttu-id="d3a8b-180">因為這個緣故，升級 」 網域通常也會定義其中計劃升級期間停機軟體 hello 界限。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-180">Because of this, Upgrade Domains also often define hello boundaries where software is taken down during planned upgrades.</span></span> <span data-ttu-id="d3a8b-181">服務網狀架構和您的服務升級遵循 hello 相同的模型。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-181">Upgrades of both Service Fabric and your services follow hello same model.</span></span> <span data-ttu-id="d3a8b-182">如需輪流升級，升級的網域和 hello 服務網狀架構健全狀況模型，可協助避免非預期的變更影響 hello 叢集與您的服務詳細資訊，請參閱這些文件：</span><span class="sxs-lookup"><span data-stu-id="d3a8b-182">For more on rolling upgrades, upgrade domains, and hello Service Fabric health model that helps prevent unintended changes from impacting hello cluster and your service, see these documents:</span></span>

 - [<span data-ttu-id="d3a8b-183">應用程式升級</span><span class="sxs-lookup"><span data-stu-id="d3a8b-183">Application Upgrade</span></span>](service-fabric-application-upgrade.md)
 - [<span data-ttu-id="d3a8b-184">應用程式升級教學課程</span><span class="sxs-lookup"><span data-stu-id="d3a8b-184">Application Upgrade Tutorial</span></span>](service-fabric-application-upgrade-tutorial.md)
 - [<span data-ttu-id="d3a8b-185">Service Fabric 健康情況模型</span><span class="sxs-lookup"><span data-stu-id="d3a8b-185">Service Fabric Health Model</span></span>](service-fabric-health-introduction.md)

<span data-ttu-id="d3a8b-186">您可以視覺化 hello 版面配置使用 hello 叢集對應中提供您叢集的[Service Fabric 總管](service-fabric-visualizing-your-cluster.md):</span><span class="sxs-lookup"><span data-stu-id="d3a8b-186">You can visualize hello layout of your cluster using hello cluster map provided in [Service Fabric Explorer](service-fabric-visualizing-your-cluster.md):</span></span>

<span data-ttu-id="d3a8b-187"><center>
![節點散佈於 Service Fabric Explorer 中的各個容錯網域][sfx-cluster-map]
</center></span><span class="sxs-lookup"><span data-stu-id="d3a8b-187"><center>
![Nodes spread across fault domains in Service Fabric Explorer][sfx-cluster-map]
</center></span></span>

> [!NOTE]
> <span data-ttu-id="d3a8b-188">模型失敗，輪流升級，執行您的服務程式碼和狀態的許多執行個體的區域放置規則 tooensure 跨容錯和升級網域，執行您的服務和內建的健全狀況監視只是**某些**的 helloService Fabric 順序 tookeep 正常操作問題並開啟到的嚴重損壞時從失敗中所提供的功能。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-188">Modeling areas of failure, rolling upgrades, running many instances of your service code and state, placement rules tooensure your services run across fault and upgrade domains, and built-in health monitoring are just **some** of hello features that Service Fabric provides in order tookeep normal operational issues and failures from turning into disasters.</span></span> 
>

### <a name="handling-simultaneous-hardware-or-software-failures"></a><span data-ttu-id="d3a8b-189">處理同時發生的硬體或軟體故障</span><span class="sxs-lookup"><span data-stu-id="d3a8b-189">Handling simultaneous hardware or software failures</span></span>
<span data-ttu-id="d3a8b-190">上述探討的是單一故障。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-190">Above we talked about single failures.</span></span> <span data-ttu-id="d3a8b-191">如您所見，是無狀態與可設定狀態服務的簡單 toohandle 只會保留更多份 hello 程式碼 （和狀態） 跨過容錯和升級網域。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-191">As you can see, are easy toohandle for both stateless and stateful services just by keeping more copies of hello code (and state) running across fault and upgrade domains.</span></span> <span data-ttu-id="d3a8b-192">也可能發生多個同時的隨機失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-192">Multiple simultaneous random failures can also happen.</span></span> <span data-ttu-id="d3a8b-193">這些是更 toolead tooan 實際嚴重損壞。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-193">These are more likely toolead tooan actual disaster.</span></span>


### <a name="random-failures-leading-tooservice-failures"></a><span data-ttu-id="d3a8b-194">隨機失敗導致 tooservice 失敗</span><span class="sxs-lookup"><span data-stu-id="d3a8b-194">Random failures leading tooservice failures</span></span>
<span data-ttu-id="d3a8b-195">假設 hello 服務有`InstanceCount`5 和數個節點執行這些執行個體的所有失敗 hello 在相同的時間。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-195">Let's say that hello service had an `InstanceCount` of 5, and several nodes running those instances all failed at hello same time.</span></span> <span data-ttu-id="d3a8b-196">Service Fabric 會藉由在其他節點上自動建立取代執行個體來回應。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-196">Service Fabric responds by automatically creating replacement instances on other nodes.</span></span> <span data-ttu-id="d3a8b-197">它會繼續建立取代項目，直到 hello 服務回 tooits 預期執行個體計數。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-197">It will continue creating replacements until hello service is back tooits desired instance count.</span></span> <span data-ttu-id="d3a8b-198">另舉一例，假設有無狀態服務與`InstanceCount`-1，這表示在 hello 叢集中的所有有效的節點上執行。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-198">As another example, let's say there was a stateless service with an `InstanceCount`of -1, meaning it runs on all valid nodes in hello cluster.</span></span> <span data-ttu-id="d3a8b-199">比方說有些這些執行個體是 toofail。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-199">Let's say that some of those instances were toofail.</span></span> <span data-ttu-id="d3a8b-200">在此情況下，Service Fabric 通知 hello 服務不在其所需的狀態，而且會嘗試 toocreate hello 執行個體，但已遺失的 hello 節點上。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-200">In this case, Service Fabric notices that hello service is not in its desired state, and tries toocreate hello instances on hello nodes where they are missing.</span></span> 

<span data-ttu-id="d3a8b-201">可設定狀態服務的 hello 情況取決於是否 hello 服務已保存狀態或不。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-201">For stateful services hello situation depends on whether hello service has persisted state or not.</span></span> <span data-ttu-id="d3a8b-202">它也取決於有多少複本 hello 服務和多少失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-202">It also depends on how many replicas hello service had and how many failed.</span></span> <span data-ttu-id="d3a8b-203">判斷具狀態服務是否發生災害，並進行管理，如下三個階段：</span><span class="sxs-lookup"><span data-stu-id="d3a8b-203">Determining whether a disaster occurred for a stateful service and managing it follows three stages:</span></span>

1. <span data-ttu-id="d3a8b-204">判斷是否有仲裁遺失</span><span class="sxs-lookup"><span data-stu-id="d3a8b-204">Determining if there has been quorum loss or not</span></span>
 - <span data-ttu-id="d3a8b-205">仲裁遺失是可設定狀態服務的 hello 複本的大部分都可以向下 hello 任何時間相同的時間，包括主要 hello。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-205">A quorum loss is any time a majority of hello replicas of a stateful service are down at hello same time, including hello Primary.</span></span>
2. <span data-ttu-id="d3a8b-206">判斷 hello 仲裁遺失是否為永久</span><span class="sxs-lookup"><span data-stu-id="d3a8b-206">Determining if hello quorum loss is permanent or not</span></span>
 - <span data-ttu-id="d3a8b-207">大部分的 hello 時間是暫時性失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-207">Most of hello time, failures are transient.</span></span> <span data-ttu-id="d3a8b-208">處理程序會重新啟動，節點會重新啟動，VM 會重新啟動，網路磁碟分割會修復。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-208">Processes are restarted, nodes are restarted, VMs are relaunched, network partitions heal.</span></span> <span data-ttu-id="d3a8b-209">不過有時候失敗是永久性的。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-209">Sometimes though, failures are permanent.</span></span> 
    - <span data-ttu-id="d3a8b-210">對於非持續狀態的服務，單一仲裁或多個複本的失敗會_立即_導致永久性的仲裁遺失。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-210">For services without persisted state, a failure of a quorum or more of replicas results _immediately_ in permanent quorum loss.</span></span> <span data-ttu-id="d3a8b-211">當 Service Fabric 偵測到遺失仲裁，可設定狀態的非持續性服務中時，便會立即繼續 dataloss toostep 3 藉由宣告 （可能）。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-211">When Service Fabric detects quorum loss in a stateful non-persistent service, it immediately proceeds toostep 3 by declaring (potential) dataloss.</span></span> <span data-ttu-id="d3a8b-212">因為 Service Fabric 知道沒有點中等候 hello 複本 toocome 後，即使它們都已復原它們會是空白，再繼續 toodataloss 才有意義。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-212">Proceeding toodataloss makes sense because Service Fabric knows that there's no point in waiting for hello replicas toocome back, because even if they were recovered they would be empty.</span></span>
    - <span data-ttu-id="d3a8b-213">對於可設定狀態的持續性服務，仲裁或多個複本的失敗會造成 Service Fabric toostart 等候 hello 複本 toocome 備份和還原仲裁。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-213">For stateful persistent services, a failure of a quorum or more of replicas causes Service Fabric toostart waiting for hello replicas toocome back and restore quorum.</span></span> <span data-ttu-id="d3a8b-214">這會導致服務中斷的任何_寫入_toohello 影響 hello 服務磁碟分割 （或 「 複本集 」）。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-214">This results in a service outage for any _writes_ toohello affected partition (or "replica set") of hello service.</span></span> <span data-ttu-id="d3a8b-215">不過，仍可能在降低一致性保證的情況下進行讀取。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-215">However, reads may still be possible with reduced consistency guarantees.</span></span> <span data-ttu-id="d3a8b-216">hello 預設的 Service Fabric 等候仲裁 toobe 還原量是時間的無限的因為繼續 （可能） 的 dataloss 事件，而且附帶其他風險。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-216">hello default amount of time that Service Fabric waits for quorum toobe restored is infinite, since proceeding is a (potential) dataloss event and carries other risks.</span></span> <span data-ttu-id="d3a8b-217">覆寫 hello 預設`QuorumLossWaitDuration`值可能會但不是建議使用。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-217">Overriding hello default `QuorumLossWaitDuration` value is possible but is not recommended.</span></span> <span data-ttu-id="d3a8b-218">改為在此階段中，所有投入時間應該會向複本 toorestore hello。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-218">Instead at this time, all efforts should be made toorestore hello down replicas.</span></span> <span data-ttu-id="d3a8b-219">這需要攜帶 hello 的節點下一步，並確保它們可以重新掛接 hello 它們用來儲存 hello 本機永續性狀態的磁碟機。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-219">This requires bringing hello nodes that are down back up, and ensuring that they can remount hello drives where they stored hello local persistent state.</span></span> <span data-ttu-id="d3a8b-220">如果 hello 仲裁遺失因為處理序失敗，Service Fabric 自動嘗試 toorecreate hello 處理程序，並重新啟動那些 hello 複本。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-220">If hello quorum loss is caused by process failure, Service Fabric automatically tries toorecreate hello processes and restart hello replicas inside them.</span></span> <span data-ttu-id="d3a8b-221">如果此作業失敗，Service Fabric 會報告健康情況錯誤。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-221">If this fails, Service Fabric reports health errors.</span></span> <span data-ttu-id="d3a8b-222">如果這些都無法解析然後 hello 複本通常回來。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-222">If these can be resolved then hello replicas usually come back.</span></span> <span data-ttu-id="d3a8b-223">有時候，不過，hello 複本將無法處於回。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-223">Sometimes, though, hello replicas can't be brought back.</span></span> <span data-ttu-id="d3a8b-224">比方說，hello 磁碟機可能所有已失敗，或由於某種原因而 hello 機器實際終結。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-224">For example, hello drives may all have failed, or hello machines physically destroyed somehow.</span></span> <span data-ttu-id="d3a8b-225">在這些情況下，我們現在有一個永久性的仲裁遺失事件。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-225">In these cases, we now have a permanent quorum loss event.</span></span> <span data-ttu-id="d3a8b-226">等候複本 toocome，叢集系統管理員向 hello tootell Service Fabric toostop 必須判斷哪些資料分割，其中，服務會受到影響，呼叫 hello`Repair-ServiceFabricPartition -PartitionId`或` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)`應用程式開發介面。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-226">tootell Service Fabric toostop waiting for hello down replicas toocome back, a cluster administrator must determine which partitions of which services are affected and call hello `Repair-ServiceFabricPartition -PartitionId` or ` System.Fabric.FabricClient.ClusterManagementClient.RecoverPartitionAsync(Guid partitionId)` API.</span></span>  <span data-ttu-id="d3a8b-227">此 API 可讓您指定的從 QuorumLoss 移至潛在 dataloss hello 分割 toomove hello ID。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-227">This API allows specifying hello ID of hello partition toomove out of QuorumLoss and into potential dataloss.</span></span>

> [!NOTE]
> <span data-ttu-id="d3a8b-228">它是_從未_安全 toouse 此應用程式開發介面以外的目標方式，針對特定的資料分割中。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-228">It is _never_ safe toouse this API other than in a targeted way against specific partitions.</span></span> 
>

3. <span data-ttu-id="d3a8b-229">判斷是否已有實際資料遺失，並從備份還原</span><span class="sxs-lookup"><span data-stu-id="d3a8b-229">Determining if there has been actual data loss, and restoring from backups</span></span>
  - <span data-ttu-id="d3a8b-230">當 Service Fabric 呼叫 hello`OnDataLossAsync`方法一律會因為_懷疑_dataloss。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-230">When Service Fabric calls hello `OnDataLossAsync` method it is always because of _suspected_ dataloss.</span></span> <span data-ttu-id="d3a8b-231">服務網狀架構，可確保此呼叫會傳遞 toohello_最佳_其餘複本。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-231">Service Fabric ensures that this call is delivered toohello _best_ remaining replica.</span></span> <span data-ttu-id="d3a8b-232">這是不論哪一個複本已進行 hello 大部分的進度。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-232">This is whichever replica has made hello most progress.</span></span> <span data-ttu-id="d3a8b-233">hello 的原因，我們一律說_懷疑_dataloss 是，它可能該 hello 其餘複本確實具有相同的所有狀態 hello 主要它已關閉時一樣。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-233">hello reason we always say _suspected_ dataloss is that it is possible that hello remaining replica actually has all same state as hello Primary did when it went down.</span></span> <span data-ttu-id="d3a8b-234">不過，如果該狀態 toocompare 沒有它，就不好的 Service Fabric 或運算子 tooknow 供確認。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-234">However, without that state toocompare it to, there's no good way for Service Fabric or operators tooknow for sure.</span></span> <span data-ttu-id="d3a8b-235">此時，Service Fabric 也知道 hello 其他複本無法返回。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-235">At this point, Service Fabric also knows hello other replicas are not coming back.</span></span> <span data-ttu-id="d3a8b-236">這是我們停止等待 hello 仲裁遺失 tooresolve 本身時所做的 hello 決策。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-236">That was hello decision made when we stopped waiting for hello quorum loss tooresolve itself.</span></span> <span data-ttu-id="d3a8b-237">hello 最佳的 hello 服務動作通常是 toofreeze，等候特定的系統管理員操作。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-237">hello best course of action for hello service is usually toofreeze and wait for specific administrative intervention.</span></span> <span data-ttu-id="d3a8b-238">項目也會跟著 hello 的一般實作`OnDataLossAsync`方法？</span><span class="sxs-lookup"><span data-stu-id="d3a8b-238">So what does a typical implementation of hello `OnDataLossAsync` method do?</span></span>
  - <span data-ttu-id="d3a8b-239">首先，記錄 `OnDataLossAsync` 已觸發，並關閉任何必要的管理警示。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-239">First, log that `OnDataLossAsync` has been triggered, and fire off any necessary administrative alerts.</span></span>
   - <span data-ttu-id="d3a8b-240">通常這個時候 toopause，並等待進一步決策及採取手動動作 toobe。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-240">Usually at this point, toopause and wait for further decisions and manual actions toobe taken.</span></span> <span data-ttu-id="d3a8b-241">這是因為即使備份可供使用，它們就可能需要 toobe 備妥。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-241">This is because even if backups are available they may need toobe prepared.</span></span> <span data-ttu-id="d3a8b-242">例如，如果兩個不同的服務會協調資訊，這些備份可能需要 toobe 一旦 hello 還原發生的 hello 資訊的兩個服務關心是一致的順序 tooensure 中修改。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-242">For example, if two different services coordinate information, those backups may need toobe modified in order tooensure that once hello restore happens that hello information those two services care about is consistent.</span></span> 
  - <span data-ttu-id="d3a8b-243">通常另外還有一些其他遙測或排氣從 hello 服務。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-243">Often there is also some other telemetry or exhaust from hello service.</span></span> <span data-ttu-id="d3a8b-244">此中繼資料可能包含在其他服務或記錄中。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-244">This metadata may be contained in other services or in logs.</span></span> <span data-ttu-id="d3a8b-245">這項資訊可以使用所需的 toodetermine hello 備份或複寫 toothis 特定複本中沒有任何呼叫接收及處理在主要 hello 一樣。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-245">This information can be used needed toodetermine if there were any calls received and processed at hello primary that were not present in hello backup or replicated toothis particular replica.</span></span> <span data-ttu-id="d3a8b-246">這些可能需要 toobe 重新執行或者新增 toohello 備份還原才可行。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-246">These may need toobe replayed or added toohello backup before restoration is feasible.</span></span>  
   - <span data-ttu-id="d3a8b-247">Hello 剩餘複本的狀態 toothat 任何可用的備份中包含的比較。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-247">Comparisons of hello remaining replica's state toothat contained in any backups that are available.</span></span> <span data-ttu-id="d3a8b-248">如果使用 hello Service Fabric 可靠集合，則表示工具，處理供執行這項作業中所述[本文](service-fabric-reliable-services-backup-restore.md)。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-248">If using hello Service Fabric reliable collections then there are tools and processes available for doing so, described in [this article](service-fabric-reliable-services-backup-restore.md).</span></span> <span data-ttu-id="d3a8b-249">hello 目標時 toosee hello 複本中的 hello 狀態即已足夠，或也 hello 備份可能已遺失。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-249">hello goal is toosee if hello state within hello replica is sufficient, or also what hello backup may be missing.</span></span>
  - <span data-ttu-id="d3a8b-250">一次 hello 而做比較，如果完成必要的 hello 還原，hello 服務程式碼應該會傳回有任何狀態變更時，則為 true。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-250">Once hello comparison is done, and if necessary hello restore completed, hello service code should return true if any state changes were made.</span></span> <span data-ttu-id="d3a8b-251">如果它已 hello 最佳可用的複本 hello 狀態，並不會變更，判斷出 hello 複本，則傳回 false。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-251">If hello replica determined that it was hello best available copy of hello state and made no changes, then return false.</span></span> <span data-ttu-id="d3a8b-252">True 表示任何_其他_剩餘複本現在可能與此複本不一致。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-252">True indicates that any _other_ remaining replicas may now be inconsistent with this one.</span></span> <span data-ttu-id="d3a8b-253">將會卸除這些複本，並從此複本重建。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-253">They will be dropped and rebuilt from this replica.</span></span> <span data-ttu-id="d3a8b-254">False 表示未進行任何狀態變更，因此 hello 其他複本可以保留它們的有。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-254">False indicates that no state changes were made, so hello other replicas can keep what they have.</span></span> 

<span data-ttu-id="d3a8b-255">在生產中部署服務之前，服務製作者在實踐潛在的資料遺失和失敗案例至關重要。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-255">It is critically important that service authors practice potential dataloss and failure scenarios before services are ever deployed in production.</span></span> <span data-ttu-id="d3a8b-256">針對 dataloss hello 可能性 tooprotect，務必 tooperiodically[備份 hello 狀態](service-fabric-reliable-services-backup-restore.md)任何您可設定狀態服務 tooa 異地備援存放區。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-256">tooprotect against hello possibility of dataloss, it is important tooperiodically [back up hello state](service-fabric-reliable-services-backup-restore.md) of any of your stateful services tooa geo-redundant store.</span></span> <span data-ttu-id="d3a8b-257">您也必須確定您擁有 hello 能力 toorestore 它。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-257">You must also ensure that you have hello ability toorestore it.</span></span> <span data-ttu-id="d3a8b-258">因為許多不同的服務備份在不同的時間，您需要在還原後您的服務具有一致的檢視彼此的 tooensure。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-258">Since backups of many different services are taken at different times, you need tooensure that after a restore your services have a consistent view of each other.</span></span> <span data-ttu-id="d3a8b-259">例如，假設一項服務產生數字和加以儲存，然後將它傳送 tooanother 服務，也會儲存的位置。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-259">For example, consider a situation where one service generates a number and stores it, then sends it tooanother service that also stores it.</span></span> <span data-ttu-id="d3a8b-260">還原之後，您可能會發現 hello 第二個服務都有 hello 數字，但 hello 第一次不存在，因為它的備份不包含該作業。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-260">After a restore, you might discover that hello second service has hello number but hello first does not, because it's backup didn't include that operation.</span></span>

<span data-ttu-id="d3a8b-261">如果您發現該 hello 剩餘複本不足 toocontinue 從在 dataloss 案例中，而且您無法建構從遙測或排氣服務狀態，您的備份的 hello 頻率會決定您最佳的可能復原點目標 (RPO).</span><span class="sxs-lookup"><span data-stu-id="d3a8b-261">If you find out that hello remaining replicas are insufficient toocontinue from in a dataloss scenario, and you can't reconstruct service state from telemetry or exhaust, hello frequency of your backups determines your best possible recovery point objective (RPO).</span></span> <span data-ttu-id="d3a8b-262">Service Fabric 會提供許多工具，以測試各種失敗情況，包括需要從備份還原的永久仲裁和資料遺失。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-262">Service Fabric provides many tools for testing various failure scenarios, including permanent quorum and dataloss requiring restoration from a backup.</span></span> <span data-ttu-id="d3a8b-263">這些案例會包含 Service Fabric 可測試性工具，受 hello 錯誤 Analysis Service 的一部分。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-263">These scenarios are included as a part of Service Fabric's testability tools, managed by hello Fault Analysis Service.</span></span> <span data-ttu-id="d3a8b-264">這些工具和模式的詳細資訊，請見[這裡](service-fabric-testability-overview.md)。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-264">More info on those tools and patterns is available [here](service-fabric-testability-overview.md).</span></span> 

> [!NOTE]
> <span data-ttu-id="d3a8b-265">系統服務也可能會遺失仲裁、 與 hello 影響因素是特定 toohello 服務有問題。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-265">System services can also suffer quorum loss, with hello impact being specific toohello service in question.</span></span> <span data-ttu-id="d3a8b-266">比方說，hello 命名服務中的仲裁遺失而仲裁遺失中 hello 容錯移轉管理員服務會封鎖新的服務建立和容錯移轉會影響名稱解析。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-266">For instance, quorum loss in hello naming service impacts name resolution, whereas quorum loss in hello failover manager service blocks new service creation and failovers.</span></span> <span data-ttu-id="d3a8b-267">雖然 hello Service Fabric 系統服務會依照相同模式為您進行狀態管理服務的 hello，但不建議您應該嘗試 toomove 從仲裁遺失移至潛在 dataloss 它們。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-267">While hello Service Fabric system services follow hello same pattern as your services for state management, it is not recommended that you should attempt toomove them out of Quorum Loss and into potential dataloss.</span></span> <span data-ttu-id="d3a8b-268">hello 建議改用太[尋求支援](service-fabric-support.md)toodetermine 方案，目標 tooyour 特定情況。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-268">hello recommendation is instead too[seek support](service-fabric-support.md) toodetermine a solution that is targeted tooyour specific situation.</span></span>  <span data-ttu-id="d3a8b-269">通常它是比 toosimply 等候直到向複本傳回 hello。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-269">Usually it is preferable toosimply wait until hello down replicas return.</span></span>
>

## <a name="availability-of-hello-service-fabric-cluster"></a><span data-ttu-id="d3a8b-270">Hello Service Fabric 叢集的可用性</span><span class="sxs-lookup"><span data-stu-id="d3a8b-270">Availability of hello Service Fabric cluster</span></span>
<span data-ttu-id="d3a8b-271">一般而言，hello Service Fabric 叢集本身是任何單一失敗點的高度分散的環境。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-271">Generally speaking, hello Service Fabric cluster itself is a highly distributed environment with no single points of failure.</span></span> <span data-ttu-id="d3a8b-272">任何一個節點的失敗不會導致可用性或可靠性問題 hello 叢集，主要是因為 hello Service Fabric 系統服務會遵循相同的指導方針之前所提供的 hello： 它們永遠都可執行三個或多個複本的預設值，以及系統服務是無狀態的所有節點上執行。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-272">A failure of any one node will not cause availability or reliability issues for hello cluster, primarily because hello Service Fabric system services follow hello same guidelines provided earlier: they always run with three or more replicas by default, and those system services that are stateless run on all nodes.</span></span> <span data-ttu-id="d3a8b-273">完整散發 hello 基礎 Service Fabric 網路功能與失敗偵測層級。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-273">hello underlying Service Fabric networking and failure detection layers are fully distributed.</span></span> <span data-ttu-id="d3a8b-274">大部分的系統服務從 hello 叢集中的中繼資料就可以重建，或者知道如何 tooresynchronize 其狀態的其他位置。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-274">Most system services can be rebuilt from metadata in hello cluster, or know how tooresynchronize their state from other places.</span></span> <span data-ttu-id="d3a8b-275">如果系統服務會進入仲裁遺失情況說明上述 hello hello 叢集可用性可能會變成洩露出去。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-275">hello availability of hello cluster can become compromised if system services get into quorum loss situations like those described above.</span></span> <span data-ttu-id="d3a8b-276">在這些情況下您可能不是能 tooperform 特定 hello 叢集上的作業，例如開始升級或部署新服務，但 hello 叢集本身仍在運作。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-276">In these cases you may not be able tooperform certain operations on hello cluster like starting an upgrade or deploying new services, but hello cluster itself is still up.</span></span> <span data-ttu-id="d3a8b-277">已在執行上的服務會保留在這些情況中執行，除非它們需要寫入 toohello 系統服務 toocontinue 運作。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-277">Services on already running will remain running in these conditions unless they require writes toohello system services toocontinue functioning.</span></span> <span data-ttu-id="d3a8b-278">比方說，如果 hello 容錯移轉管理員是在仲裁遺失所有的服務將繼續 toorun，但失敗的任何服務將無法 tooautomatically 無法重新啟動，因為它會要求 hello 參與 hello 容錯移轉管理員。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-278">For example, if hello Failover Manager is in quorum loss all services will continue toorun, but any services that fail will not be able tooautomatically restart, since this requires hello involvement of hello Failover Manager.</span></span> 

### <a name="failures-of-a-datacenter-or-azure-region"></a><span data-ttu-id="d3a8b-279">資料中心或 Azure 區域的失敗</span><span class="sxs-lookup"><span data-stu-id="d3a8b-279">Failures of a datacenter or Azure region</span></span>
<span data-ttu-id="d3a8b-280">在少數情況下，實體資料中心可能會因為暫時無法使用 tooloss 電源或網路連線能力。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-280">In rare cases, a physical data center can become temporarily unavailable due tooloss of power or network connectivity.</span></span> <span data-ttu-id="d3a8b-281">在這些情況下，在資料中心或 Azure 區域中的 Service Fabric 叢集和服務將無法使用。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-281">In these cases, your Service Fabric clusters and services in that datacenter or Azure region will be unavailable.</span></span> <span data-ttu-id="d3a8b-282">不過，_您的資料會保留下來_。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-282">However, _your data is preserved_.</span></span> <span data-ttu-id="d3a8b-283">在 Azure 中執行的叢集，您可以檢視更新上關閉 hello [Azure 狀態 頁面][azure-status-dashboard]。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-283">For clusters running in Azure, you can view updates on outages on hello [Azure status page][azure-status-dashboard].</span></span> <span data-ttu-id="d3a8b-284">在 hello 終結不太可能在實體資料中心是部分或完整的事件，那里裝載任何 Service Fabric 叢集或其中 hello 服務可能會遺失。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-284">In hello highly unlikely event that a physical data center is partially or fully destroyed, any Service Fabric clusters hosted there or hello services inside them could be lost.</span></span> <span data-ttu-id="d3a8b-285">這包括在該資料中心或區域外部未備份的任何狀態。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-285">This includes any state not backed up outside of that datacenter or region.</span></span>

<span data-ttu-id="d3a8b-286">沒有兩個不同的策略來存活 hello 的單一資料中心或地區的永久或持續性失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-286">There's two different strategies for surviving hello permanent or sustained failure of a single datacenter or region.</span></span> 

1. <span data-ttu-id="d3a8b-287">在多個此類區域中，執行不同的 Service Fabric 叢集，並在這些環境之間利用容錯移轉和容錯回復的一些機制。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-287">Run separate Service Fabric clusters in multiple such regions, and utilize some mechanism for failover and fail-back between these environments.</span></span> <span data-ttu-id="d3a8b-288">這種多叢集的主動至主動或主動至被動模型需要其他管理和作業碼。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-288">This sort of multi-cluster active-active or active-passive model requires additional management and operations code.</span></span> <span data-ttu-id="d3a8b-289">這也需要的備份，從一個資料中心或區域中的 hello 服務協調，讓它們可用於其他資料中心或區域時失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-289">This also requires coordination of backups from hello services in one datacenter or region so that they are available in other datacenters or regions when one fails.</span></span> 
2. <span data-ttu-id="d3a8b-290">執行跨越多個資料中心或區域的單一 Service Fabric 叢集。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-290">Run a single Service Fabric cluster that spans multiple datacenters or regions.</span></span> <span data-ttu-id="d3a8b-291">hello 最低支援的設定這是三個資料中心或區域。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-291">hello minimum supported configuration for this is three datacenters or regions.</span></span> <span data-ttu-id="d3a8b-292">hello 建議的區域數目或資料中心為 5。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-292">hello recommended number of regions or datacenters is five.</span></span> <span data-ttu-id="d3a8b-293">這需要更複雜的叢集拓撲。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-293">This requires a more complex cluster topology.</span></span> <span data-ttu-id="d3a8b-294">不過，hello 這個模型的優點是，一個資料中心或地區的失敗從嚴重損毀轉換成一般失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-294">However, hello benefit of this model is that failure of one datacenter or region is converted from a disaster into a normal failure.</span></span> <span data-ttu-id="d3a8b-295">可由單一區域內的叢集 hello 機制可處理這些失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-295">These failures can be handled by hello mechanisms that work for clusters within a single region.</span></span> <span data-ttu-id="d3a8b-296">容錯網域、升級網域，以及 Service Fabric 放置規則確保工作負載是分散的，以便容忍一般失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-296">Fault domains, upgrade domains, and Service Fabric's placement rules ensure workloads are distributed so that they tolerate normal failures.</span></span> <span data-ttu-id="d3a8b-297">如需有助於在此類叢集中操作服務之原則的詳細資訊，請參閱[放置原則](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span><span class="sxs-lookup"><span data-stu-id="d3a8b-297">For more information on policies that can help operate services in this type of cluster, read up on [placement policies](service-fabric-cluster-resource-manager-advanced-placement-rules-placement-policies.md)</span></span>

### <a name="random-failures-leading-toocluster-failures"></a><span data-ttu-id="d3a8b-298">隨機失敗導致 toocluster 失敗</span><span class="sxs-lookup"><span data-stu-id="d3a8b-298">Random failures leading toocluster failures</span></span>
<span data-ttu-id="d3a8b-299">Service Fabric 有 hello 概念的種子節點。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-299">Service Fabric has hello concept of Seed Nodes.</span></span> <span data-ttu-id="d3a8b-300">這些是維護 hello 可用性的 hello 基礎叢集的節點。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-300">These are nodes that maintain hello availability of hello underlying cluster.</span></span> <span data-ttu-id="d3a8b-301">這些節點會協助 tooensure hello 叢集會維持總建立租用，與其他節點，以及作為 tiebreakers 期間特定種類的網路失敗。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-301">These nodes help tooensure hello cluster remains up by establishing leases with other nodes and serving as tiebreakers during certain kinds of network failures.</span></span> <span data-ttu-id="d3a8b-302">如果隨機失敗 hello 叢集中移除大多數 hello 種子節點，它們不帶回 hello 叢集會自動關閉。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-302">If random failures remove a majority of hello seed nodes in hello cluster and they are not brought back, hello cluster automatically shuts down.</span></span> <span data-ttu-id="d3a8b-303">在 Azure 中，種子節點會自動管理： 它們透過 hello 提供容錯和升級的網域，散發，而且如果從 hello 叢集中移除單一種子節點將在其位置中建立另一個。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-303">In Azure, Seed Nodes are automatically managed: they are distributed over hello available fault and upgrade domains, and if a single seed node is removed from hello cluster another one will be created in its place.</span></span> 

<span data-ttu-id="d3a8b-304">獨立 Service Fabric 叢集和 Azure 中 hello 「 主要節點類型 」 是指執行 hello 種子 hello。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-304">In both standalone Service Fabric clusters and Azure, hello "Primary Node Type" is hello one that runs hello seeds.</span></span> <span data-ttu-id="d3a8b-305">當定義主要節點類型，Service Fabric 會自動利用 hello 提供藉由建立 too9 種子節點和 9 hello 系統服務的每個複本的節點數目。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-305">When defining a primary node type, Service Fabric will automatically take advantage of hello number of nodes provided by creating up too9 seed nodes and 9 replicas of each of hello system services.</span></span> <span data-ttu-id="d3a8b-306">如果在一組隨機失敗出這些系統服務複本的多數同時，hello 系統服務會進入遺失仲裁、 如我們上面所述。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-306">If a set of random failures takes out a majority of those system service replicas simultaneously, hello system services will enter quorum loss, as we described above.</span></span> <span data-ttu-id="d3a8b-307">如果大多數 hello 種子節點遺失，hello 叢集會關閉後立即。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-307">If a majority of hello seed nodes are lost, hello cluster will shut down soon after.</span></span>

## <a name="next-steps"></a><span data-ttu-id="d3a8b-308">後續步驟</span><span class="sxs-lookup"><span data-stu-id="d3a8b-308">Next steps</span></span>
- <span data-ttu-id="d3a8b-309">深入了解如何 toosimulate 各種失敗情況使用 hello[可測試性架構](service-fabric-testability-overview.md)</span><span class="sxs-lookup"><span data-stu-id="d3a8b-309">Learn how toosimulate various failures using hello [testability framework](service-fabric-testability-overview.md)</span></span>
- <span data-ttu-id="d3a8b-310">閱讀其他災害復原和高可用性的資源。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-310">Read other disaster-recovery and high-availability resources.</span></span> <span data-ttu-id="d3a8b-311">Microsoft 已發佈大量有關這些主題的指引。</span><span class="sxs-lookup"><span data-stu-id="d3a8b-311">Microsoft has published a large amount of guidance on these topics.</span></span> <span data-ttu-id="d3a8b-312">而這些文件的某些其他產品的使用 toospecific 技術，其中包含許多一般的最佳作法，您可以套用 hello Service Fabric 內容中：</span><span class="sxs-lookup"><span data-stu-id="d3a8b-312">While some of these documents refer toospecific techniques for use in other products, they contain many general best practices you can apply in hello Service Fabric context as well:</span></span>
  - [<span data-ttu-id="d3a8b-313">可用性檢查清單</span><span class="sxs-lookup"><span data-stu-id="d3a8b-313">Availability checklist</span></span>](../best-practices-availability-checklist.md)
  - [<span data-ttu-id="d3a8b-314">執行災害復原演練</span><span class="sxs-lookup"><span data-stu-id="d3a8b-314">Performing a disaster recovery drill</span></span>](../sql-database/sql-database-disaster-recovery-drills.md)
  - <span data-ttu-id="d3a8b-315">[Azure 應用程式的災害復原和高可用性][dr-ha-guide]</span><span class="sxs-lookup"><span data-stu-id="d3a8b-315">[Disaster recovery and high availability for Azure applications][dr-ha-guide]</span></span>
- <span data-ttu-id="d3a8b-316">了解 [Service Fabric 支援選項](service-fabric-support.md)</span><span class="sxs-lookup"><span data-stu-id="d3a8b-316">Learn about [Service Fabric support options](service-fabric-support.md)</span></span>

<!-- External links -->

[repair-partition-ps]: https://msdn.microsoft.com/library/mt163522.aspx
[azure-status-dashboard]:https://azure.microsoft.com/status/
[azure-regions]: https://azure.microsoft.com/regions/
[dr-ha-guide]: https://msdn.microsoft.com/library/azure/dn251004.aspx


<!-- Images -->

[sfx-cluster-map]: ./media/service-fabric-disaster-recovery/sfx-clustermap.png
