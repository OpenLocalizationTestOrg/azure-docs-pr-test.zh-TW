---
title: "aaaFeature hello 小組資料科學程序中的選取範圍 |Microsoft 文件"
description: "說明 hello 用途的特徵選取，並提供在機器學習的 hello 資料增強程序中的角色的範例。"
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 878541f5-1df8-4368-889a-ced6852aba47
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/24/2017
ms.author: zhangya;bradsev
ms.openlocfilehash: 54af93c83e4cc6a3670b3ad62490e0f74082b4ee
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 10/06/2017
---
# <a name="feature-selection-in-hello-team-data-science-process-tdsp"></a><span data-ttu-id="abee0-103">Hello 小組資料科學程序 (TDSP) 中的特徵選取</span><span class="sxs-lookup"><span data-stu-id="abee0-103">Feature selection in hello Team Data Science Process (TDSP)</span></span>
<span data-ttu-id="abee0-104">本文說明 hello 用途的特徵選取，並提供其角色在機器學習的 hello 資料增強功能程序中的範例。</span><span class="sxs-lookup"><span data-stu-id="abee0-104">This article explains hello purposes of feature selection and provides examples of its role in hello data enhancement process of machine learning.</span></span> <span data-ttu-id="abee0-105">這些範例是根據 Azure Machine Learning Studio 繪製。</span><span class="sxs-lookup"><span data-stu-id="abee0-105">These examples are drawn from Azure Machine Learning Studio.</span></span> 

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="abee0-106">hello 工程和選取的功能是 hello 小組資料科學程序 (TDSP) 中所述的某一部分[hello 小組資料科學程序是什麼？](data-science-process-overview.md)。</span><span class="sxs-lookup"><span data-stu-id="abee0-106">hello engineering and selection of features is one part of hello Team Data Science Process (TDSP) outlined in [What is hello Team Data Science Process?](data-science-process-overview.md).</span></span> <span data-ttu-id="abee0-107">功能工程及選取項目是部分 hello**開發功能**hello TDSP 的步驟。</span><span class="sxs-lookup"><span data-stu-id="abee0-107">Feature engineering and selection are parts of hello **Develop features** step of hello TDSP.</span></span>

* <span data-ttu-id="abee0-108">**功能工程**： 這個處理序嘗試 toocreate 其他相關的功能從 hello 現有未經處理的功能在 hello 資料和 tooincrease 預測檢定力 toohello 學習演算法。</span><span class="sxs-lookup"><span data-stu-id="abee0-108">**feature engineering**: This process attempts toocreate additional relevant features from hello existing raw features in hello data, and tooincrease predictive power toohello learning algorithm.</span></span>
* <span data-ttu-id="abee0-109">**特徵選取**： 此程序中 hello 訓練問題嘗試 tooreduce hello 維度性選取 hello 索引鍵子集的原始資料的功能。</span><span class="sxs-lookup"><span data-stu-id="abee0-109">**feature selection**: This process selects hello key subset of original data features in an attempt tooreduce hello dimensionality of hello training problem.</span></span>

<span data-ttu-id="abee0-110">通常**功能工程**套用第一個 toogenerate 額外的功能，然後在 hello**特徵選取**步驟是執行的 tooeliminate 無關、 備援性，或高度相關功能。</span><span class="sxs-lookup"><span data-stu-id="abee0-110">Normally **feature engineering** is applied first toogenerate additional features, and then hello **feature selection** step is performed tooeliminate irrelevant, redundant, or highly correlated features.</span></span>

## <a name="filtering-features-from-your-data---feature-selection"></a><span data-ttu-id="abee0-111">從您的資料篩選特性 - 特性選取</span><span class="sxs-lookup"><span data-stu-id="abee0-111">Filtering Features from Your Data - Feature Selection</span></span>
<span data-ttu-id="abee0-112">特徵選取是定型資料集的預測模型工作，例如分類或迴歸工作 hello 建構的通常會套用的處理序。</span><span class="sxs-lookup"><span data-stu-id="abee0-112">Feature selection is a process that is commonly applied for hello construction of training datasets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="abee0-113">hello 目標是 tooselect hello 資料中使用的最少的功能 toorepresent hello 最大數量的變異數減少其維度的 hello 功能從 hello 原始資料集的子集。</span><span class="sxs-lookup"><span data-stu-id="abee0-113">hello goal is tooselect a subset of hello features from hello original dataset that reduce its dimensions by using a minimal set of features toorepresent hello maximum amount of variance in hello data.</span></span> <span data-ttu-id="abee0-114">這個子集的功能是，則唯一功能 toobe hello 包含 tootrain hello 模型。</span><span class="sxs-lookup"><span data-stu-id="abee0-114">This subset of features are, then, hello only features toobe included tootrain hello model.</span></span> <span data-ttu-id="abee0-115">特性選取有兩個主要目的。</span><span class="sxs-lookup"><span data-stu-id="abee0-115">Feature selection serves two main purposes.</span></span>

* <span data-ttu-id="abee0-116">第一，特性選取通常會排除不相關、多餘或高度相關的特性，進而提高分類正確性。</span><span class="sxs-lookup"><span data-stu-id="abee0-116">First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="abee0-117">第二，也會降低 hello 數字的功能可讓模型定型程序更有效率。</span><span class="sxs-lookup"><span data-stu-id="abee0-117">Second, it decreases hello number of features which makes model training process more efficient.</span></span> <span data-ttu-id="abee0-118">這是特別重要，是高度耗費資源 tootrain 支援向量機器之類的學習模組。</span><span class="sxs-lookup"><span data-stu-id="abee0-118">This is particularly important for learners that are expensive tootrain such as support vector machines.</span></span>

<span data-ttu-id="abee0-119">雖然特徵選取，並搜尋 tooreduce hello 數目 hello 使用資料集 tootrain hello 模型中的功能，它通常不參考的 tooby hello 詞彙 「 縮減 」。</span><span class="sxs-lookup"><span data-stu-id="abee0-119">Although feature selection does seek tooreduce hello number of features in hello dataset used tootrain hello model, it is not usually referred tooby hello term "dimensionality reduction".</span></span> <span data-ttu-id="abee0-120">特徵選取方法會擷取 hello 資料中的原始功能的子集，而不變更它們。</span><span class="sxs-lookup"><span data-stu-id="abee0-120">Feature selection methods extract a subset of original features in hello data without changing them.</span></span>  <span data-ttu-id="abee0-121">維度性降低方法採用工程的功能，可以轉換 hello 原始功能，並因此能加以修改。</span><span class="sxs-lookup"><span data-stu-id="abee0-121">Dimensionality reduction methods employ engineered features that can transform hello original features and thus modify them.</span></span> <span data-ttu-id="abee0-122">維度縮減方法的範例包含主成分分析、典型相關分析和奇異值分解。</span><span class="sxs-lookup"><span data-stu-id="abee0-122">Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</span></span>

<span data-ttu-id="abee0-123">監督環境中有一個廣泛應用的特性選取方法類別，稱之為「以篩選為基礎的特性選取」。</span><span class="sxs-lookup"><span data-stu-id="abee0-123">Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</span></span> <span data-ttu-id="abee0-124">藉由評估每一個功能和 hello 目標屬性之間的 hello 相互關聯，這些方法會套用統計量值 tooassign 分數 tooeach 功能。</span><span class="sxs-lookup"><span data-stu-id="abee0-124">By evaluating hello correlation between each feature and hello target attribute, these methods apply a statistical measure tooassign a score tooeach feature.</span></span> <span data-ttu-id="abee0-125">hello 功能，然後會 hello 分數，可能會使用的 toohelp hello 閾值或排除的特定功能的項目。</span><span class="sxs-lookup"><span data-stu-id="abee0-125">hello features are then ranked by hello score, which may be used toohelp set hello threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="abee0-126">使用這些方法中的 hello 統計量值的範例包括人員相互關聯、 相互資訊和 hello 卡平方的測試。</span><span class="sxs-lookup"><span data-stu-id="abee0-126">Examples of hello statistical measures used in these methods include Person correlation, mutual information, and hello Chi squared test.</span></span>

<span data-ttu-id="abee0-127">Azure Machine Learning Studio 中有針對特性選取而提供的模組。</span><span class="sxs-lookup"><span data-stu-id="abee0-127">In Azure Machine Learning Studio, there are modules provided for feature selection.</span></span> <span data-ttu-id="abee0-128">Hello 遵循圖所示，這些模組包括[篩選器為基礎的特徵選取][ filter-based-feature-selection]和[費雪線性判別分析][ fisher-linear-discriminant-analysis].</span><span class="sxs-lookup"><span data-stu-id="abee0-128">As shown in hello following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![特性選取範例](./media/machine-learning-data-science-select-features/feature-Selection.png)

<span data-ttu-id="abee0-130">請考慮，例如，使用 hello hello[篩選器為基礎的特徵選取][ filter-based-feature-selection]模組。</span><span class="sxs-lookup"><span data-stu-id="abee0-130">Consider, for example, hello use of hello [Filter-Based Feature Selection][filter-based-feature-selection] module.</span></span> <span data-ttu-id="abee0-131">為了 hello 方便起見，我們將繼續 toouse hello 文字採礦範例以上所述。</span><span class="sxs-lookup"><span data-stu-id="abee0-131">For hello purpose of convenience, we continue toouse hello text mining example outlined above.</span></span> <span data-ttu-id="abee0-132">假設我們想要透過 hello 建立迴歸模型之後的一組 256 功能 toobuild[特徵雜湊][ feature-hashing]模組，該 hello 回應變數 hello"Col1"且代表活頁簿檢閱 [評等範圍從 1 too5。</span><span class="sxs-lookup"><span data-stu-id="abee0-132">Assume that we want toobuild a regression model after a set of 256 features are created through hello [Feature Hashing][feature-hashing] module, and that hello response variable is hello "Col1" and represents a book review ratings ranging from 1 too5.</span></span> <span data-ttu-id="abee0-133">藉由設定 」 功能計分方法 」 toobe"皮耳森"，hello"的目標資料行 」 toobe"Col1"，」 和 「 hello 」 所需特徵數目 」 too50。</span><span class="sxs-lookup"><span data-stu-id="abee0-133">By setting "Feature scoring method" toobe "Pearson Correlation", hello "Target column" toobe "Col1", and hello "Number of desired features" too50.</span></span> <span data-ttu-id="abee0-134">然後 hello 模組[篩選器為基礎的特徵選取][ filter-based-feature-selection]會產生包含 50 的功能與 hello 目標屬性"Col1"的資料集。</span><span class="sxs-lookup"><span data-stu-id="abee0-134">Then hello module [Filter-Based Feature Selection][filter-based-feature-selection] will produce a dataset containing 50 features together with hello target attribute "Col1".</span></span> <span data-ttu-id="abee0-135">hello 以下圖顯示這項實驗 hello 流程，並 hello 我們剛剛所述的輸入的參數。</span><span class="sxs-lookup"><span data-stu-id="abee0-135">hello following figure shows hello flow of this experiment and hello input parameters we just described.</span></span>

![特性選取範例](./media/machine-learning-data-science-select-features/feature-Selection1.png)

<span data-ttu-id="abee0-137">hello 下圖顯示 hello 產生資料集。</span><span class="sxs-lookup"><span data-stu-id="abee0-137">hello following figure shows hello resulting datasets.</span></span> <span data-ttu-id="abee0-138">每個功能計分根據 hello 皮耳森相關之間本身上和 hello "Col1"的目標屬性。</span><span class="sxs-lookup"><span data-stu-id="abee0-138">Each feature is scored based on hello Pearson Correlation between itself and hello target attribute "Col1".</span></span> <span data-ttu-id="abee0-139">具有高分的 hello 功能會保留。</span><span class="sxs-lookup"><span data-stu-id="abee0-139">hello features with top scores are kept.</span></span>

![特性選取範例](./media/machine-learning-data-science-select-features/feature-Selection2.png)

<span data-ttu-id="abee0-141">hello 遵循圖會顯示 hello 對應分數的 hello 選取功能。</span><span class="sxs-lookup"><span data-stu-id="abee0-141">hello corresponding scores of hello selected features are shown in hello following figure.</span></span>

![特性選取範例](./media/machine-learning-data-science-select-features/feature-Selection3.png)

<span data-ttu-id="abee0-143">藉由套用這[篩選器為基礎的特徵選取][ filter-based-feature-selection]模組，50 超出 256，它們有 hello 最多的相互關聯的功能與 hello 目標變數"Col1"，因為選取的功能，會根據 hello 計分方法"皮耳森"。</span><span class="sxs-lookup"><span data-stu-id="abee0-143">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have hello most correlated features with hello target variable "Col1", based on hello scoring method "Pearson Correlation".</span></span>

## <a name="conclusion"></a><span data-ttu-id="abee0-144">結論</span><span class="sxs-lookup"><span data-stu-id="abee0-144">Conclusion</span></span>
<span data-ttu-id="abee0-145">特徵設計和特徵選取是 hello 的兩個通常工程和選取的功能會提高定型程序，從而試著 tooextract hello 金鑰資訊 hello 資料中包含 hello 效率。</span><span class="sxs-lookup"><span data-stu-id="abee0-145">Feature engineering and feature selection are two commonly Engineered and selected features increase hello efficiency of hello training process which attempts tooextract hello key information contained in hello data.</span></span> <span data-ttu-id="abee0-146">它們也改善 hello 電源，這些模型 tooclassify hello 輸入資料的精確和 toopredict 結果感興趣多穩當地。</span><span class="sxs-lookup"><span data-stu-id="abee0-146">They also improve hello power of these models tooclassify hello input data accurately and toopredict outcomes of interest more robustly.</span></span> <span data-ttu-id="abee0-147">特徵設計和選取範圍也可以結合 toomake hello 學習更多計算容易處理。</span><span class="sxs-lookup"><span data-stu-id="abee0-147">Feature engineering and selection can also combine toomake hello learning more computationally tractable.</span></span> <span data-ttu-id="abee0-148">它是藉由增強，則減少 hello 一些功能需要 toocalibrate 」 或 「 定型模型。</span><span class="sxs-lookup"><span data-stu-id="abee0-148">It does so by enhancing and then reducing hello number of features needed toocalibrate or train a model.</span></span> <span data-ttu-id="abee0-149">Hello 功能選取的 tootrain hello 模型數學上來說，是最基本的獨立變數說明 hello 資料中的 hello 模式，然後預測結果成功。</span><span class="sxs-lookup"><span data-stu-id="abee0-149">Mathematically speaking, hello features selected tootrain hello model are a minimal set of independent variables that explain hello patterns in hello data and then predict outcomes successfully.</span></span>

<span data-ttu-id="abee0-150">請注意，它不一定一定 tooperform 工程或功能特徵。</span><span class="sxs-lookup"><span data-stu-id="abee0-150">Note that it is not always necessarily tooperform feature engineering or feature selection.</span></span> <span data-ttu-id="abee0-151">是否需要與否，取決於 hello 資料我們或者收集，我們挑選，hello 演算法，然後再 hello hello 實驗的目標。</span><span class="sxs-lookup"><span data-stu-id="abee0-151">Whether it is needed or not depends on hello data we have or collect, hello algorithm we pick, and hello objective of hello experiment.</span></span>

<!-- Module References -->
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/

