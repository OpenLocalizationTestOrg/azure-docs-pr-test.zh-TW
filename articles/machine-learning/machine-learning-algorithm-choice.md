---
title: "aaaHow toochoose 機器學習演算法 |Microsoft 文件"
description: "如何 toochoose Azure 機器學習演算法在叢集中受監督和不受監督的學習、 分類或迴歸實驗。"
services: machine-learning
documentationcenter: 
author: garyericson
manager: jhubbard
editor: cgronlun
tags: 
ms.assetid: a3b23d7f-f083-49c4-b6b1-3911cd69f1b4
ms.service: machine-learning
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: data-services
ms.date: 04/25/2017
ms.author: garye
ms.openlocfilehash: 367b2278acc2435f27f9d24ead8199db58aca283
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 10/06/2017
---
# <a name="how-toochoose-algorithms-for-microsoft-azure-machine-learning"></a><span data-ttu-id="7011b-103">如何為 Microsoft Azure Machine Learning toochoose 演算法</span><span class="sxs-lookup"><span data-stu-id="7011b-103">How toochoose algorithms for Microsoft Azure Machine Learning</span></span>
<span data-ttu-id="7011b-104">hello 回答 toohello 問題 「 哪些機器學習演算法應該使用？ 」</span><span class="sxs-lookup"><span data-stu-id="7011b-104">hello answer toohello question "What machine learning algorithm should I use?"</span></span> <span data-ttu-id="7011b-105">」的答案永遠都是「視情況。</span><span class="sxs-lookup"><span data-stu-id="7011b-105">is always "It depends."</span></span> <span data-ttu-id="7011b-106">Hello 大小、 品質以及 hello 資料的本質而定。</span><span class="sxs-lookup"><span data-stu-id="7011b-106">It depends on hello size, quality, and nature of hello data.</span></span> <span data-ttu-id="7011b-107">這取決於您想要 toodo hello 的答案。</span><span class="sxs-lookup"><span data-stu-id="7011b-107">It depends on what you want toodo with hello answer.</span></span> <span data-ttu-id="7011b-108">它相依於 hello 數學 hello 演算法的方式轉譯為您正在使用的 hello 電腦的指示。</span><span class="sxs-lookup"><span data-stu-id="7011b-108">It depends on how hello math of hello algorithm was translated into instructions for hello computer you are using.</span></span> <span data-ttu-id="7011b-109">而這又需視您有多少時間。</span><span class="sxs-lookup"><span data-stu-id="7011b-109">And it depends on how much time you have.</span></span> <span data-ttu-id="7011b-110">即使最發生 hello 資料科學家無法分辨哪一個演算法將會執行最佳之前嘗試它們。</span><span class="sxs-lookup"><span data-stu-id="7011b-110">Even hello most experienced data scientists can't tell which algorithm will perform best before trying them.</span></span>

## <a name="hello-machine-learning-algorithm-cheat-sheet"></a><span data-ttu-id="7011b-111">hello 機器學習演算法小工作表</span><span class="sxs-lookup"><span data-stu-id="7011b-111">hello Machine Learning Algorithm Cheat Sheet</span></span>
<span data-ttu-id="7011b-112">hello **Microsoft Azure 機器學習演算法小工作表**可協助您選擇 hello 右邊機器學習演算法，為從 hello Microsoft Azure 機器學習程式庫演算法的預測分析解決方案。</span><span class="sxs-lookup"><span data-stu-id="7011b-112">hello **Microsoft Azure Machine Learning Algorithm Cheat Sheet** helps you choose hello right machine learning algorithm for your predictive analytics solutions from hello Microsoft Azure Machine Learning library of algorithms.</span></span>
<span data-ttu-id="7011b-113">本文將逐步引導您 toouse 它。</span><span class="sxs-lookup"><span data-stu-id="7011b-113">This article walks you through how toouse it.</span></span>

> [!NOTE]
> <span data-ttu-id="7011b-114">toodownload hello 祕技與遵循這個發行項，請移過[機器的 Microsoft Azure Machine Learning Studio 學習演算法小祕技](machine-learning-algorithm-cheat-sheet.md)。</span><span class="sxs-lookup"><span data-stu-id="7011b-114">toodownload hello cheat sheet and follow along with this article, go too[Machine learning algorithm cheat sheet for Microsoft Azure Machine Learning Studio](machine-learning-algorithm-cheat-sheet.md).</span></span>
> 
> 

<span data-ttu-id="7011b-115">這個小祕技記住有非常特定的對象： 開頭資料科學家 charlie 層級的機器學習 」 中，以嘗試 toochoose Azure Machine Learning Studio 中使用的演算法 toostart。</span><span class="sxs-lookup"><span data-stu-id="7011b-115">This cheat sheet has a very specific audience in mind: a beginning data scientist with undergraduate-level machine learning, trying toochoose an algorithm toostart with in Azure Machine Learning Studio.</span></span> <span data-ttu-id="7011b-116">這表示小祕技可能會比較概括且過於簡化，但它為您指引一個可靠的方向。</span><span class="sxs-lookup"><span data-stu-id="7011b-116">That means that it makes some generalizations and oversimplifications, but it points you in a safe direction.</span></span> <span data-ttu-id="7011b-117">同時這也意味著還有許多演算法並未列入其中。</span><span class="sxs-lookup"><span data-stu-id="7011b-117">It also means that there are lots of algorithms not listed here.</span></span> <span data-ttu-id="7011b-118">Azure Machine Learning 隨著 tooencompass 一組更完整的可用方法，我們會將其加入。</span><span class="sxs-lookup"><span data-stu-id="7011b-118">As Azure Machine Learning grows tooencompass a more complete set of available methods, we'll add them.</span></span>

<span data-ttu-id="7011b-119">這些建議是收集許多資料科學家與機器學習專家的意見反應和提示所編撰而成。</span><span class="sxs-lookup"><span data-stu-id="7011b-119">These recommendations are compiled feedback and tips from many data scientists and machine learning experts.</span></span> <span data-ttu-id="7011b-120">我們未同意的任何項目，但我嘗試過 tooharmonize 意見到粗略共識。</span><span class="sxs-lookup"><span data-stu-id="7011b-120">We didn't agree on everything, but I've tried tooharmonize our opinions into a rough consensus.</span></span> <span data-ttu-id="7011b-121">大部分的 disagreement hello 陳述式的開頭為 「 它相依...」</span><span class="sxs-lookup"><span data-stu-id="7011b-121">Most of hello statements of disagreement begin with "It depends…"</span></span>

### <a name="how-toouse-hello-cheat-sheet"></a><span data-ttu-id="7011b-122">如何 toouse hello 速查表</span><span class="sxs-lookup"><span data-stu-id="7011b-122">How toouse hello cheat sheet</span></span>
<span data-ttu-id="7011b-123">讀取 hello 做為 hello 圖表上的路徑和演算法標籤 」 的*&lt;路徑標籤&gt;*，使用*&lt;演算法&gt;*。 」</span><span class="sxs-lookup"><span data-stu-id="7011b-123">Read hello path and algorithm labels on hello chart as "For *&lt;path label&gt;*, use *&lt;algorithm&gt;*."</span></span> <span data-ttu-id="7011b-124">例如「如果需要 *speed* (速度)，則使用 *two class logistic regression* (雙類別羅吉斯迴歸)。」</span><span class="sxs-lookup"><span data-stu-id="7011b-124">For example, "For *speed*, use *two class logistic regression*."</span></span> <span data-ttu-id="7011b-125">有時候適用於多個分支。</span><span class="sxs-lookup"><span data-stu-id="7011b-125">Sometimes more than one branch applies.</span></span>
<span data-ttu-id="7011b-126">有時候則不完全適用。</span><span class="sxs-lookup"><span data-stu-id="7011b-126">Sometimes none of them are a perfect fit.</span></span> <span data-ttu-id="7011b-127">它們是預定的 toobe 法則的建議，因此不要擔心正在確切。</span><span class="sxs-lookup"><span data-stu-id="7011b-127">They're intended toobe rule-of-thumb recommendations, so don't worry about it being exact.</span></span>
<span data-ttu-id="7011b-128">談到具有數個資料科學家說只確定的方式來尋找 hello 非常最佳的演算法是 tootry 該 hello 它們全部。</span><span class="sxs-lookup"><span data-stu-id="7011b-128">Several data scientists I talked with said that hello only sure way to find hello very best algorithm is tootry all of them.</span></span>

<span data-ttu-id="7011b-129">以下是從 hello 範例[Cortana 智慧組件庫](http://gallery.cortanaintelligence.com/)的實驗，並試著 hello 針對數種演算法相同的資料和比較 hello 結果：[比較多級分類器： 字母辨識](http://gallery.cortanaintelligence.com/Details/a635502fc98b402a890efe21cec65b92).</span><span class="sxs-lookup"><span data-stu-id="7011b-129">Here's an example from hello [Cortana Intelligence Gallery](http://gallery.cortanaintelligence.com/) of an experiment that tries several algorithms against hello same data and compares hello results: [Compare Multi-class Classifiers: Letter recognition](http://gallery.cortanaintelligence.com/Details/a635502fc98b402a890efe21cec65b92).</span></span>

> [!TIP]
> <span data-ttu-id="7011b-130">toodownload 及列印的圖表，hello Machine Learning Studio 中，功能的概觀，請參閱[Azure Machine Learning Studio 功能的概觀圖表](machine-learning-studio-overview-diagram.md)。</span><span class="sxs-lookup"><span data-stu-id="7011b-130">toodownload and print a diagram that gives an overview of hello capabilities of Machine Learning Studio, see [Overview diagram of Azure Machine Learning Studio capabilities](machine-learning-studio-overview-diagram.md).</span></span>
> 
> 

## <a name="flavors-of-machine-learning"></a><span data-ttu-id="7011b-131">機器學習的類型</span><span class="sxs-lookup"><span data-stu-id="7011b-131">Flavors of machine learning</span></span>
### <a name="supervised"></a><span data-ttu-id="7011b-132">監督式</span><span class="sxs-lookup"><span data-stu-id="7011b-132">Supervised</span></span>
<span data-ttu-id="7011b-133">監督式學習演算法會根據一組範例做出預測。</span><span class="sxs-lookup"><span data-stu-id="7011b-133">Supervised learning algorithms make predictions based on a set of examples.</span></span> <span data-ttu-id="7011b-134">比方說，歷程記錄股票價格可以是使用的 toohazard 猜測，在未來的價格。</span><span class="sxs-lookup"><span data-stu-id="7011b-134">For instance, historical stock prices can be used toohazard guesses at future prices.</span></span> <span data-ttu-id="7011b-135">用於定型的每個範例會加上感興趣的 hello 值 — 在此情況下 hello 股票價格。</span><span class="sxs-lookup"><span data-stu-id="7011b-135">Each example used for training is labeled with hello value of interest—in this case hello stock price.</span></span> <span data-ttu-id="7011b-136">監督式學習演算法會在這些值標籤中尋找模式。</span><span class="sxs-lookup"><span data-stu-id="7011b-136">A supervised learning algorithm looks for patterns in those value labels.</span></span> <span data-ttu-id="7011b-137">它可以使用任何可能相關的資訊 — hello 天數 hello 週 hello 季節、 hello 公司財務資料，hello 的產業、 類型 hello 干擾地理政治事件是否存在，及每一個演算法會尋找不同類型的模式。</span><span class="sxs-lookup"><span data-stu-id="7011b-137">It can use any information that might be relevant—hello day of hello week, hello season, hello company's financial data, hello type of industry, hello presence of disruptive geopolitical events—and each algorithm looks for different types of patterns.</span></span> <span data-ttu-id="7011b-138">Hello 演算法找到 hello 最佳的模式，它可以後，它會使用模式 toomake 預測未標記測試資料 — 明天的價格。</span><span class="sxs-lookup"><span data-stu-id="7011b-138">After hello algorithm has found hello best pattern it can, it uses that pattern toomake predictions for unlabeled testing data—tomorrow's prices.</span></span>

<span data-ttu-id="7011b-139">監督式學習是常見且實用的機器學習類型。</span><span class="sxs-lookup"><span data-stu-id="7011b-139">Supervised learning is a popular and useful type of machine learning.</span></span> <span data-ttu-id="7011b-140">有一個例外狀況，在 Azure 機器學習所有 hello 模組是監督都式學習演算法。</span><span class="sxs-lookup"><span data-stu-id="7011b-140">With one exception, all hello modules in Azure Machine Learning are supervised learning algorithms.</span></span> <span data-ttu-id="7011b-141">Azure 機器學習中有幾個代表性的特定監督式學習類型：分類、迴歸和異常偵測。</span><span class="sxs-lookup"><span data-stu-id="7011b-141">There are several specific types of supervised learning that are represented within Azure Machine Learning: classification, regression, and anomaly detection.</span></span>

* <span data-ttu-id="7011b-142">**分類**。</span><span class="sxs-lookup"><span data-stu-id="7011b-142">**Classification**.</span></span> <span data-ttu-id="7011b-143">當 hello 資料正在使用的 toopredict 類別時，監督式的學習也稱為分類。</span><span class="sxs-lookup"><span data-stu-id="7011b-143">When hello data are being used toopredict a category, supervised learning is also called classification.</span></span> <span data-ttu-id="7011b-144">這是指派為 'cat' 或 'dog' 圖片影像 hello 情況。</span><span class="sxs-lookup"><span data-stu-id="7011b-144">This is hello case when assigning an image as a picture of either a 'cat' or a 'dog'.</span></span> <span data-ttu-id="7011b-145">如果只有兩個選擇，則稱作**雙類別**或**二項式分類**。</span><span class="sxs-lookup"><span data-stu-id="7011b-145">When there are only two choices, it's called **two-class** or **binomial classification**.</span></span> <span data-ttu-id="7011b-146">時有更多的類別，做為預測的 hello NCAA 年 3 月瘋狂聯賽 hello 成功者時，此問題稱為**多級分類**。</span><span class="sxs-lookup"><span data-stu-id="7011b-146">When there are more categories, as when predicting hello winner of hello NCAA March Madness tournament, this problem is known as **multi-class classification**.</span></span>
* <span data-ttu-id="7011b-147">**迴歸**。</span><span class="sxs-lookup"><span data-stu-id="7011b-147">**Regression**.</span></span> <span data-ttu-id="7011b-148">如果要預測值，例如股價，這種監督式學習稱為迴歸。</span><span class="sxs-lookup"><span data-stu-id="7011b-148">When a value is being predicted, as with stock prices, supervised learning is called regression.</span></span>
* <span data-ttu-id="7011b-149">**異常偵測**。</span><span class="sxs-lookup"><span data-stu-id="7011b-149">**Anomaly detection**.</span></span> <span data-ttu-id="7011b-150">有時 hello 的目標是只是不尋常的 tooidentify 資料點。</span><span class="sxs-lookup"><span data-stu-id="7011b-150">Sometimes hello goal is tooidentify data points that are simply unusual.</span></span> <span data-ttu-id="7011b-151">例如在偵測詐騙時，只要是極不尋常的信用卡消費模式都有嫌疑。</span><span class="sxs-lookup"><span data-stu-id="7011b-151">In fraud detection, for example, any highly unusual credit card spending patterns are suspect.</span></span> <span data-ttu-id="7011b-152">hello 可能變化這麼多和 hello 因此數，它是不可行 toolearn 詐騙活動看起來的定型範例。</span><span class="sxs-lookup"><span data-stu-id="7011b-152">hello possible variations are so numerous and hello training examples so few, that it's not feasible toolearn what fraudulent activity looks like.</span></span> <span data-ttu-id="7011b-153">異常偵測採用的方法是 toosimply 深入了解哪些活動正常看起來像 （使用歷程記錄非詐騙交易），並找出任何明顯不同的項目。</span><span class="sxs-lookup"><span data-stu-id="7011b-153">The approach that anomaly detection takes is toosimply learn what normal activity looks like (using a history non-fraudulent transactions) and identify anything that is significantly different.</span></span>

### <a name="unsupervised"></a><span data-ttu-id="7011b-154">未監督式</span><span class="sxs-lookup"><span data-stu-id="7011b-154">Unsupervised</span></span>
<span data-ttu-id="7011b-155">在未監督的學習中，資料點沒有與其相關聯的標籤。</span><span class="sxs-lookup"><span data-stu-id="7011b-155">In unsupervised learning, data points have no labels associated with them.</span></span> <span data-ttu-id="7011b-156">相反地，hello 目標不受監督的學習演算法組織中部分的方式或 toodescribe hello 資料結構。</span><span class="sxs-lookup"><span data-stu-id="7011b-156">Instead, hello goal of an unsupervised learning algorithm is to organize hello data in some way or toodescribe its structure.</span></span> <span data-ttu-id="7011b-157">這種方式可能是將資料劃分為叢集，或尋找各種查看複雜資料的方式，讓資料變得更簡單或更整齊。</span><span class="sxs-lookup"><span data-stu-id="7011b-157">This can mean grouping it into clusters or finding different ways of looking at complex data so that it appears simpler or more organized.</span></span>

### <a name="reinforcement-learning"></a><span data-ttu-id="7011b-158">增強式學習</span><span class="sxs-lookup"><span data-stu-id="7011b-158">Reinforcement learning</span></span>
<span data-ttu-id="7011b-159">增援學習 hello 演算法取得 toochoose 動作以回應 tooeach 資料點。</span><span class="sxs-lookup"><span data-stu-id="7011b-159">In reinforcement learning, hello algorithm gets toochoose an action in response tooeach data point.</span></span> <span data-ttu-id="7011b-160">hello 學習演算法也會收到一小段時間之後，指出妥當 hello 決策是報酬訊號。</span><span class="sxs-lookup"><span data-stu-id="7011b-160">hello learning algorithm also receives a reward signal a short time later, indicating how good hello decision was.</span></span>
<span data-ttu-id="7011b-161">根據這個 hello 演算法修改其策略中順序 tooachieve hello 最高的報酬。</span><span class="sxs-lookup"><span data-stu-id="7011b-161">Based on this, hello algorithm modifies its strategy in order tooachieve hello highest reward.</span></span> <span data-ttu-id="7011b-162">Azure 機器學習中目前沒有增強式學習演算法模組。</span><span class="sxs-lookup"><span data-stu-id="7011b-162">Currently there are no reinforcement learning algorithm modules in Azure Machine Learning.</span></span> <span data-ttu-id="7011b-163">增援學習中很常見機器人其中 hello 時間在一處的感應器讀數組是資料點，而 hello 演算法必須選擇 hello 機器人的下一個動作。</span><span class="sxs-lookup"><span data-stu-id="7011b-163">Reinforcement learning is common in robotics, where hello set of sensor readings at one point in time is a data point, and hello algorithm must choose hello robot's next action.</span></span> <span data-ttu-id="7011b-164">它的性質也很適合物聯網應用。</span><span class="sxs-lookup"><span data-stu-id="7011b-164">It is also a natural fit for Internet of Things applications.</span></span>

## <a name="considerations-when-choosing-an-algorithm"></a><span data-ttu-id="7011b-165">選擇演算法時的考量</span><span class="sxs-lookup"><span data-stu-id="7011b-165">Considerations when choosing an algorithm</span></span>
### <a name="accuracy"></a><span data-ttu-id="7011b-166">精確度</span><span class="sxs-lookup"><span data-stu-id="7011b-166">Accuracy</span></span>
<span data-ttu-id="7011b-167">取得 hello 最精確回應可能並不一定。</span><span class="sxs-lookup"><span data-stu-id="7011b-167">Getting hello most accurate answer possible isn't always necessary.</span></span>
<span data-ttu-id="7011b-168">視您的用途而定，有時候近似值便已足夠。</span><span class="sxs-lookup"><span data-stu-id="7011b-168">Sometimes an approximation is adequate, depending on what you want to use it for.</span></span> <span data-ttu-id="7011b-169">在 hello 情形下，您可能無法 toocut 您處理時間大幅會繼續使用多個相近的方法。</span><span class="sxs-lookup"><span data-stu-id="7011b-169">If that's hello case, you may be able toocut your processing time dramatically by sticking with more approximate methods.</span></span> <span data-ttu-id="7011b-170">近似法的另一項優點是，它們會自然傾向於避免 [過度學習](https://youtu.be/DQWI1kvmwRg)。</span><span class="sxs-lookup"><span data-stu-id="7011b-170">Another advantage of more approximate methods is that they naturally tend to avoid [overfitting](https://youtu.be/DQWI1kvmwRg).</span></span>

### <a name="training-time"></a><span data-ttu-id="7011b-171">定型時間</span><span class="sxs-lookup"><span data-stu-id="7011b-171">Training time</span></span>
<span data-ttu-id="7011b-172">hello 的分鐘數或小時必要 tootrain 模型而異划算演算法。</span><span class="sxs-lookup"><span data-stu-id="7011b-172">hello number of minutes or hours necessary tootrain a model varies a great deal between algorithms.</span></span> <span data-ttu-id="7011b-173">定型時間精確度與通常緊密相關-一個通常會伴隨其他 hello。</span><span class="sxs-lookup"><span data-stu-id="7011b-173">Training time is often closely tied to accuracy—one typically accompanies hello other.</span></span> <span data-ttu-id="7011b-174">此外，某些演算法會更為敏感 toohello 資料點數目比其他。</span><span class="sxs-lookup"><span data-stu-id="7011b-174">In addition, some algorithms are more sensitive toohello number of data points than others.</span></span>
<span data-ttu-id="7011b-175">時間限制時它資料表可以驅動 hello 演算法的選擇，尤其 hello 資料集很大。</span><span class="sxs-lookup"><span data-stu-id="7011b-175">When time is limited it can drive hello choice of algorithm, especially when hello data set is large.</span></span>

### <a name="linearity"></a><span data-ttu-id="7011b-176">線性</span><span class="sxs-lookup"><span data-stu-id="7011b-176">Linearity</span></span>
<span data-ttu-id="7011b-177">許多機器學習演算法都會使用線性。</span><span class="sxs-lookup"><span data-stu-id="7011b-177">Lots of machine learning algorithms make use of linearity.</span></span> <span data-ttu-id="7011b-178">線性分類演算法會假設可以直線 (或較高維度類比) 分隔類別。</span><span class="sxs-lookup"><span data-stu-id="7011b-178">Linear classification algorithms assume that classes can be separated by a straight line (or its higher-dimensional analog).</span></span> <span data-ttu-id="7011b-179">這些演算法包括羅吉斯迴歸和支援向量機器 (如同 Azure 機器學習中所實作)。</span><span class="sxs-lookup"><span data-stu-id="7011b-179">These include logistic regression and support vector machines (as implemented in Azure Machine Learning).</span></span>
<span data-ttu-id="7011b-180">線性迴歸演算法會假設資料趨勢依循著一條直線。</span><span class="sxs-lookup"><span data-stu-id="7011b-180">Linear regression algorithms assume that data trends follow a straight line.</span></span> <span data-ttu-id="7011b-181">這類假設對某些問題而言還不錯，但在其他問題上會降低精確度。</span><span class="sxs-lookup"><span data-stu-id="7011b-181">These assumptions aren't bad for some problems, but on others they bring accuracy down.</span></span>

![非線性類別界限][1]

<span data-ttu-id="7011b-183">***非線性類別界限*** *- 依賴線性分類演算法會造成低精確度的結果*</span><span class="sxs-lookup"><span data-stu-id="7011b-183">***Non-linear class boundary*** *- relying on a linear classification algorithm would result in low accuracy*</span></span>

![具有非線性趨勢的資料][2]

<span data-ttu-id="7011b-185">***具有非線性趨勢的資料*** *：使用線性迴歸方法會產生較大且不必要的誤差*</span><span class="sxs-lookup"><span data-stu-id="7011b-185">***Data with a nonlinear trend*** *- using a linear regression method would generate much larger errors than necessary*</span></span>

<span data-ttu-id="7011b-186">儘管有風險，線性演算法對於首次攻擊而言仍是一種非常熱門的方式。</span><span class="sxs-lookup"><span data-stu-id="7011b-186">Despite their dangers, linear algorithms are very popular as a first line of attack.</span></span> <span data-ttu-id="7011b-187">它們通常 toobe 以演算法簡單和快速定型。</span><span class="sxs-lookup"><span data-stu-id="7011b-187">They tend toobe algorithmically simple and fast to train.</span></span>

### <a name="number-of-parameters"></a><span data-ttu-id="7011b-188">參數數目</span><span class="sxs-lookup"><span data-stu-id="7011b-188">Number of parameters</span></span>
<span data-ttu-id="7011b-189">參數是 hello 參數設定的演算法時，資料科學家取得 tooturn。</span><span class="sxs-lookup"><span data-stu-id="7011b-189">Parameters are hello knobs a data scientist gets tooturn when setting up an algorithm.</span></span> <span data-ttu-id="7011b-190">它們是會影響 hello 演算法的行為，例如容錯或反覆項目或之間的 hello 演算法的運作方式的選項數目的數字。</span><span class="sxs-lookup"><span data-stu-id="7011b-190">They are numbers that affect hello algorithm's behavior, such as error tolerance or number of iterations, or options between variants of how hello algorithm behaves.</span></span> <span data-ttu-id="7011b-191">hello 定型時間和精確度的 hello 演算法有時可能很容易受到 toogetting 只 hello 正確設定。</span><span class="sxs-lookup"><span data-stu-id="7011b-191">hello training time and accuracy of hello algorithm can sometimes be quite sensitive toogetting just hello right settings.</span></span> <span data-ttu-id="7011b-192">一般而言，大量參數的演算法需要最試用版的 hello 和錯誤 toofind 良好的組合。</span><span class="sxs-lookup"><span data-stu-id="7011b-192">Typically, algorithms with large numbers parameters require hello most trial and error toofind a good combination.</span></span>

<span data-ttu-id="7011b-193">或者，Azure 機器學習中有 [參數掃掠](machine-learning-algorithm-parameters-optimize.md) 模組區塊，會依照您選擇的細微性，自動嘗試所有參數組合。</span><span class="sxs-lookup"><span data-stu-id="7011b-193">Alternatively, there is a [parameter sweeping](machine-learning-algorithm-parameters-optimize.md) module block in Azure Machine Learning that automatically tries all parameter combinations at whatever granularity you choose.</span></span> <span data-ttu-id="7011b-194">雖然這是很好的方法 toomake 確定您已合併 hello 參數空間; hello 所需時間 tootrain 模型以等比級數增加 hello 參數數目。</span><span class="sxs-lookup"><span data-stu-id="7011b-194">While this is a great way toomake sure you've spanned hello parameter space, hello time required tootrain a model increases exponentially with hello number of parameters.</span></span>

<span data-ttu-id="7011b-195">hello 優點是，通常具有許多參數指出演算法是否有更大的彈性。</span><span class="sxs-lookup"><span data-stu-id="7011b-195">hello upside is that having many parameters typically indicates that an algorithm has greater flexibility.</span></span> <span data-ttu-id="7011b-196">這通常可以達到很高的精確度。</span><span class="sxs-lookup"><span data-stu-id="7011b-196">It can often achieve very good accuracy.</span></span> <span data-ttu-id="7011b-197">提供您可以找到 hello 正確的參數設定的組合。</span><span class="sxs-lookup"><span data-stu-id="7011b-197">Provided you can find hello right combination of parameter settings.</span></span>

### <a name="number-of-features"></a><span data-ttu-id="7011b-198">特徵數目</span><span class="sxs-lookup"><span data-stu-id="7011b-198">Number of features</span></span>
<span data-ttu-id="7011b-199">對於特定類型的資料，hello 的特徵數目可以是非常大的比較的 toohello 資料點數目。</span><span class="sxs-lookup"><span data-stu-id="7011b-199">For certain types of data, hello number of features can be very large compared toohello number of data points.</span></span> <span data-ttu-id="7011b-200">這通常是 hello 跟 genetics 或文字資料。</span><span class="sxs-lookup"><span data-stu-id="7011b-200">This is often hello case with genetics or textual data.</span></span> <span data-ttu-id="7011b-201">hello 大量的功能可以陷入困境、 導致關閉某些學習演算法，進行訓練 unfeasibly 長的時間。</span><span class="sxs-lookup"><span data-stu-id="7011b-201">hello large number of features can bog down some learning algorithms, making training time unfeasibly long.</span></span> <span data-ttu-id="7011b-202">支援向量機器是特別適用於 toothis 案例 （請參閱下文）。</span><span class="sxs-lookup"><span data-stu-id="7011b-202">Support Vector Machines are particularly well suited toothis case (see below).</span></span>

### <a name="special-cases"></a><span data-ttu-id="7011b-203">特殊案例</span><span class="sxs-lookup"><span data-stu-id="7011b-203">Special cases</span></span>
<span data-ttu-id="7011b-204">某些學習演算法會做出特定假設 hello 結構 hello 資料或 hello 預期結果。</span><span class="sxs-lookup"><span data-stu-id="7011b-204">Some learning algorithms make particular assumptions about hello structure of hello data or hello desired results.</span></span> <span data-ttu-id="7011b-205">如果可以找到符合需求的假設，您就能獲得更實用的結果、更精確的預測或更快的定型時間。</span><span class="sxs-lookup"><span data-stu-id="7011b-205">If you can find one that fits your needs, it can give you more useful results, more accurate predictions, or faster training times.</span></span>

| <span data-ttu-id="7011b-206">**演算法**</span><span class="sxs-lookup"><span data-stu-id="7011b-206">**Algorithm**</span></span> | <span data-ttu-id="7011b-207">**精確度**</span><span class="sxs-lookup"><span data-stu-id="7011b-207">**Accuracy**</span></span> | <span data-ttu-id="7011b-208">**定型時間**</span><span class="sxs-lookup"><span data-stu-id="7011b-208">**Training time**</span></span> | <span data-ttu-id="7011b-209">**線性**</span><span class="sxs-lookup"><span data-stu-id="7011b-209">**Linearity**</span></span> | <span data-ttu-id="7011b-210">**參數**</span><span class="sxs-lookup"><span data-stu-id="7011b-210">**Parameters**</span></span> | <span data-ttu-id="7011b-211">**注意事項**</span><span class="sxs-lookup"><span data-stu-id="7011b-211">**Notes**</span></span> |
| --- |:---:|:---:|:---:|:---:| --- |
| <span data-ttu-id="7011b-212">**雙類別分類**</span><span class="sxs-lookup"><span data-stu-id="7011b-212">**Two-class classification**</span></span> | | | | | |
| [<span data-ttu-id="7011b-213">羅吉斯迴歸</span><span class="sxs-lookup"><span data-stu-id="7011b-213">logistic regression</span></span>](https://msdn.microsoft.com/library/azure/dn905994.aspx) | |<span data-ttu-id="7011b-214">●</span><span class="sxs-lookup"><span data-stu-id="7011b-214">●</span></span> |<span data-ttu-id="7011b-215">●</span><span class="sxs-lookup"><span data-stu-id="7011b-215">●</span></span> |<span data-ttu-id="7011b-216">5</span><span class="sxs-lookup"><span data-stu-id="7011b-216">5</span></span> | |
| [<span data-ttu-id="7011b-217">決策樹系</span><span class="sxs-lookup"><span data-stu-id="7011b-217">decision forest</span></span>](https://msdn.microsoft.com/library/azure/dn906008.aspx) |<span data-ttu-id="7011b-218">●</span><span class="sxs-lookup"><span data-stu-id="7011b-218">●</span></span> |<span data-ttu-id="7011b-219">○</span><span class="sxs-lookup"><span data-stu-id="7011b-219">○</span></span> | |<span data-ttu-id="7011b-220">6</span><span class="sxs-lookup"><span data-stu-id="7011b-220">6</span></span> | |
| [<span data-ttu-id="7011b-221">決策叢林</span><span class="sxs-lookup"><span data-stu-id="7011b-221">decision jungle</span></span>](https://msdn.microsoft.com/library/azure/dn905976.aspx) |<span data-ttu-id="7011b-222">●</span><span class="sxs-lookup"><span data-stu-id="7011b-222">●</span></span> |<span data-ttu-id="7011b-223">○</span><span class="sxs-lookup"><span data-stu-id="7011b-223">○</span></span> | |<span data-ttu-id="7011b-224">6</span><span class="sxs-lookup"><span data-stu-id="7011b-224">6</span></span> |<span data-ttu-id="7011b-225">低記憶體使用量</span><span class="sxs-lookup"><span data-stu-id="7011b-225">Low memory footprint</span></span> |
| [<span data-ttu-id="7011b-226">促進式決策樹</span><span class="sxs-lookup"><span data-stu-id="7011b-226">boosted decision tree</span></span>](https://msdn.microsoft.com/library/azure/dn906025.aspx) |<span data-ttu-id="7011b-227">●</span><span class="sxs-lookup"><span data-stu-id="7011b-227">●</span></span> |<span data-ttu-id="7011b-228">○</span><span class="sxs-lookup"><span data-stu-id="7011b-228">○</span></span> | |<span data-ttu-id="7011b-229">6</span><span class="sxs-lookup"><span data-stu-id="7011b-229">6</span></span> |<span data-ttu-id="7011b-230">高記憶體使用量</span><span class="sxs-lookup"><span data-stu-id="7011b-230">Large memory footprint</span></span> |
| [<span data-ttu-id="7011b-231">類神經網路</span><span class="sxs-lookup"><span data-stu-id="7011b-231">neural network</span></span>](https://msdn.microsoft.com/library/azure/dn905947.aspx) |<span data-ttu-id="7011b-232">●</span><span class="sxs-lookup"><span data-stu-id="7011b-232">●</span></span> | | |<span data-ttu-id="7011b-233">9</span><span class="sxs-lookup"><span data-stu-id="7011b-233">9</span></span> |[<span data-ttu-id="7011b-234">支援其他自訂項目</span><span class="sxs-lookup"><span data-stu-id="7011b-234">Additional customization is possible</span></span>](http://go.microsoft.com/fwlink/?LinkId=402867) |
| [<span data-ttu-id="7011b-235">平均感知器</span><span class="sxs-lookup"><span data-stu-id="7011b-235">averaged perceptron</span></span>](https://msdn.microsoft.com/library/azure/dn906036.aspx) |<span data-ttu-id="7011b-236">○</span><span class="sxs-lookup"><span data-stu-id="7011b-236">○</span></span> |<span data-ttu-id="7011b-237">○</span><span class="sxs-lookup"><span data-stu-id="7011b-237">○</span></span> |<span data-ttu-id="7011b-238">●</span><span class="sxs-lookup"><span data-stu-id="7011b-238">●</span></span> |<span data-ttu-id="7011b-239">4</span><span class="sxs-lookup"><span data-stu-id="7011b-239">4</span></span> | |
| [<span data-ttu-id="7011b-240">支援向量機器</span><span class="sxs-lookup"><span data-stu-id="7011b-240">support vector machine</span></span>](https://msdn.microsoft.com/library/azure/dn905835.aspx) | |<span data-ttu-id="7011b-241">○</span><span class="sxs-lookup"><span data-stu-id="7011b-241">○</span></span> |<span data-ttu-id="7011b-242">●</span><span class="sxs-lookup"><span data-stu-id="7011b-242">●</span></span> |<span data-ttu-id="7011b-243">5</span><span class="sxs-lookup"><span data-stu-id="7011b-243">5</span></span> |<span data-ttu-id="7011b-244">適用於大型特徵集</span><span class="sxs-lookup"><span data-stu-id="7011b-244">Good for large feature sets</span></span> |
| [<span data-ttu-id="7011b-245">本機深度支援向量機器</span><span class="sxs-lookup"><span data-stu-id="7011b-245">locally deep support vector machine</span></span>](https://msdn.microsoft.com/library/azure/dn913070.aspx) |<span data-ttu-id="7011b-246">○</span><span class="sxs-lookup"><span data-stu-id="7011b-246">○</span></span> | | |<span data-ttu-id="7011b-247">8</span><span class="sxs-lookup"><span data-stu-id="7011b-247">8</span></span> |<span data-ttu-id="7011b-248">適用於大型特徵集</span><span class="sxs-lookup"><span data-stu-id="7011b-248">Good for large feature sets</span></span> |
| [<span data-ttu-id="7011b-249">貝氏點機器</span><span class="sxs-lookup"><span data-stu-id="7011b-249">Bayes’ point machine</span></span>](https://msdn.microsoft.com/library/azure/dn905930.aspx) | |<span data-ttu-id="7011b-250">○</span><span class="sxs-lookup"><span data-stu-id="7011b-250">○</span></span> |<span data-ttu-id="7011b-251">●</span><span class="sxs-lookup"><span data-stu-id="7011b-251">●</span></span> |<span data-ttu-id="7011b-252">3</span><span class="sxs-lookup"><span data-stu-id="7011b-252">3</span></span> | |
| <span data-ttu-id="7011b-253">**多類別分類**</span><span class="sxs-lookup"><span data-stu-id="7011b-253">**Multi-class classification**</span></span> | | | | | |
| [<span data-ttu-id="7011b-254">羅吉斯迴歸</span><span class="sxs-lookup"><span data-stu-id="7011b-254">logistic regression</span></span>](https://msdn.microsoft.com/library/azure/dn905853.aspx) | |<span data-ttu-id="7011b-255">●</span><span class="sxs-lookup"><span data-stu-id="7011b-255">●</span></span> |<span data-ttu-id="7011b-256">●</span><span class="sxs-lookup"><span data-stu-id="7011b-256">●</span></span> |<span data-ttu-id="7011b-257">5</span><span class="sxs-lookup"><span data-stu-id="7011b-257">5</span></span> | |
| [<span data-ttu-id="7011b-258">決策樹系</span><span class="sxs-lookup"><span data-stu-id="7011b-258">decision forest</span></span>](https://msdn.microsoft.com/library/azure/dn906015.aspx) |<span data-ttu-id="7011b-259">●</span><span class="sxs-lookup"><span data-stu-id="7011b-259">●</span></span> |<span data-ttu-id="7011b-260">○</span><span class="sxs-lookup"><span data-stu-id="7011b-260">○</span></span> | |<span data-ttu-id="7011b-261">6</span><span class="sxs-lookup"><span data-stu-id="7011b-261">6</span></span> | |
| [<span data-ttu-id="7011b-262">決策叢林 </span><span class="sxs-lookup"><span data-stu-id="7011b-262">decision jungle </span></span>](https://msdn.microsoft.com/library/azure/dn905963.aspx) |<span data-ttu-id="7011b-263">●</span><span class="sxs-lookup"><span data-stu-id="7011b-263">●</span></span> |<span data-ttu-id="7011b-264">○</span><span class="sxs-lookup"><span data-stu-id="7011b-264">○</span></span> | |<span data-ttu-id="7011b-265">6</span><span class="sxs-lookup"><span data-stu-id="7011b-265">6</span></span> |<span data-ttu-id="7011b-266">低記憶體使用量</span><span class="sxs-lookup"><span data-stu-id="7011b-266">Low memory footprint</span></span> |
| [<span data-ttu-id="7011b-267">類神經網路</span><span class="sxs-lookup"><span data-stu-id="7011b-267">neural network</span></span>](https://msdn.microsoft.com/library/azure/dn906030.aspx) |<span data-ttu-id="7011b-268">●</span><span class="sxs-lookup"><span data-stu-id="7011b-268">●</span></span> | | |<span data-ttu-id="7011b-269">9</span><span class="sxs-lookup"><span data-stu-id="7011b-269">9</span></span> |[<span data-ttu-id="7011b-270">支援其他自訂項目</span><span class="sxs-lookup"><span data-stu-id="7011b-270">Additional customization is possible</span></span>](http://go.microsoft.com/fwlink/?LinkId=402867) |
| [<span data-ttu-id="7011b-271">one-v-all</span><span class="sxs-lookup"><span data-stu-id="7011b-271">one-v-all</span></span>](https://msdn.microsoft.com/library/azure/dn905887.aspx) |- |- |- |- |<span data-ttu-id="7011b-272">請參閱 hello 選取兩個類別方法的屬性</span><span class="sxs-lookup"><span data-stu-id="7011b-272">See properties of hello two-class method selected</span></span> |
| <span data-ttu-id="7011b-273">**迴歸**</span><span class="sxs-lookup"><span data-stu-id="7011b-273">**Regression**</span></span> | | | | | |
| [<span data-ttu-id="7011b-274">線性</span><span class="sxs-lookup"><span data-stu-id="7011b-274">linear</span></span>](https://msdn.microsoft.com/library/azure/dn905978.aspx) | |<span data-ttu-id="7011b-275">●</span><span class="sxs-lookup"><span data-stu-id="7011b-275">●</span></span> |<span data-ttu-id="7011b-276">●</span><span class="sxs-lookup"><span data-stu-id="7011b-276">●</span></span> |<span data-ttu-id="7011b-277">4</span><span class="sxs-lookup"><span data-stu-id="7011b-277">4</span></span> | |
| [<span data-ttu-id="7011b-278">貝氏線性</span><span class="sxs-lookup"><span data-stu-id="7011b-278">Bayesian linear</span></span>](https://msdn.microsoft.com/library/azure/dn906022.aspx) | |<span data-ttu-id="7011b-279">○</span><span class="sxs-lookup"><span data-stu-id="7011b-279">○</span></span> |<span data-ttu-id="7011b-280">●</span><span class="sxs-lookup"><span data-stu-id="7011b-280">●</span></span> |<span data-ttu-id="7011b-281">2</span><span class="sxs-lookup"><span data-stu-id="7011b-281">2</span></span> | |
| [<span data-ttu-id="7011b-282">決策樹系</span><span class="sxs-lookup"><span data-stu-id="7011b-282">decision forest</span></span>](https://msdn.microsoft.com/library/azure/dn905862.aspx) |<span data-ttu-id="7011b-283">●</span><span class="sxs-lookup"><span data-stu-id="7011b-283">●</span></span> |<span data-ttu-id="7011b-284">○</span><span class="sxs-lookup"><span data-stu-id="7011b-284">○</span></span> | |<span data-ttu-id="7011b-285">6</span><span class="sxs-lookup"><span data-stu-id="7011b-285">6</span></span> | |
| [<span data-ttu-id="7011b-286">促進式決策樹</span><span class="sxs-lookup"><span data-stu-id="7011b-286">boosted decision tree</span></span>](https://msdn.microsoft.com/library/azure/dn905801.aspx) |<span data-ttu-id="7011b-287">●</span><span class="sxs-lookup"><span data-stu-id="7011b-287">●</span></span> |<span data-ttu-id="7011b-288">○</span><span class="sxs-lookup"><span data-stu-id="7011b-288">○</span></span> | |<span data-ttu-id="7011b-289">5</span><span class="sxs-lookup"><span data-stu-id="7011b-289">5</span></span> |<span data-ttu-id="7011b-290">高記憶體使用量</span><span class="sxs-lookup"><span data-stu-id="7011b-290">Large memory footprint</span></span> |
| [<span data-ttu-id="7011b-291">快速樹系分量</span><span class="sxs-lookup"><span data-stu-id="7011b-291">fast forest quantile</span></span>](https://msdn.microsoft.com/library/azure/dn913093.aspx) |<span data-ttu-id="7011b-292">●</span><span class="sxs-lookup"><span data-stu-id="7011b-292">●</span></span> |<span data-ttu-id="7011b-293">○</span><span class="sxs-lookup"><span data-stu-id="7011b-293">○</span></span> | |<span data-ttu-id="7011b-294">9</span><span class="sxs-lookup"><span data-stu-id="7011b-294">9</span></span> |<span data-ttu-id="7011b-295">分佈而不是點預測</span><span class="sxs-lookup"><span data-stu-id="7011b-295">Distributions rather than point predictions</span></span> |
| [<span data-ttu-id="7011b-296">類神經網路</span><span class="sxs-lookup"><span data-stu-id="7011b-296">neural network</span></span>](https://msdn.microsoft.com/library/azure/dn905924.aspx) |<span data-ttu-id="7011b-297">●</span><span class="sxs-lookup"><span data-stu-id="7011b-297">●</span></span> | | |<span data-ttu-id="7011b-298">9</span><span class="sxs-lookup"><span data-stu-id="7011b-298">9</span></span> |[<span data-ttu-id="7011b-299">支援其他自訂項目</span><span class="sxs-lookup"><span data-stu-id="7011b-299">Additional customization is possible</span></span>](http://go.microsoft.com/fwlink/?LinkId=402867) |
| [<span data-ttu-id="7011b-300">波氏</span><span class="sxs-lookup"><span data-stu-id="7011b-300">Poisson</span></span>](https://msdn.microsoft.com/library/azure/dn905988.aspx) | | |<span data-ttu-id="7011b-301">●</span><span class="sxs-lookup"><span data-stu-id="7011b-301">●</span></span> |<span data-ttu-id="7011b-302">5</span><span class="sxs-lookup"><span data-stu-id="7011b-302">5</span></span> |<span data-ttu-id="7011b-303">技術上為對數線性。</span><span class="sxs-lookup"><span data-stu-id="7011b-303">Technically log-linear.</span></span> <span data-ttu-id="7011b-304">針對預測計算</span><span class="sxs-lookup"><span data-stu-id="7011b-304">For predicting counts</span></span> |
| [<span data-ttu-id="7011b-305">序數</span><span class="sxs-lookup"><span data-stu-id="7011b-305">ordinal</span></span>](https://msdn.microsoft.com/library/azure/dn906029.aspx) | | | |<span data-ttu-id="7011b-306">0</span><span class="sxs-lookup"><span data-stu-id="7011b-306">0</span></span> |<span data-ttu-id="7011b-307">針對預測順位排序</span><span class="sxs-lookup"><span data-stu-id="7011b-307">For predicting rank-ordering</span></span> |
| <span data-ttu-id="7011b-308">**異常偵測**</span><span class="sxs-lookup"><span data-stu-id="7011b-308">**Anomaly detection**</span></span> | | | | | |
| [<span data-ttu-id="7011b-309">支援向量機器</span><span class="sxs-lookup"><span data-stu-id="7011b-309">support vector machine</span></span>](https://msdn.microsoft.com/library/azure/dn913103.aspx) |<span data-ttu-id="7011b-310">○</span><span class="sxs-lookup"><span data-stu-id="7011b-310">○</span></span> |<span data-ttu-id="7011b-311">○</span><span class="sxs-lookup"><span data-stu-id="7011b-311">○</span></span> | |<span data-ttu-id="7011b-312">2</span><span class="sxs-lookup"><span data-stu-id="7011b-312">2</span></span> |<span data-ttu-id="7011b-313">特別適用於大型特徵集</span><span class="sxs-lookup"><span data-stu-id="7011b-313">Especially good for large feature sets</span></span> |
| [<span data-ttu-id="7011b-314">PCA 型異常偵測</span><span class="sxs-lookup"><span data-stu-id="7011b-314">PCA-based anomaly detection</span></span>](https://msdn.microsoft.com/library/azure/dn913102.aspx) | |<span data-ttu-id="7011b-315">○</span><span class="sxs-lookup"><span data-stu-id="7011b-315">○</span></span> |<span data-ttu-id="7011b-316">●</span><span class="sxs-lookup"><span data-stu-id="7011b-316">●</span></span> |<span data-ttu-id="7011b-317">3</span><span class="sxs-lookup"><span data-stu-id="7011b-317">3</span></span> | |
| [<span data-ttu-id="7011b-318">K-means</span><span class="sxs-lookup"><span data-stu-id="7011b-318">K-means</span></span>](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/) | |<span data-ttu-id="7011b-319">○</span><span class="sxs-lookup"><span data-stu-id="7011b-319">○</span></span> |<span data-ttu-id="7011b-320">●</span><span class="sxs-lookup"><span data-stu-id="7011b-320">●</span></span> |<span data-ttu-id="7011b-321">4</span><span class="sxs-lookup"><span data-stu-id="7011b-321">4</span></span> |<span data-ttu-id="7011b-322">叢集演算法</span><span class="sxs-lookup"><span data-stu-id="7011b-322">A clustering algorithm</span></span> |

<span data-ttu-id="7011b-323">**演算法屬性：**</span><span class="sxs-lookup"><span data-stu-id="7011b-323">**Algorithm properties:**</span></span>

<span data-ttu-id="7011b-324">**●** -顯示極佳的精確度、 快速的定型時間及和線性 hello 使用</span><span class="sxs-lookup"><span data-stu-id="7011b-324">**●** - shows excellent accuracy, fast training times, and hello use of linearity</span></span>

<span data-ttu-id="7011b-325">**○** ：顯示不錯的精確度和適度的定型時間</span><span class="sxs-lookup"><span data-stu-id="7011b-325">**○** - shows good accuracy and moderate training times</span></span>

## <a name="algorithm-notes"></a><span data-ttu-id="7011b-326">演算法備註</span><span class="sxs-lookup"><span data-stu-id="7011b-326">Algorithm notes</span></span>
### <a name="linear-regression"></a><span data-ttu-id="7011b-327">線性迴歸</span><span class="sxs-lookup"><span data-stu-id="7011b-327">Linear regression</span></span>
<span data-ttu-id="7011b-328">如前所述，[線性迴歸](https://msdn.microsoft.com/library/azure/dn905978.aspx)符合一行 （或平面或超平面） toohello 資料集。</span><span class="sxs-lookup"><span data-stu-id="7011b-328">As mentioned previously, [linear regression](https://msdn.microsoft.com/library/azure/dn905978.aspx) fits a line (or plane, or hyperplane) toohello data set.</span></span> <span data-ttu-id="7011b-329">它是常被使用的主力，簡單又快速，但可能會過度簡化某些問題。</span><span class="sxs-lookup"><span data-stu-id="7011b-329">It's a workhorse, simple and fast, but it may be overly simplistic for some problems.</span></span>
<span data-ttu-id="7011b-330">請查看這裡的 [線性迴歸教學課程](machine-learning-linear-regression-in-azure.md)。</span><span class="sxs-lookup"><span data-stu-id="7011b-330">Check here for a [linear regression tutorial](machine-learning-linear-regression-in-azure.md).</span></span>

![具有線性趨勢的資料][3]

<span data-ttu-id="7011b-332">***具有線性趨勢的資料***</span><span class="sxs-lookup"><span data-stu-id="7011b-332">***Data with a linear trend***</span></span>

### <a name="logistic-regression"></a><span data-ttu-id="7011b-333">羅吉斯迴歸</span><span class="sxs-lookup"><span data-stu-id="7011b-333">Logistic regression</span></span>
<span data-ttu-id="7011b-334">雖然困惑，它包含 '迴歸' hello 名稱中，羅吉斯迴歸是實際的強大工具[二級](https://msdn.microsoft.com/library/azure/dn905994.aspx)和[多級](https://msdn.microsoft.com/library/azure/dn905853.aspx)分類。</span><span class="sxs-lookup"><span data-stu-id="7011b-334">Although it confusingly includes 'regression' in hello name, logistic regression is actually a powerful tool for [two-class](https://msdn.microsoft.com/library/azure/dn905994.aspx) and [multiclass](https://msdn.microsoft.com/library/azure/dn905853.aspx) classification.</span></span> <span data-ttu-id="7011b-335">它既快速又簡單。</span><span class="sxs-lookup"><span data-stu-id="7011b-335">It's fast and simple.</span></span> <span data-ttu-id="7011b-336">hello 事實，它所使用的 '-使得自然適合將資料分組造形的曲線而不是直線。</span><span class="sxs-lookup"><span data-stu-id="7011b-336">hello fact that it uses an 'S'-shaped curve instead of a straight line makes it a natural fit for dividing data into groups.</span></span> <span data-ttu-id="7011b-337">羅吉斯迴歸提供線性類別界限，因此使用它時，請確定線性近似值是您能接受的結果。</span><span class="sxs-lookup"><span data-stu-id="7011b-337">Logistic regression gives linear class boundaries, so when you use it, make sure a linear approximation is something you can live with.</span></span>

![只要其中一項功能與羅吉斯迴歸 tootwo 類別資料][4]

<span data-ttu-id="7011b-339">***只要其中一項功能與羅吉斯迴歸 tootwo 類別資料*** *-類別界限是 hello 點在哪一個 hello 羅吉斯曲線只要接近 tooboth 類別*</span><span class="sxs-lookup"><span data-stu-id="7011b-339">***A logistic regression tootwo-class data with just one feature*** *- the class boundary is hello point at which hello logistic curve is just as close tooboth classes*</span></span>

### <a name="trees-forests-and-jungles"></a><span data-ttu-id="7011b-340">樹、樹系和叢林</span><span class="sxs-lookup"><span data-stu-id="7011b-340">Trees, forests, and jungles</span></span>
<span data-ttu-id="7011b-341">決策樹系 ([迴歸](https://msdn.microsoft.com/library/azure/dn905862.aspx)、[雙類別](https://msdn.microsoft.com/library/azure/dn906008.aspx)和[多類別](https://msdn.microsoft.com/library/azure/dn906015.aspx))、決策叢林 ([雙類別](https://msdn.microsoft.com/library/azure/dn905976.aspx)和[多類別](https://msdn.microsoft.com/library/azure/dn905963.aspx)) 以及促進式決策樹 ([迴歸](https://msdn.microsoft.com/library/azure/dn905801.aspx)和[雙類別](https://msdn.microsoft.com/library/azure/dn906025.aspx))，都是以基本的機器學習概念「決策樹」做為基礎。</span><span class="sxs-lookup"><span data-stu-id="7011b-341">Decision forests ([regression](https://msdn.microsoft.com/library/azure/dn905862.aspx), [two-class](https://msdn.microsoft.com/library/azure/dn906008.aspx), and [multiclass](https://msdn.microsoft.com/library/azure/dn906015.aspx)), decision jungles ([two-class](https://msdn.microsoft.com/library/azure/dn905976.aspx) and [multiclass](https://msdn.microsoft.com/library/azure/dn905963.aspx)), and boosted decision trees ([regression](https://msdn.microsoft.com/library/azure/dn905801.aspx) and [two-class](https://msdn.microsoft.com/library/azure/dn906025.aspx)) are all based on decision trees, a foundational machine learning concept.</span></span> <span data-ttu-id="7011b-342">有許多變化的決策樹，但它們全都執行相同的動作： hello 特徵空間細分為區域大部分 hello 與相同的標籤。</span><span class="sxs-lookup"><span data-stu-id="7011b-342">There are many variants of decision trees, but they all do the same thing—subdivide hello feature space into regions with mostly hello same label.</span></span> <span data-ttu-id="7011b-343">根據您是執行分類或迴歸而定，這些區域可能會有一致的類別或常數值。</span><span class="sxs-lookup"><span data-stu-id="7011b-343">These can be regions of consistent category or of constant value, depending on whether you are doing classification or regression.</span></span>

![細分特徵空間的決策樹][5]

<span data-ttu-id="7011b-345">***此決策樹將特徵空間細分為值大致統一的區域***</span><span class="sxs-lookup"><span data-stu-id="7011b-345">***A decision tree subdivides a feature space into regions of roughly uniform values***</span></span>

<span data-ttu-id="7011b-346">特徵空間可以再細分為很小的區域，因為它是簡單 tooimagine 拆解自助式足夠 toohave 一個資料點每個區域。</span><span class="sxs-lookup"><span data-stu-id="7011b-346">Because a feature space can be subdivided into arbitrarily small regions, it's easy tooimagine dividing it finely enough toohave one data point per region.</span></span> <span data-ttu-id="7011b-347">而這就是過度學習的極端範例。</span><span class="sxs-lookup"><span data-stu-id="7011b-347">This is an extreme example of overfitting.</span></span> <span data-ttu-id="7011b-348">在順序 tooavoid 樹狀結構的大型集合，這被建構小心特殊數學採取 hello 樹狀結構並沒有關聯。</span><span class="sxs-lookup"><span data-stu-id="7011b-348">In order tooavoid this, a large set of trees are constructed with special mathematical care taken that hello trees are not correlated.</span></span> <span data-ttu-id="7011b-349">這個 「 決策樹系 」 hello 平均是樹狀目錄中可避免過度配適。</span><span class="sxs-lookup"><span data-stu-id="7011b-349">hello average of this "decision forest" is a tree that avoids overfitting.</span></span> <span data-ttu-id="7011b-350">決策樹系會使用大量記憶體。</span><span class="sxs-lookup"><span data-stu-id="7011b-350">Decision forests can use a lot of memory.</span></span> <span data-ttu-id="7011b-351">決策叢林是會耗用較少的記憶體在 hello 費用的定型時間稍微長的 variant。</span><span class="sxs-lookup"><span data-stu-id="7011b-351">Decision jungles are a variant that consumes less memory at hello expense of a slightly longer training time.</span></span>

<span data-ttu-id="7011b-352">促進式決策樹可藉由限制細分的次數，以及每個區域中允許的最少資料點，來避免過度學習。</span><span class="sxs-lookup"><span data-stu-id="7011b-352">Boosted decision trees avoid overfitting by limiting how many times they can subdivide and how few data points are allowed in each region.</span></span> <span data-ttu-id="7011b-353">此演算法建構的一連串的樹狀結構，其中每個學習 hello 樹狀之前所留下的 hello 錯誤補償。</span><span class="sxs-lookup"><span data-stu-id="7011b-353">The algorithm constructs a sequence of trees, each of which learns to compensate for hello error left by hello tree before.</span></span> <span data-ttu-id="7011b-354">hello 結果會是記憶體的非常精確的學習因子，其中通常會 toouse 大量。</span><span class="sxs-lookup"><span data-stu-id="7011b-354">hello result is a very accurate learner that tends toouse a lot of memory.</span></span> <span data-ttu-id="7011b-355">如 hello 完整技術說明，請參閱[Friedman 的原始紙張](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf)。</span><span class="sxs-lookup"><span data-stu-id="7011b-355">For hello full technical description, check out [Friedman's original paper](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf).</span></span>

<span data-ttu-id="7011b-356">[快速樹系分量迴歸](https://msdn.microsoft.com/library/azure/dn913093.aspx)是您要知道不僅 hello 典型 （中間） 值 hello 中的資料區域，但也分量 hello 形式其散發 hello 特殊案例的決策樹的變化。</span><span class="sxs-lookup"><span data-stu-id="7011b-356">[Fast forest quantile regression](https://msdn.microsoft.com/library/azure/dn913093.aspx) is a variation of decision trees for hello special case where you want to know not only hello typical (median) value of hello data within a region, but also its distribution in hello form of quantiles.</span></span>

### <a name="neural-networks-and-perceptrons"></a><span data-ttu-id="7011b-357">類神經網路和感知器</span><span class="sxs-lookup"><span data-stu-id="7011b-357">Neural networks and perceptrons</span></span>
<span data-ttu-id="7011b-358">類神經網路是受大腦啟發的學習演算法，其中涵蓋[多類別](https://msdn.microsoft.com/library/azure/dn906030.aspx)、[雙類別](https://msdn.microsoft.com/library/azure/dn905947.aspx)和[迴歸](https://msdn.microsoft.com/library/azure/dn905924.aspx)問題。</span><span class="sxs-lookup"><span data-stu-id="7011b-358">Neural networks are brain-inspired learning algorithms covering [multiclass](https://msdn.microsoft.com/library/azure/dn906030.aspx), [two-class](https://msdn.microsoft.com/library/azure/dn905947.aspx), and [regression](https://msdn.microsoft.com/library/azure/dn905924.aspx) problems.</span></span> <span data-ttu-id="7011b-359">它們有無限的各種不同，但在 Azure Machine Learning 中的 hello 類神經網路 hello 導向非循環圖形式的所有。</span><span class="sxs-lookup"><span data-stu-id="7011b-359">They come in an infinite variety, but hello neural networks within Azure Machine Learning are all of hello form of directed acyclic graphs.</span></span> <span data-ttu-id="7011b-360">這表示輸入的特徵在轉換成輸出前，會在一連串的層中一直向前傳遞，且永遠不會向後。</span><span class="sxs-lookup"><span data-stu-id="7011b-360">That means that input features are passed forward (never backward) through a sequence of layers before being turned into outputs.</span></span> <span data-ttu-id="7011b-361">在每個圖層中，輸入會在不同的組合，經過加總，並且傳入 hello 下一層加權。</span><span class="sxs-lookup"><span data-stu-id="7011b-361">In each layer, inputs are weighted in various combinations, summed, and passed on to hello next layer.</span></span> <span data-ttu-id="7011b-362">這種簡單的計算能力 toolearn 導致組合魔術看似複雜類別界限和資料趨勢。</span><span class="sxs-lookup"><span data-stu-id="7011b-362">This combination of simple calculations results in the ability toolearn sophisticated class boundaries and data trends, seemingly by magic.</span></span> <span data-ttu-id="7011b-363">多層網路，這種執行 hello"深入學習 」 的 fuels 因此大多數時候有科技 reporting 和科幻。</span><span class="sxs-lookup"><span data-stu-id="7011b-363">Many-layered networks of this sort perform hello "deep learning" that fuels so much tech reporting and science fiction.</span></span>

<span data-ttu-id="7011b-364">可惜這種高效能並非隨手可得。</span><span class="sxs-lookup"><span data-stu-id="7011b-364">This high performance doesn't come for free, though.</span></span> <span data-ttu-id="7011b-365">類神經網路可能需要很長的時間 tootrain，特別是針對大型資料集具有許多功能。</span><span class="sxs-lookup"><span data-stu-id="7011b-365">Neural networks can take a long time tootrain, particularly for large data sets with lots of features.</span></span> <span data-ttu-id="7011b-366">他們也擁有比大部分的演算法，這表示參數掃掠划算展開 hello 定型時間更多參數。</span><span class="sxs-lookup"><span data-stu-id="7011b-366">They also have more parameters than most algorithms, which means that parameter sweeping expands hello training time a great deal.</span></span>
<span data-ttu-id="7011b-367">與這些使用者需要太 overachievers[指定自己的網路結構](http://go.microsoft.com/fwlink/?LinkId=402867)，這些可能是 inexhaustible。</span><span class="sxs-lookup"><span data-stu-id="7011b-367">And for those overachievers who wish too[specify their own network structure](http://go.microsoft.com/fwlink/?LinkId=402867), the possibilities are inexhaustible.</span></span>

<span data-ttu-id="7011b-368">![類神經網路學習界限][6]
***hello 界限所類神經網路學習很複雜，異常***</span><span class="sxs-lookup"><span data-stu-id="7011b-368">![Boundaries learned by neural networks][6]
***hello boundaries learned by neural networks can be complex and irregular***</span></span>

<span data-ttu-id="7011b-369">hello[二級平均感知器](https://msdn.microsoft.com/library/azure/dn906036.aspx)為類神經網路回應 tooskyrocketing 定型時間。</span><span class="sxs-lookup"><span data-stu-id="7011b-369">hello [two-class averaged perceptron](https://msdn.microsoft.com/library/azure/dn906036.aspx) is neural networks' answer tooskyrocketing training times.</span></span> <span data-ttu-id="7011b-370">它使用的網路結構會提供線性類別界限。</span><span class="sxs-lookup"><span data-stu-id="7011b-370">It uses a network structure that gives linear class boundaries.</span></span> <span data-ttu-id="7011b-371">幾乎基本今天的標準，但它已有悠久的歷史穩當地運作的而且夠小，toolearn 快速。</span><span class="sxs-lookup"><span data-stu-id="7011b-371">It is almost primitive by today's standards, but it has a long history of working robustly and is small enough toolearn quickly.</span></span>

### <a name="svms"></a><span data-ttu-id="7011b-372">SVM</span><span class="sxs-lookup"><span data-stu-id="7011b-372">SVMs</span></span>
<span data-ttu-id="7011b-373">支援向量機器 (Svm) 尋找 hello 界限分隔為盡可能寬的邊界的類別。</span><span class="sxs-lookup"><span data-stu-id="7011b-373">Support vector machines (SVMs) find hello boundary that separates classes by as wide a margin as possible.</span></span> <span data-ttu-id="7011b-374">當無法清楚地分隔 hello 兩個類別時，hello 演算法會發現 hello 最佳的界限，他們可以。</span><span class="sxs-lookup"><span data-stu-id="7011b-374">When hello two classes can't be clearly separated, hello algorithms find hello best boundary they can.</span></span> <span data-ttu-id="7011b-375">如同 Azure Machine Learning 中的 hello[二級 SVM](https://msdn.microsoft.com/library/azure/dn905835.aspx)進行這項只有直線。</span><span class="sxs-lookup"><span data-stu-id="7011b-375">As written in Azure Machine Learning, hello [two-class SVM](https://msdn.microsoft.com/library/azure/dn905835.aspx) does this with a straight line only.</span></span> <span data-ttu-id="7011b-376">(但以 SVM 的說法，應該是使用線性核心)。因為這會讓此之線性近似值，就能 toorun 速度會相當快。</span><span class="sxs-lookup"><span data-stu-id="7011b-376">(In SVM-speak, it uses a linear kernel.) Because it makes this linear approximation, it is able toorun fairly quickly.</span></span> <span data-ttu-id="7011b-377">可以真正發揮它功效的地方是特徵密集的資料，例如文字或基因資料。</span><span class="sxs-lookup"><span data-stu-id="7011b-377">Where it really shines is with feature-intense data, like text or genomic.</span></span> <span data-ttu-id="7011b-378">在這些情況下 Svm 是記憶體的無法 tooseparate 類別打字速度並減少過度配適比其他大部分的演算法，此外 toorequiring 只有少量數量。</span><span class="sxs-lookup"><span data-stu-id="7011b-378">In these cases SVMs are able tooseparate classes more quickly and with less overfitting than most other algorithms, in addition toorequiring only a modest amount of memory.</span></span>

![支援向量機器類別界限][7]

<span data-ttu-id="7011b-380">***典型的支援向量機器類別界限最大化 hello 邊界分隔兩個類別***</span><span class="sxs-lookup"><span data-stu-id="7011b-380">***A typical support vector machine class boundary maximizes hello margin separating two classes***</span></span>

<span data-ttu-id="7011b-381">另一個產品的 Microsoft Research hello[二級局部深度 SVM](https://msdn.microsoft.com/library/azure/dn913070.aspx)是非線性 SVM 保留大部分的 hello hello 線性版本的速度和記憶體效率。</span><span class="sxs-lookup"><span data-stu-id="7011b-381">Another product of Microsoft Research, hello [two-class locally deep SVM](https://msdn.microsoft.com/library/azure/dn913070.aspx) is a non-linear variant of SVM that retains most of hello speed and memory efficiency of hello linear version.</span></span> <span data-ttu-id="7011b-382">理想的情況下 hello 線性方法不會授與足夠精確回應。</span><span class="sxs-lookup"><span data-stu-id="7011b-382">It is ideal for cases where hello linear approach doesn't give accurate enough answers.</span></span> <span data-ttu-id="7011b-383">hello 開發人員保持它快速分解成一堆線性的小型 SVM 問題 hello 問題。</span><span class="sxs-lookup"><span data-stu-id="7011b-383">hello developers kept it fast by breaking down hello problem into a bunch of small linear SVM problems.</span></span> <span data-ttu-id="7011b-384">讀取 hello[完整描述](http://research.microsoft.com/um/people/manik/pubs/Jose13.pdf)如 hello 它們提取關閉這個方法同樣的方式的詳細資訊。</span><span class="sxs-lookup"><span data-stu-id="7011b-384">Read hello [full description](http://research.microsoft.com/um/people/manik/pubs/Jose13.pdf) for hello details on how they pulled off this trick.</span></span>

<span data-ttu-id="7011b-385">使用的非線性 Svm 聰明的副檔名，hello[一級 SVM](https://msdn.microsoft.com/library/azure/dn913103.aspx)繪製緊密概述 hello 整個資料集的界限。</span><span class="sxs-lookup"><span data-stu-id="7011b-385">Using a clever extension of nonlinear SVMs, hello [one-class SVM](https://msdn.microsoft.com/library/azure/dn913103.aspx) draws a boundary that tightly outlines hello entire data set.</span></span> <span data-ttu-id="7011b-386">這適合用於異常偵測。</span><span class="sxs-lookup"><span data-stu-id="7011b-386">It is useful for anomaly detection.</span></span> <span data-ttu-id="7011b-387">遠超出該界限任何新資料點是不尋常夠 toobe 值得注意。</span><span class="sxs-lookup"><span data-stu-id="7011b-387">Any new data points that fall far outside that boundary are unusual enough toobe noteworthy.</span></span>

### <a name="bayesian-methods"></a><span data-ttu-id="7011b-388">貝氏方法</span><span class="sxs-lookup"><span data-stu-id="7011b-388">Bayesian methods</span></span>
<span data-ttu-id="7011b-389">貝氏方法具有令人滿意的高品質：可避免過度學習。</span><span class="sxs-lookup"><span data-stu-id="7011b-389">Bayesian methods have a highly desirable quality: they avoid overfitting.</span></span> <span data-ttu-id="7011b-390">他們這麼做，藉由一些假設事先 hello 回應 hello 可能分佈。</span><span class="sxs-lookup"><span data-stu-id="7011b-390">They do this by making some assumptions beforehand about hello likely distribution of hello answer.</span></span> <span data-ttu-id="7011b-391">這種方法的另一個附加好處在於其參數非常少。</span><span class="sxs-lookup"><span data-stu-id="7011b-391">Another byproduct of this approach is that they have very few parameters.</span></span> <span data-ttu-id="7011b-392">Azure 機器學習中的分類 ([雙類別貝氏點機器](https://msdn.microsoft.com/library/azure/dn905930.aspx)) 和迴歸 ([貝氏線性迴歸](https://msdn.microsoft.com/library/azure/dn906022.aspx)) 都各有貝氏演算法。</span><span class="sxs-lookup"><span data-stu-id="7011b-392">Azure Machine Learning has both Bayesian algorithms for both classification ([Two-class Bayes' point machine](https://msdn.microsoft.com/library/azure/dn905930.aspx)) and regression ([Bayesian linear regression](https://msdn.microsoft.com/library/azure/dn906022.aspx)).</span></span>
<span data-ttu-id="7011b-393">請注意，這些假設 hello 資料可以分割或配合一條直線。</span><span class="sxs-lookup"><span data-stu-id="7011b-393">Note that these assume that hello data can be split or fit with a straight line.</span></span>

<span data-ttu-id="7011b-394">在歷史記錄中，貝氏點機器是由 Microsoft Research 所開發。</span><span class="sxs-lookup"><span data-stu-id="7011b-394">On a historical note, Bayes' point machines were developed at Microsoft Research.</span></span> <span data-ttu-id="7011b-395">它們有一些格外出色的理論做為後盾。</span><span class="sxs-lookup"><span data-stu-id="7011b-395">They have some exceptionally beautiful theoretical work behind them.</span></span> <span data-ttu-id="7011b-396">hello 興趣的學生會導向的 toohello [JMLR 中的原始文章](http://jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf)和[見解部落格，Chris Bishop](http://blogs.technet.com/b/machinelearning/archive/2014/10/30/embracing-uncertainty-probabilistic-inference.aspx)。</span><span class="sxs-lookup"><span data-stu-id="7011b-396">hello interested student is directed toohello [original article in JMLR](http://jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf) and an [insightful blog by Chris Bishop](http://blogs.technet.com/b/machinelearning/archive/2014/10/30/embracing-uncertainty-probabilistic-inference.aspx).</span></span>

### <a name="specialized-algorithms"></a><span data-ttu-id="7011b-397">專門的演算法</span><span class="sxs-lookup"><span data-stu-id="7011b-397">Specialized algorithms</span></span>
<span data-ttu-id="7011b-398">如果您有非常特定的目標，那麼您的可能運氣特別好。</span><span class="sxs-lookup"><span data-stu-id="7011b-398">If you have a very specific goal you may be in luck.</span></span> <span data-ttu-id="7011b-399">Azure Machine Learning 集合 hello，有在特製化的演算法：</span><span class="sxs-lookup"><span data-stu-id="7011b-399">Within hello Azure Machine Learning collection, there are algorithms that specialize in:</span></span>

- <span data-ttu-id="7011b-400">排名預測 ([序數迴歸](https://msdn.microsoft.com/library/azure/dn906029.aspx))、</span><span class="sxs-lookup"><span data-stu-id="7011b-400">rank prediction ([ordinal regression](https://msdn.microsoft.com/library/azure/dn906029.aspx)),</span></span>
- <span data-ttu-id="7011b-401">計算預測 ([波氏迴歸](https://msdn.microsoft.com/library/azure/dn905988.aspx))、</span><span class="sxs-lookup"><span data-stu-id="7011b-401">count prediction ([Poisson regression](https://msdn.microsoft.com/library/azure/dn905988.aspx)),</span></span>
- <span data-ttu-id="7011b-402">異常偵測 (一個以[主體元件分析](https://msdn.microsoft.com/library/azure/dn913102.aspx)為基礎，一個以[支援向量機器](https://msdn.microsoft.com/library/azure/dn913103.aspx)為基礎)</span><span class="sxs-lookup"><span data-stu-id="7011b-402">anomaly detection (one based on [principal components analysis](https://msdn.microsoft.com/library/azure/dn913102.aspx) and one based on [support vector machine](https://msdn.microsoft.com/library/azure/dn913103.aspx)s)</span></span>
- <span data-ttu-id="7011b-403">叢集 ([K-means](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/))</span><span class="sxs-lookup"><span data-stu-id="7011b-403">clustering ([K-means](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/))</span></span>

![PCA 型異常偵測][8]

<span data-ttu-id="7011b-405">***以 PCA 為基礎的異常偵測*** *-hello 的大部分 hello 資料分成 stereotypical 發佈; 大幅偏離該發佈點都有疑問*</span><span class="sxs-lookup"><span data-stu-id="7011b-405">***PCA-based anomaly detection*** *- hello vast majority of hello data falls into a stereotypical distribution; points deviating dramatically from that distribution are suspect*</span></span>

![使用 K-means 分組的資料集][9]

<span data-ttu-id="7011b-407">***資料集使用 K-means 分為五個叢集***</span><span class="sxs-lookup"><span data-stu-id="7011b-407">***A data set is grouped into five clusters using K-means***</span></span>

<span data-ttu-id="7011b-408">另外還有一整團[一個 v 所有多級分類器](https://msdn.microsoft.com/library/azure/dn905887.aspx)，N-1 二級分類問題哪些符號 hello N 類別分類問題。</span><span class="sxs-lookup"><span data-stu-id="7011b-408">There is also an ensemble [one-v-all multiclass classifier](https://msdn.microsoft.com/library/azure/dn905887.aspx), which breaks hello N-class classification problem into N-1 two-class classification problems.</span></span> <span data-ttu-id="7011b-409">hello 精確度、 定型時間及線性屬性取決於使用 hello 二級分類器。</span><span class="sxs-lookup"><span data-stu-id="7011b-409">hello accuracy, training time, and linearity properties are determined by hello two-class classifiers used.</span></span>

![二級分類器結合 tooform 三個類別分類器][10]

<span data-ttu-id="7011b-411">***二級分類器的一組結合 tooform 三個類別分類器***</span><span class="sxs-lookup"><span data-stu-id="7011b-411">***A pair of two-class classifiers combine tooform a three-class classifier***</span></span>

<span data-ttu-id="7011b-412">Azure 機器學習也包含存取 tooa 功能強大的機器學習架構的 hello 標題底下[Vowpal Wabbit](https://msdn.microsoft.com/library/azure/8383eb49-c0a3-45db-95c8-eb56a1fef5bf)。</span><span class="sxs-lookup"><span data-stu-id="7011b-412">Azure Machine Learning also includes access tooa powerful machine learning framework under hello title of [Vowpal Wabbit](https://msdn.microsoft.com/library/azure/8383eb49-c0a3-45db-95c8-eb56a1fef5bf).</span></span>
<span data-ttu-id="7011b-413">VW 背離這裡的歸納，因為它可以學習分類和迴歸問題，甚至還能從一些沒有標記的資料中學習。</span><span class="sxs-lookup"><span data-stu-id="7011b-413">VW defies categorization here, since it can learn both classification and regression problems and can even learn from partially unlabeled data.</span></span> <span data-ttu-id="7011b-414">您可以設定 toouse 學習演算法、 遺失函式，以及最佳化演算法的數字的任何一個。</span><span class="sxs-lookup"><span data-stu-id="7011b-414">You can configure it toouse any one of a number of learning algorithms, loss functions, and optimization algorithms.</span></span> <span data-ttu-id="7011b-415">此設計是從 hello 地面向上 toobe 有效率、 平行、 且非常快速。</span><span class="sxs-lookup"><span data-stu-id="7011b-415">It was designed from hello ground up toobe efficient, parallel, and extremely fast.</span></span> <span data-ttu-id="7011b-416">它可以輕鬆處理大到不可思議的特徵集。</span><span class="sxs-lookup"><span data-stu-id="7011b-416">It handles ridiculously large feature sets with little apparent effort.</span></span>
<span data-ttu-id="7011b-417">由創辦 Microsoft Research 的 John Langford 所發起及領導的 VW，可謂原裝賽車演算法領域中的一級方程式項目。</span><span class="sxs-lookup"><span data-stu-id="7011b-417">Started and led by Microsoft Research's own John Langford, VW is a Formula One entry in a field of stock car algorithms.</span></span> <span data-ttu-id="7011b-418">並非所有問題都適合 VW，但如果有的話，它可能是非常值得 tooclimb 其介面上的學習曲線。</span><span class="sxs-lookup"><span data-stu-id="7011b-418">Not every problem fits VW, but if yours does, it may be worth your while tooclimb the learning curve on its interface.</span></span> <span data-ttu-id="7011b-419">它也有以數種語言提供 [獨立的開放原始程式碼](https://github.com/JohnLangford/vowpal_wabbit) 。</span><span class="sxs-lookup"><span data-stu-id="7011b-419">It's also available as [stand-alone open source code](https://github.com/JohnLangford/vowpal_wabbit) in several languages.</span></span>

## <a name="more-help-with-algorithms"></a><span data-ttu-id="7011b-420">更多演算法的詳細說明</span><span class="sxs-lookup"><span data-stu-id="7011b-420">More help with algorithms</span></span>
* <span data-ttu-id="7011b-421">如需描述演算法並提供範例的可下載資訊圖詳細資訊，請參閱[可下載的資訊圖：機器學習服務基本概念和演算法範例](machine-learning-basics-infographic-with-algorithm-examples.md)。</span><span class="sxs-lookup"><span data-stu-id="7011b-421">For a downloadable infographic that describes algorithms and provides examples, see [Downloadable Infographic: Machine learning basics with algorithm examples](machine-learning-basics-infographic-with-algorithm-examples.md).</span></span>
* <span data-ttu-id="7011b-422">如需依類別目錄可用 Azure Machine Learning Studio 中的所有 hello 機器學習演算法的清單，請參閱[初始化模型][ initialize-model] hello 機器學習 Studio 演算法和模組說明。</span><span class="sxs-lookup"><span data-stu-id="7011b-422">For a list by category of all hello machine learning algorithms available in Azure Machine Learning Studio, see [Initialize Model][initialize-model] in hello Machine Learning Studio Algorithm and Module Help.</span></span>
* <span data-ttu-id="7011b-423">如需 Azure Machine Learning Studio 中按字母順序排列的完整演算法和模組清單，請參閱＜Machine Learning Studio 演算法和模組說明＞中的 [Machine Learning Studio 模組的 A-Z 清單][a-z-list]。</span><span class="sxs-lookup"><span data-stu-id="7011b-423">For a complete alphabetical list of algorithms and modules in Azure Machine Learning Studio, see [A-Z list of Machine Learning Studio modules][a-z-list] in Machine Learning Studio Algorithm and Module Help.</span></span>
* <span data-ttu-id="7011b-424">toodownload 及列印一張圖表，hello Azure Machine Learning Studio 中，功能的概觀，請參閱[Azure Machine Learning Studio 功能的概觀圖表](machine-learning-studio-overview-diagram.md)。</span><span class="sxs-lookup"><span data-stu-id="7011b-424">toodownload and print a diagram that gives an overview of hello capabilities of Azure Machine Learning Studio, see [Overview diagram of Azure Machine Learning Studio capabilities](machine-learning-studio-overview-diagram.md).</span></span>


<!-- Reference links -->
[initialize-model]: https://msdn.microsoft.com/library/azure/dn905812.aspx
[a-z-list]: https://msdn.microsoft.com/library/azure/dn906033.aspx

<!-- Media -->

[1]: ./media/machine-learning-algorithm-choice/image1.png
[2]: ./media/machine-learning-algorithm-choice/image2.png
[3]: ./media/machine-learning-algorithm-choice/image3.png
[4]: ./media/machine-learning-algorithm-choice/image4.png
[5]: ./media/machine-learning-algorithm-choice/image5.png
[6]: ./media/machine-learning-algorithm-choice/image6.png
[7]: ./media/machine-learning-algorithm-choice/image7.png
[8]: ./media/machine-learning-algorithm-choice/image8.png
[9]: ./media/machine-learning-algorithm-choice/image9.png
[10]: ./media/machine-learning-algorithm-choice/image10.png
