---
title: "在 Azure HDInsight 上使用 Spark 的資料科學概觀 | Microsoft Docs"
description: "Spark MLlib 工具組將可觀的機器學習模型化功能引進分散式 HDInsight 環境中。"
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: a4e1de99-a554-4240-9647-2c6d669593c8
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 03/15/2017
ms.author: deguhath;bradsev;gokuma
ms.openlocfilehash: 379b32f4e533f48f1593a97e73737a0c5bfb9135
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 07/11/2017
---
# <a name="overview-of-data-science-using-spark-on-azure-hdinsight"></a><span data-ttu-id="37fcd-103">在 Azure HDInsight 上使用 Spark 的資料科學概觀</span><span class="sxs-lookup"><span data-stu-id="37fcd-103">Overview of data science using Spark on Azure HDInsight</span></span>
[!INCLUDE [machine-learning-spark-modeling](../../includes/machine-learning-spark-modeling.md)]

<span data-ttu-id="37fcd-104">這組主題說明如何使用 HDInsight Spark 來完成常見的資料科學工作，例如資料擷取、特徵工程、模型化和模型評估。</span><span class="sxs-lookup"><span data-stu-id="37fcd-104">This suite of topics shows how to use HDInsight Spark to complete common data science tasks such as data ingestion, feature engineering, modeling, and model evaluation.</span></span> <span data-ttu-id="37fcd-105">所使用的資料是 2013 NYC 計程車車程和費用資料集的抽樣樣本。</span><span class="sxs-lookup"><span data-stu-id="37fcd-105">The data used is a sample of the 2013 NYC taxi trip and fare dataset.</span></span> <span data-ttu-id="37fcd-106">建立的模型包括羅吉斯和線性迴歸、隨機樹系和漸層停駐推進式決策樹。</span><span class="sxs-lookup"><span data-stu-id="37fcd-106">The models built include logistic and linear regression, random forests, and gradient boosted trees.</span></span> <span data-ttu-id="37fcd-107">這些主題也會說明如何將這些模型儲存至 Azure Blob 儲存體 (WASB)，以及如何評分及評估模型的預測效能。</span><span class="sxs-lookup"><span data-stu-id="37fcd-107">The topics also show how to store these models in Azure blob storage (WASB) and how to score and evaluate their predictive performance.</span></span> <span data-ttu-id="37fcd-108">更進階的主題會討論如何使用交叉驗證和超參數掃掠來訓練模型。</span><span class="sxs-lookup"><span data-stu-id="37fcd-108">More advanced topics cover how models can be trained using cross-validation and hyper-parameter sweeping.</span></span> <span data-ttu-id="37fcd-109">此概觀主題也參考說明了如何設定所需的 Spark 叢集，以完成逐步解說中的步驟的主題。</span><span class="sxs-lookup"><span data-stu-id="37fcd-109">This overview topic also references the topics that describe how to set up the Spark cluster that you need to complete the steps in the walkthroughs provided.</span></span> 

## <a name="spark-and-mllib"></a><span data-ttu-id="37fcd-110">Spark 及 MLlib</span><span class="sxs-lookup"><span data-stu-id="37fcd-110">Spark and MLlib</span></span>
<span data-ttu-id="37fcd-111">[Spark](http://spark.apache.org/) 是一個開放原始碼平行處理架構，可支援記憶體內部處理，大幅提升巨量資料分析應用程式的效能。</span><span class="sxs-lookup"><span data-stu-id="37fcd-111">[Spark](http://spark.apache.org/) is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.</span></span> <span data-ttu-id="37fcd-112">Spark 處理引擎是專為速度、易用性及精密分析打造的產品。</span><span class="sxs-lookup"><span data-stu-id="37fcd-112">The Spark processing engine is built for speed, ease of use, and sophisticated analytics.</span></span> <span data-ttu-id="37fcd-113">Spark 的記憶體內分散式計算功能，使其成為機器學習和圖表計算中所使用反覆演算法的絕佳選擇。</span><span class="sxs-lookup"><span data-stu-id="37fcd-113">Spark's in-memory distributed computation capabilities make it a good choice for the iterative algorithms used in machine learning and graph computations.</span></span> <span data-ttu-id="37fcd-114">[MLlib](http://spark.apache.org/mllib/) 是將演算法模型化功能引進此分散式環境的 Spark 可調整機器學習程式庫。</span><span class="sxs-lookup"><span data-stu-id="37fcd-114">[MLlib](http://spark.apache.org/mllib/) is Spark's scalable machine learning library that brings the algorithmic modeling capabilities to this distributed environment.</span></span> 

## <a name="hdinsight-spark"></a><span data-ttu-id="37fcd-115">HDInsight Spark</span><span class="sxs-lookup"><span data-stu-id="37fcd-115">HDInsight Spark</span></span>
<span data-ttu-id="37fcd-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) 是開放原始碼 Spark 的 Azure 託管服務。</span><span class="sxs-lookup"><span data-stu-id="37fcd-116">[HDInsight Spark](../hdinsight/hdinsight-apache-spark-overview.md) is the Azure hosted offering of open-source Spark.</span></span> <span data-ttu-id="37fcd-117">它也支援 Spark 叢集上的 **Jupyter PySpark notebooks**，可執行 Spark SQL 互動式查詢以轉換、篩選和視覺化 Azure Blob (WASB) 中儲存的資料。</span><span class="sxs-lookup"><span data-stu-id="37fcd-117">It also includes support for **Jupyter PySpark notebooks** on the Spark cluster that can run Spark SQL interactive queries for transforming, filtering, and visualizing data stored in Azure Blobs (WASB).</span></span> <span data-ttu-id="37fcd-118">PySpark 是適用於 Spark 的 Python API。</span><span class="sxs-lookup"><span data-stu-id="37fcd-118">PySpark is the Python API for Spark.</span></span> <span data-ttu-id="37fcd-119">程式碼片段提供了解決方案，並且顯示相關的繪圖，進而將安裝在 Spark 叢集上的 Jupyter Notebook 資料加以視覺化。</span><span class="sxs-lookup"><span data-stu-id="37fcd-119">The code snippets that provide the solutions and show the relevant plots to visualize the data here run in Jupyter notebooks installed on the Spark clusters.</span></span> <span data-ttu-id="37fcd-120">這些主題中的模型化步驟包括程式碼，以示範如何訓練、評估、儲存和使用各類模型。</span><span class="sxs-lookup"><span data-stu-id="37fcd-120">The modeling steps in these topics contain code that shows how to train, evaluate, save, and consume each type of model.</span></span> 

## <a name="setup-spark-clusters-and-jupyter-notebooks"></a><span data-ttu-id="37fcd-121">設定：Spark 叢集和 Jupyter Notebook</span><span class="sxs-lookup"><span data-stu-id="37fcd-121">Setup: Spark clusters and Jupyter notebooks</span></span>
<span data-ttu-id="37fcd-122">此逐步解說所提供的設定步驟和程式碼適用於使用 HDInsight Spark 1.6。</span><span class="sxs-lookup"><span data-stu-id="37fcd-122">Setup steps and code are provided in this walkthrough for using an HDInsight Spark 1.6.</span></span> <span data-ttu-id="37fcd-123">不過 Jupyter Notebook 可供 HDInsight Spark 1.6 版和 Spark 2.0 叢集兩者使用。</span><span class="sxs-lookup"><span data-stu-id="37fcd-123">But Jupyter notebooks are provided for both HDInsight Spark 1.6 and Spark 2.0 clusters.</span></span> <span data-ttu-id="37fcd-124">Notebook 的描述及它們的連結已在包含它們的 GitHub 儲存機制的 [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md) 中提供。</span><span class="sxs-lookup"><span data-stu-id="37fcd-124">A description of the notebooks and links to them are provided in the [Readme.md](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Readme.md) for the GitHub repository containing them.</span></span> <span data-ttu-id="37fcd-125">此外，此處及連結的 Notebook 內的程式碼皆屬泛型程式碼，而且應該能在任何 Spark 叢集上運作。</span><span class="sxs-lookup"><span data-stu-id="37fcd-125">Moreover, the code here and in the linked notebooks is generic and should work on any Spark cluster.</span></span> <span data-ttu-id="37fcd-126">若您不是使用 HDInsight Spark，叢集設定和管理步驟可能與這裡顯示的稍有不同。</span><span class="sxs-lookup"><span data-stu-id="37fcd-126">If you are not using HDInsight Spark, the cluster setup and management steps may be slightly different from what is shown here.</span></span> <span data-ttu-id="37fcd-127">為了方便起見，以下是可讓 Spark 1.6 版 (在 Jupyter Notebook 伺服器的 pySpark 核心中執行) 和 Spark 2.0 版 (在 Jupyter Notebook 伺服器的 pySpark3 核心中執行) 的 Jupyter Notebook 連結：</span><span class="sxs-lookup"><span data-stu-id="37fcd-127">For convenience, here are the links to the Jupyter notebooks for Spark 1.6 (to be run in the pySpark kernel of the Jupyter Notebook server) and  Spark 2.0 (to be run in the pySpark3 kernel of the Jupyter Notebook server):</span></span>

### <a name="spark-16-notebooks"></a><span data-ttu-id="37fcd-128">Spark 1.6 Notebook</span><span class="sxs-lookup"><span data-stu-id="37fcd-128">Spark 1.6 notebooks</span></span>
<span data-ttu-id="37fcd-129">這些 Notebook 是在 Jupyter Notebook 伺服器的 pySpark 核心中執行。</span><span class="sxs-lookup"><span data-stu-id="37fcd-129">These notebooks are to be run in the pySpark kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="37fcd-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb)：提供如何利用數個不同的演算法來執行資料瀏覽、模型化和評分的相關資訊。</span><span class="sxs-lookup"><span data-stu-id="37fcd-130">[pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-data-exploration-modeling.ipynb): Provides information on how to perform data exploration, modeling, and scoring with several different algorithms.</span></span>
- <span data-ttu-id="37fcd-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb)：包含Notebook #1 中的主題，以及使用超參數微調和交叉驗證的模型開發。</span><span class="sxs-lookup"><span data-stu-id="37fcd-131">[pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): Includes topics in notebook #1, and model development using hyperparameter tuning and cross-validation.</span></span>
- <span data-ttu-id="37fcd-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb)：示範如何在 HDInsight 叢集上使用 Python 將儲存的模型實際運作。</span><span class="sxs-lookup"><span data-stu-id="37fcd-132">[pySpark-machine-learning-data-science-spark-model-consumption.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb): Shows how to operationalize a saved model using Python on HDInsight clusters.</span></span>

### <a name="spark-20-notebooks"></a><span data-ttu-id="37fcd-133">Spark 2.0 Notebook</span><span class="sxs-lookup"><span data-stu-id="37fcd-133">Spark 2.0 notebooks</span></span>
<span data-ttu-id="37fcd-134">這些 Notebook 是在 Jupyter Notebook 伺服器的 pySpark3 核心中執行。</span><span class="sxs-lookup"><span data-stu-id="37fcd-134">These notebooks are to be run in the pySpark3 kernel of Jupyter notebook server.</span></span>

- <span data-ttu-id="37fcd-135">[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb)：此檔案使用在[這裡](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data)描述的 NYC 計程車車程和費用資料集，提供如何在 Spark 2.0 叢集中執行資料瀏覽、模型化和評分的相關資訊。</span><span class="sxs-lookup"><span data-stu-id="37fcd-135">[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb): This file provides information on how to perform data exploration, modeling, and scoring in Spark 2.0 clusters using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span> <span data-ttu-id="37fcd-136">Notebook 可能是很好的起點，可快速瀏覽我們針對 Spark 2.0 所提供的程式碼。</span><span class="sxs-lookup"><span data-stu-id="37fcd-136">This notebook may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> <span data-ttu-id="37fcd-137">如需更多分析 NYC 計程車資料的 Notebook 詳細資訊，請參閱這份清單中的下一個 Notebook。</span><span class="sxs-lookup"><span data-stu-id="37fcd-137">For a more detailed notebook analyzes the NYC Taxi data, see the next notebook in this list.</span></span> <span data-ttu-id="37fcd-138">請參閱此清單之後比較這些 Notebook 的附註。</span><span class="sxs-lookup"><span data-stu-id="37fcd-138">See the notes following this list that compare these notebooks.</span></span> 
- <span data-ttu-id="37fcd-139">[Spark2.0 pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb)：這個檔案會顯示如何使用[這裡](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data)所述的 NYC 計程車車程和費用資料集，執行資料爭議 (Spark SQL 和資料框架作業)、瀏覽、模型化和評分。</span><span class="sxs-lookup"><span data-stu-id="37fcd-139">[Spark2.0-pySpark3_NYC_Taxi_Tip_Regression.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_NYC_Taxi_Tip_Regression.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the NYC Taxi trip and fare data-set described [here](https://docs.microsoft.com/en-us/azure/machine-learning/machine-learning-data-science-spark-overview#the-nyc-2013-taxi-data).</span></span>
- <span data-ttu-id="37fcd-140">[Spark2.0 pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb)：這個檔案會顯示如何使用已知的 2011 年和 2012 年航班準時出發資料集，執行資料爭議 (Spark SQL 和資料框架作業)、瀏覽、模型化和評分。</span><span class="sxs-lookup"><span data-stu-id="37fcd-140">[Spark2.0-pySpark3_Airline_Departure_Delay_Classification.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0_pySpark3_Airline_Departure_Delay_Classification.ipynb): This file shows how to perform data wrangling (Spark SQL and dataframe operations), exploration, modeling and scoring using the well-known Airline On-time departure dataset from 2011 and 2012.</span></span> <span data-ttu-id="37fcd-141">我們在模型化之前將航班資料集與機場天氣資料 (例如風速、溫度、高度等等) 整合，因此可以在模型中包含這些天氣功能。</span><span class="sxs-lookup"><span data-stu-id="37fcd-141">We integrated the airline dataset with the airport weather data (e.g. windspeed, temperature, altitude etc.) prior to modeling, so these weather features can be included in the model.</span></span>

<!-- -->

> [!NOTE]
> <span data-ttu-id="37fcd-142">Spark 2.0 Notebook 中新增了航班資料集，以更清楚地說明使用的分類演算法。</span><span class="sxs-lookup"><span data-stu-id="37fcd-142">The airline dataset was added to the Spark 2.0 notebooks to better illustrate the use of classification algorithms.</span></span> <span data-ttu-id="37fcd-143">請參閱下列連結，以取得航班準時出發資料集和天氣資料集的相關資訊：</span><span class="sxs-lookup"><span data-stu-id="37fcd-143">See the following links for information about airline on-time departure dataset and weather dataset:</span></span>

>- <span data-ttu-id="37fcd-144">航班準時出發資料：[http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span><span class="sxs-lookup"><span data-stu-id="37fcd-144">Airline on-time departure data: [http://www.transtats.bts.gov/ONTIME/](http://www.transtats.bts.gov/ONTIME/)</span></span>

>- <span data-ttu-id="37fcd-145">機場天氣資料：[https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span><span class="sxs-lookup"><span data-stu-id="37fcd-145">Airport weather data: [https://www.ncdc.noaa.gov/](https://www.ncdc.noaa.gov/)</span></span> 
> 
> 

<!-- -->

<!-- -->

> [!NOTE]
<span data-ttu-id="37fcd-146">NYC 計程車和飛行航班延遲資料集上的 Spark 2.0 Notebook 需要 10 分鐘或更久 (取決於 HDI 叢集的大小) 才能執行。</span><span class="sxs-lookup"><span data-stu-id="37fcd-146">The Spark 2.0 notebooks on the NYC taxi and airline flight delay data-sets can take 10 mins or more to run (depending on the size of your HDI cluster).</span></span> <span data-ttu-id="37fcd-147">上述清單中的第一個 Notebook 說明 Notebook 中許多層面的資料瀏覽、視覺效果和 ML 模型訓練，會使用向下取樣 NYC 資料集以較短時間執行，其中計程車和車資檔案已預先聯結︰[Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb)此 Notebook 會採用較短的時間來完成 (2-3 分鐘)，並可能是快速瀏覽我們針對 Spark 2.0 所提供之程式碼的一個良好起點。</span><span class="sxs-lookup"><span data-stu-id="37fcd-147">The first notebook in the above list shows many aspects of the data exploration, visualization and ML model training in a notebook that takes less time to run with down-sampled NYC data set, in which the taxi and fare files have been pre-joined: [Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark2.0/Spark2.0-pySpark3-machine-learning-data-science-spark-advanced-data-exploration-modeling.ipynb) This notebook takes a much shorter time to finish (2-3 mins) and may be a good starting point for quickly exploring the code we have provided for Spark 2.0.</span></span> 

<!-- -->

<span data-ttu-id="37fcd-148">如需 Spark 2.0 模型和評分的模型耗用量實際運作的相關指引，請參閱[有關耗用量的 Spark 1.6 文件](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb)，以取得概述所需步驟的範例。</span><span class="sxs-lookup"><span data-stu-id="37fcd-148">For guidance on the operationalization of a Spark 2.0 model and model consumption for scoring, see the [Spark 1.6 document on consumption](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/pySpark/Spark1.6/pySpark-machine-learning-data-science-spark-model-consumption.ipynb) for an example outlining the steps required.</span></span> <span data-ttu-id="37fcd-149">若要在 Spark 2.0 上使用此檔案，將使用[這個檔案](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py)來取代 Python 程式碼。</span><span class="sxs-lookup"><span data-stu-id="37fcd-149">To use this on Spark 2.0, replace the Python code file with [this file](https://github.com/Azure/Azure-MachineLearning-DataScience/blob/master/Misc/Spark/Python/Spark2.0_ConsumeRFCV_NYCReg.py).</span></span>

### <a name="prerequisites"></a><span data-ttu-id="37fcd-150">必要條件</span><span class="sxs-lookup"><span data-stu-id="37fcd-150">Prerequisites</span></span>
<span data-ttu-id="37fcd-151">下列程序與 Spark 1.6 相關。</span><span class="sxs-lookup"><span data-stu-id="37fcd-151">The following procedures are related to Spark 1.6.</span></span> <span data-ttu-id="37fcd-152">對於 Spark 2.0 版本，請使用先前說明和連結的 Notebook。</span><span class="sxs-lookup"><span data-stu-id="37fcd-152">For  the Spark 2.0 version, use the notebooks described and linked to previously.</span></span> 

<span data-ttu-id="37fcd-153">1. 您必須擁有 Azure 訂用帳戶。</span><span class="sxs-lookup"><span data-stu-id="37fcd-153">1.You must have an Azure subscription.</span></span> <span data-ttu-id="37fcd-154">如果還沒有訂用帳戶，請參閱 [取得 Azure 免費試用](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)。</span><span class="sxs-lookup"><span data-stu-id="37fcd-154">If you do not already have one, see [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).</span></span>

<span data-ttu-id="37fcd-155">2. 您需要 Spark 1.6 叢集才能完成這個逐步解說。</span><span class="sxs-lookup"><span data-stu-id="37fcd-155">2.You need a Spark 1.6 cluster to complete this walkthrough.</span></span> <span data-ttu-id="37fcd-156">若要建立該項目，請參閱 [開始使用：在 Azure HDInsight 上建立 Apache Spark](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md)所提供的指示。</span><span class="sxs-lookup"><span data-stu-id="37fcd-156">To create one, see the instructions provided in [Get started: create Apache Spark on Azure HDInsight](../hdinsight/hdinsight-apache-spark-jupyter-spark-sql.md).</span></span> <span data-ttu-id="37fcd-157">叢集類型和版本是由 [選取叢集類型]  功能表來指定。</span><span class="sxs-lookup"><span data-stu-id="37fcd-157">The cluster type and version is specified from the **Select Cluster Type** menu.</span></span> 

![設定叢集](./media/machine-learning-data-science-spark-overview/spark-cluster-on-portal.png)

<!-- -->

> [!NOTE]
> <span data-ttu-id="37fcd-159">如需示範如何使用 Scala 而非 Python 完成端對端資料科學程序工作的主題，請參閱 [在 Azure 上使用 Spark 與 Spark 的資料科學](machine-learning-data-science-process-scala-walkthrough.md)。</span><span class="sxs-lookup"><span data-stu-id="37fcd-159">For a topic that shows how to use Scala rather than Python to complete tasks for an end-to-end data science process, see the [Data Science using Scala with Spark on Azure](machine-learning-data-science-process-scala-walkthrough.md).</span></span>
> 
> 

<!-- -->

> [!INCLUDE [delete-cluster-warning](../../includes/hdinsight-delete-cluster-warning.md)]
> 
> 

## <a name="the-nyc-2013-taxi-data"></a><span data-ttu-id="37fcd-160">NYC 2013 計程車資料</span><span class="sxs-lookup"><span data-stu-id="37fcd-160">The NYC 2013 Taxi data</span></span>
<span data-ttu-id="37fcd-161">「NYC 計程車車程」資料大約是 20GB 以逗號分隔值 (CSV) 的壓縮檔 (未壓縮時可達 48GB)，其中包含超過 1 億 7300 萬筆個別車程及針對每趟車程支付的費用。</span><span class="sxs-lookup"><span data-stu-id="37fcd-161">The NYC Taxi Trip data is about 20 GB of compressed comma-separated values (CSV) files (~48 GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip.</span></span> <span data-ttu-id="37fcd-162">每趟車程記錄包括上車和下車的位置與時間、匿名的計程車司機駕照號碼，以及圓形徽章 (計程車的唯一識別碼) 號碼。</span><span class="sxs-lookup"><span data-stu-id="37fcd-162">Each trip record includes the pick up and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi’s unique id) number.</span></span> <span data-ttu-id="37fcd-163">資料涵蓋 2013 年的所有車程，並且每月會在下列兩個資料集中加以提供：</span><span class="sxs-lookup"><span data-stu-id="37fcd-163">The data covers all trips in the year 2013 and is provided in the following two datasets for each month:</span></span>

1. <span data-ttu-id="37fcd-164">'trip_data' CSV 檔案包含車程詳細資訊，例如乘客數、上車和下車地點、車程持續時間，以及車程長度。</span><span class="sxs-lookup"><span data-stu-id="37fcd-164">The 'trip_data' CSV files contain trip details, such as number of passengers, pick up and dropoff points, trip duration, and trip length.</span></span> <span data-ttu-id="37fcd-165">以下是一些範例記錄：</span><span class="sxs-lookup"><span data-stu-id="37fcd-165">Here are a few sample records:</span></span>
   
        medallion,hack_license,vendor_id,rate_code,store_and_fwd_flag,pickup_datetime,dropoff_datetime,passenger_count,trip_time_in_secs,trip_distance,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,1,N,2013-01-01 15:11:48,2013-01-01 15:18:10,4,382,1.00,-73.978165,40.757977,-73.989838,40.751171
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-06 00:18:35,2013-01-06 00:22:54,1,259,1.50,-74.006683,40.731781,-73.994499,40.75066
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-05 18:49:41,2013-01-05 18:54:23,1,282,1.10,-74.004707,40.73777,-74.009834,40.726002
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:54:15,2013-01-07 23:58:20,2,244,.70,-73.974602,40.759945,-73.984734,40.759388
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:25:03,2013-01-07 23:34:24,1,560,2.10,-73.97625,40.748528,-74.002586,40.747868
2. <span data-ttu-id="37fcd-166">'trip_data' CSV 檔案包含針對每趟車程所支付的費用詳細資料，例如付款類型、費用金額、附加費和稅金、小費和通行費，以及支付的總金額。</span><span class="sxs-lookup"><span data-stu-id="37fcd-166">The 'trip_fare' CSV files contain details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid.</span></span> <span data-ttu-id="37fcd-167">以下是一些範例記錄：</span><span class="sxs-lookup"><span data-stu-id="37fcd-167">Here are a few sample records:</span></span>
   
        medallion, hack_license, vendor_id, pickup_datetime, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount
        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,2013-01-01 15:11:48,CSH,6.5,0,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-06 00:18:35,CSH,6,0.5,0.5,0,0,7
        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-05 18:49:41,CSH,5.5,1,0.5,0,0,7
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:54:15,CSH,5,0.5,0.5,0,0,6
        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:25:03,CSH,9.5,0.5,0.5,0,0,10.5

<span data-ttu-id="37fcd-168">我們已取得這些檔案的 0.1% 樣本，並將 trip\_data 和 trip\_fare CSV 檔聯結成單一資料集，做為此逐步解說的輸入資料集。</span><span class="sxs-lookup"><span data-stu-id="37fcd-168">We have taken a 0.1% sample of these files and joined the trip\_data and trip\_fare CVS files into a single dataset to use as the input dataset for this walkthrough.</span></span> <span data-ttu-id="37fcd-169">聯結 trip\_data 和 trip\_fare 的唯一索引鍵是由下列欄位組成：medallion、hack\_licence、pickup\_datetime。</span><span class="sxs-lookup"><span data-stu-id="37fcd-169">The unique key to join trip\_data and trip\_fare is composed of the fields: medallion, hack\_licence and pickup\_datetime.</span></span> <span data-ttu-id="37fcd-170">資料集的每一筆記錄會包含代表 NYC 計程車車程的下列屬性︰</span><span class="sxs-lookup"><span data-stu-id="37fcd-170">Each record of the dataset contains the following attributes representing a NYC Taxi trip:</span></span>

| <span data-ttu-id="37fcd-171">欄位</span><span class="sxs-lookup"><span data-stu-id="37fcd-171">Field</span></span> | <span data-ttu-id="37fcd-172">簡短描述</span><span class="sxs-lookup"><span data-stu-id="37fcd-172">Brief Description</span></span> |
| --- | --- |
| <span data-ttu-id="37fcd-173">medallion</span><span class="sxs-lookup"><span data-stu-id="37fcd-173">medallion</span></span> |<span data-ttu-id="37fcd-174">匿名的計程車圓形徽章 (唯一的計程車識別碼)</span><span class="sxs-lookup"><span data-stu-id="37fcd-174">Anonymized taxi medallion (unique taxi id)</span></span> |
| <span data-ttu-id="37fcd-175">hack_license</span><span class="sxs-lookup"><span data-stu-id="37fcd-175">hack_license</span></span> |<span data-ttu-id="37fcd-176">匿名的 Hackney 歸位字元駕照號碼</span><span class="sxs-lookup"><span data-stu-id="37fcd-176">Anonymized Hackney Carriage License number</span></span> |
| <span data-ttu-id="37fcd-177">vendor_id</span><span class="sxs-lookup"><span data-stu-id="37fcd-177">vendor_id</span></span> |<span data-ttu-id="37fcd-178">計程車廠商識別碼</span><span class="sxs-lookup"><span data-stu-id="37fcd-178">Taxi vendor id</span></span> |
| <span data-ttu-id="37fcd-179">rate_code</span><span class="sxs-lookup"><span data-stu-id="37fcd-179">rate_code</span></span> |<span data-ttu-id="37fcd-180">NYC 計程車費率</span><span class="sxs-lookup"><span data-stu-id="37fcd-180">NYC taxi rate of fare</span></span> |
| <span data-ttu-id="37fcd-181">store_and_fwd_flag</span><span class="sxs-lookup"><span data-stu-id="37fcd-181">store_and_fwd_flag</span></span> |<span data-ttu-id="37fcd-182">儲存和轉寄旗標</span><span class="sxs-lookup"><span data-stu-id="37fcd-182">Store and forward flag</span></span> |
| <span data-ttu-id="37fcd-183">pickup_datetime</span><span class="sxs-lookup"><span data-stu-id="37fcd-183">pickup_datetime</span></span> |<span data-ttu-id="37fcd-184">上車日期和時間</span><span class="sxs-lookup"><span data-stu-id="37fcd-184">Pick up date & time</span></span> |
| <span data-ttu-id="37fcd-185">dropoff_datetime</span><span class="sxs-lookup"><span data-stu-id="37fcd-185">dropoff_datetime</span></span> |<span data-ttu-id="37fcd-186">下車日期和時間</span><span class="sxs-lookup"><span data-stu-id="37fcd-186">Dropoff date & time</span></span> |
| <span data-ttu-id="37fcd-187">pickup_hour</span><span class="sxs-lookup"><span data-stu-id="37fcd-187">pickup_hour</span></span> |<span data-ttu-id="37fcd-188">於幾點上車</span><span class="sxs-lookup"><span data-stu-id="37fcd-188">Pick up hour</span></span> |
| <span data-ttu-id="37fcd-189">pickup_week</span><span class="sxs-lookup"><span data-stu-id="37fcd-189">pickup_week</span></span> |<span data-ttu-id="37fcd-190">於一年中的第幾星期上車</span><span class="sxs-lookup"><span data-stu-id="37fcd-190">Pick up week of the year</span></span> |
| <span data-ttu-id="37fcd-191">weekday</span><span class="sxs-lookup"><span data-stu-id="37fcd-191">weekday</span></span> |<span data-ttu-id="37fcd-192">工作日 (範圍 1-7)</span><span class="sxs-lookup"><span data-stu-id="37fcd-192">Weekday (range 1-7)</span></span> |
| <span data-ttu-id="37fcd-193">passenger_count</span><span class="sxs-lookup"><span data-stu-id="37fcd-193">passenger_count</span></span> |<span data-ttu-id="37fcd-194">計程車車程的乘客數目</span><span class="sxs-lookup"><span data-stu-id="37fcd-194">Number of passengers in a taxi trip</span></span> |
| <span data-ttu-id="37fcd-195">trip_time_in_secs</span><span class="sxs-lookup"><span data-stu-id="37fcd-195">trip_time_in_secs</span></span> |<span data-ttu-id="37fcd-196">往返時間 (秒)</span><span class="sxs-lookup"><span data-stu-id="37fcd-196">Trip time in seconds</span></span> |
| <span data-ttu-id="37fcd-197">trip_distance</span><span class="sxs-lookup"><span data-stu-id="37fcd-197">trip_distance</span></span> |<span data-ttu-id="37fcd-198">以英哩計的車程距離</span><span class="sxs-lookup"><span data-stu-id="37fcd-198">Trip distance traveled in miles</span></span> |
| <span data-ttu-id="37fcd-199">pickup_longitude</span><span class="sxs-lookup"><span data-stu-id="37fcd-199">pickup_longitude</span></span> |<span data-ttu-id="37fcd-200">上車處經度</span><span class="sxs-lookup"><span data-stu-id="37fcd-200">Pick up longitude</span></span> |
| <span data-ttu-id="37fcd-201">pickup_latitude</span><span class="sxs-lookup"><span data-stu-id="37fcd-201">pickup_latitude</span></span> |<span data-ttu-id="37fcd-202">上車處緯度</span><span class="sxs-lookup"><span data-stu-id="37fcd-202">Pick up latitude</span></span> |
| <span data-ttu-id="37fcd-203">dropoff_longitude</span><span class="sxs-lookup"><span data-stu-id="37fcd-203">dropoff_longitude</span></span> |<span data-ttu-id="37fcd-204">下車經度</span><span class="sxs-lookup"><span data-stu-id="37fcd-204">Dropoff longitude</span></span> |
| <span data-ttu-id="37fcd-205">dropoff_latitude</span><span class="sxs-lookup"><span data-stu-id="37fcd-205">dropoff_latitude</span></span> |<span data-ttu-id="37fcd-206">下車緯度</span><span class="sxs-lookup"><span data-stu-id="37fcd-206">Dropoff latitude</span></span> |
| <span data-ttu-id="37fcd-207">direct_distance</span><span class="sxs-lookup"><span data-stu-id="37fcd-207">direct_distance</span></span> |<span data-ttu-id="37fcd-208">上車與下車位置之間的直線距離</span><span class="sxs-lookup"><span data-stu-id="37fcd-208">Direct distance between pick up and dropoff locations</span></span> |
| <span data-ttu-id="37fcd-209">payment_type</span><span class="sxs-lookup"><span data-stu-id="37fcd-209">payment_type</span></span> |<span data-ttu-id="37fcd-210">付款類型 (cas、信用卡等)</span><span class="sxs-lookup"><span data-stu-id="37fcd-210">Payment type (cas, credit-card etc.)</span></span> |
| <span data-ttu-id="37fcd-211">fare_amount</span><span class="sxs-lookup"><span data-stu-id="37fcd-211">fare_amount</span></span> |<span data-ttu-id="37fcd-212">費用金額</span><span class="sxs-lookup"><span data-stu-id="37fcd-212">Fare amount in</span></span> |
| <span data-ttu-id="37fcd-213">surcharge</span><span class="sxs-lookup"><span data-stu-id="37fcd-213">surcharge</span></span> |<span data-ttu-id="37fcd-214">額外費用</span><span class="sxs-lookup"><span data-stu-id="37fcd-214">Surcharge</span></span> |
| <span data-ttu-id="37fcd-215">mta_tax</span><span class="sxs-lookup"><span data-stu-id="37fcd-215">mta_tax</span></span> |<span data-ttu-id="37fcd-216">Mta 稅額</span><span class="sxs-lookup"><span data-stu-id="37fcd-216">Mta tax</span></span> |
| <span data-ttu-id="37fcd-217">tip_amount</span><span class="sxs-lookup"><span data-stu-id="37fcd-217">tip_amount</span></span> |<span data-ttu-id="37fcd-218">小費金額</span><span class="sxs-lookup"><span data-stu-id="37fcd-218">Tip amount</span></span> |
| <span data-ttu-id="37fcd-219">tolls_amount</span><span class="sxs-lookup"><span data-stu-id="37fcd-219">tolls_amount</span></span> |<span data-ttu-id="37fcd-220">收費金額</span><span class="sxs-lookup"><span data-stu-id="37fcd-220">Tolls amount</span></span> |
| <span data-ttu-id="37fcd-221">total_amount</span><span class="sxs-lookup"><span data-stu-id="37fcd-221">total_amount</span></span> |<span data-ttu-id="37fcd-222">總金額</span><span class="sxs-lookup"><span data-stu-id="37fcd-222">Total amount</span></span> |
| <span data-ttu-id="37fcd-223">tipped</span><span class="sxs-lookup"><span data-stu-id="37fcd-223">tipped</span></span> |<span data-ttu-id="37fcd-224">已收到小費 (用 0 或 1 表示否或是)</span><span class="sxs-lookup"><span data-stu-id="37fcd-224">Tipped (0/1 for no or yes)</span></span> |
| <span data-ttu-id="37fcd-225">tip_class</span><span class="sxs-lookup"><span data-stu-id="37fcd-225">tip_class</span></span> |<span data-ttu-id="37fcd-226">小費類別 (0：$0、1：$0-5、2：$6-10、3：$11-20、4：> $20)</span><span class="sxs-lookup"><span data-stu-id="37fcd-226">Tip class (0: $0, 1: $0-5, 2: $6-10, 3: $11-20, 4: > $20)</span></span> |

## <a name="execute-code-from-a-jupyter-notebook-on-the-spark-cluster"></a><span data-ttu-id="37fcd-227">從 Spark 叢集的 Jupyter Notebook 中執行程式碼</span><span class="sxs-lookup"><span data-stu-id="37fcd-227">Execute code from a Jupyter notebook on the Spark cluster</span></span>
<span data-ttu-id="37fcd-228">您可以從 Azure 入口網站啟動 Jupyter Notebook。</span><span class="sxs-lookup"><span data-stu-id="37fcd-228">You can launch the Jupyter Notebook from the Azure portal.</span></span> <span data-ttu-id="37fcd-229">在儀表板上尋找 Spark 叢集，並按一下該項目以進入您的叢集管理頁面。</span><span class="sxs-lookup"><span data-stu-id="37fcd-229">Find your Spark cluster on your dashboard and click it to enter management page for your cluster.</span></span> <span data-ttu-id="37fcd-230">若要開啟與 Spark 叢集相關聯的 Notebook，按一下 [叢集儀表板]  ->  [Jupyter Notebook]。</span><span class="sxs-lookup"><span data-stu-id="37fcd-230">To open the notebook associated with the Spark cluster, click **Cluster Dashboards** -> **Jupyter Notebook** .</span></span>

![叢集儀表板](./media/machine-learning-data-science-spark-overview/spark-jupyter-on-portal.png)

<span data-ttu-id="37fcd-232">您也可以瀏覽至 ***https://CLUSTERNAME.azurehdinsight.net/jupyter*** 存取 Jupyter Notebook。</span><span class="sxs-lookup"><span data-stu-id="37fcd-232">You can also browse to ***https://CLUSTERNAME.azurehdinsight.net/jupyter*** to access the Jupyter Notebooks.</span></span> <span data-ttu-id="37fcd-233">以您的叢集名稱來取代此 URL 內的 CLUSTERNAME 部分。</span><span class="sxs-lookup"><span data-stu-id="37fcd-233">Replace the CLUSTERNAME part of this URL with the name of your own cluster.</span></span> <span data-ttu-id="37fcd-234">您需要有系統管理員帳戶的密碼才能存取 Notebook。</span><span class="sxs-lookup"><span data-stu-id="37fcd-234">You need the password for your admin account to access the notebooks.</span></span>

![瀏覽 Jupyter Notebooks](./media/machine-learning-data-science-spark-overview/spark-jupyter-notebook.png)

<span data-ttu-id="37fcd-236">選取 [PySpark] 來查看包含一些預先封裝 Notebook 範例的目錄，這些範例使用了 PySpark API。適用於這組 Spark 主題的 Notebook (包含程式碼範例) 可在 [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) 上找到。</span><span class="sxs-lookup"><span data-stu-id="37fcd-236">Select PySpark to see a directory that contains a few examples of pre-packaged notebooks that use the PySpark API.The notebooks that contain the code samples for this suite of Spark topic are available at [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark)</span></span>

<span data-ttu-id="37fcd-237">您可以將 Notebook 直接從 [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) 上傳至 Spark 叢集上的 Jupyter Notebook 伺服器。</span><span class="sxs-lookup"><span data-stu-id="37fcd-237">You can upload the notebooks directly from [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/Spark/pySpark) to the Jupyter notebook server on your Spark cluster.</span></span> <span data-ttu-id="37fcd-238">在您的 Jupyter 首頁，按一下螢幕右側的 [上傳]  按鈕。</span><span class="sxs-lookup"><span data-stu-id="37fcd-238">On the home page of your Jupyter, click the **Upload** button on the right part of the screen.</span></span> <span data-ttu-id="37fcd-239">[檔案總管] 隨即開啟。</span><span class="sxs-lookup"><span data-stu-id="37fcd-239">It opens a file explorer.</span></span> <span data-ttu-id="37fcd-240">您可以在這裡貼上 Notebook 的 GitHub (原始內容) URL，然後按一下 [開啟]。</span><span class="sxs-lookup"><span data-stu-id="37fcd-240">Here you can paste the GitHub (raw content) URL of the Notebook and click **Open**.</span></span> 

<span data-ttu-id="37fcd-241">您會在 Jupyter 檔案清單上看到檔案名稱，並重新出現 [上傳]  按鈕。</span><span class="sxs-lookup"><span data-stu-id="37fcd-241">You see the file name on your Jupyter file list with an **Upload** button again.</span></span> <span data-ttu-id="37fcd-242">按一下此 [上傳]  按鈕。</span><span class="sxs-lookup"><span data-stu-id="37fcd-242">Click this **Upload** button.</span></span> <span data-ttu-id="37fcd-243">現在您已匯入 Notebook。</span><span class="sxs-lookup"><span data-stu-id="37fcd-243">Now you have imported the notebook.</span></span> <span data-ttu-id="37fcd-244">重複這些步驟，使用此逐步解說上傳其他 Notebook。</span><span class="sxs-lookup"><span data-stu-id="37fcd-244">Repeat these steps to upload the other notebooks from this walkthrough.</span></span>

> [!TIP]
> <span data-ttu-id="37fcd-245">您可以用滑鼠右鍵按一下瀏覽器上的下列連結，然後選取 [複製連結] 取得 Github 原始內容 URL。</span><span class="sxs-lookup"><span data-stu-id="37fcd-245">You can right-click the links on your browser and select **Copy Link** to get the github raw content URL.</span></span> <span data-ttu-id="37fcd-246">您可以將此 URL 貼到 Jupyter 的 [上傳] 檔案總管對話方塊。</span><span class="sxs-lookup"><span data-stu-id="37fcd-246">You can paste this URL into the Jupyter Upload file explorer dialog box.</span></span>
> 
> 

<span data-ttu-id="37fcd-247">現在您可以：</span><span class="sxs-lookup"><span data-stu-id="37fcd-247">Now you can:</span></span>

* <span data-ttu-id="37fcd-248">按一下 Notebook，查看程式碼。</span><span class="sxs-lookup"><span data-stu-id="37fcd-248">See the code by clicking the notebook.</span></span>
* <span data-ttu-id="37fcd-249">按 **SHIFT-ENTER** 執行每個儲存格。</span><span class="sxs-lookup"><span data-stu-id="37fcd-249">Execute each cell by pressing **SHIFT-ENTER**.</span></span>
* <span data-ttu-id="37fcd-250">按一下 [儲存格]  ->  [執行] 執行整個筆記本。</span><span class="sxs-lookup"><span data-stu-id="37fcd-250">Run the entire notebook by clicking on **Cell** -> **Run**.</span></span>
* <span data-ttu-id="37fcd-251">使用查詢的自動視覺效果。</span><span class="sxs-lookup"><span data-stu-id="37fcd-251">Use the automatic visualization of queries.</span></span>

> [!TIP]
> <span data-ttu-id="37fcd-252">PySpark 核心會將 SQL (HiveQL) 查詢的輸出自動視覺化。</span><span class="sxs-lookup"><span data-stu-id="37fcd-252">The PySpark kernel automatically visualizes the output of SQL (HiveQL) queries.</span></span> <span data-ttu-id="37fcd-253">在 Notebook 內使用 [類型]  功能表按鈕，系統提供您幾種不同類型的視覺效果 (資料表、圓形圖、折線圖、區域圖或橫條圖) 可選擇。</span><span class="sxs-lookup"><span data-stu-id="37fcd-253">You are given the option to select among several different types of visualizations (Table, Pie, Line, Area, or Bar) by using the **Type** menu buttons in the notebook:</span></span>
> 
> 

![泛型方法的羅吉斯迴歸 ROC 曲線](./media/machine-learning-data-science-spark-overview/pyspark-jupyter-autovisualization.png)

## <a name="whats-next"></a><span data-ttu-id="37fcd-255">後續步驟</span><span class="sxs-lookup"><span data-stu-id="37fcd-255">What's next?</span></span>
<span data-ttu-id="37fcd-256">現在您已使用 HDInsight Spark 叢集進行設定，並已上傳 Jupyter 筆記本，您已準備要逐步執行對應至這三個 PySpark Notebook 的主題。</span><span class="sxs-lookup"><span data-stu-id="37fcd-256">Now that you are set up with an HDInsight Spark cluster and have uploaded the Jupyter notebooks, you are ready to work through the topics that correspond to the three PySpark notebooks.</span></span> <span data-ttu-id="37fcd-257">這些主題示範如何瀏覽資料、建立和取用模型。</span><span class="sxs-lookup"><span data-stu-id="37fcd-257">They show how to explore your data and then how to create and consume models.</span></span> <span data-ttu-id="37fcd-258">進階的資料探索和模型化 Notebook 顯示如何包括交叉驗證、超參數清除和模型評估。</span><span class="sxs-lookup"><span data-stu-id="37fcd-258">The advanced data exploration and modeling notebook shows how to include cross-validation, hyper-parameter sweeping, and model evaluation.</span></span> 

<span data-ttu-id="37fcd-259">**使用 Spark 資料探索和模型化：**：遵循[使用 Spark MLlib 工具組來建立資料的二進位分類和迴歸模型](machine-learning-data-science-spark-data-exploration-modeling.md)主題的內容，來探索資料集，以及建立、評分、評估 Machine Learning 模型。</span><span class="sxs-lookup"><span data-stu-id="37fcd-259">**Data Exploration and modeling with Spark:** Explore the dataset and create, score, and evaluate the machine learning models by working through the [Create binary classification and regression models for data with the Spark MLlib toolkit](machine-learning-data-science-spark-data-exploration-modeling.md) topic.</span></span>

<span data-ttu-id="37fcd-260">**模型耗用量︰** 若要瞭解如何評分本主題中所建立的分類和迴歸模型，請參閱 [評分及評估 Spark 建置機器學習服務模型](machine-learning-data-science-spark-model-consumption.md)。</span><span class="sxs-lookup"><span data-stu-id="37fcd-260">**Model consumption:** To learn how to score the classification and regression models created in this topic, see [Score and evaluate Spark-built machine learning models](machine-learning-data-science-spark-model-consumption.md).</span></span>

<span data-ttu-id="37fcd-261">**交叉驗證和超參數清除**：請參閱 [使用 Spark 進階資料探索和模型化](machine-learning-data-science-spark-advanced-data-exploration-modeling.md) 有關如何使用交叉驗證和超參數清除訓練模型</span><span class="sxs-lookup"><span data-stu-id="37fcd-261">**Cross-validation and hyperparameter sweeping**: See [Advanced data exploration and modeling with Spark](machine-learning-data-science-spark-advanced-data-exploration-modeling.md) on how models can be trained using cross-validation and hyper-parameter sweeping</span></span>

