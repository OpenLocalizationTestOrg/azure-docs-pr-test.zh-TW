---
title: "Azure Machine Learning 中的特徵設計和選取 | Microsoft Docs"
description: "說明機器學習的資料增強程序中特性選取和特性工程設計的目的，並提供其角色的範例。"
services: machine-learning
documentationcenter: 
author: bradsev
manager: jhubbard
editor: cgronlun
ms.assetid: 9ceb524d-842e-4f77-9eae-a18e599442d6
ms.service: machine-learning
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 01/18/2017
ms.author: zhangya;bradsev
ROBOTS: NOINDEX
redirect_url: machine-learning-data-science-create-features
redirect_document_id: TRUE
ms.openlocfilehash: 51a5d8fed492cb9301e048c2b6a721e4573a47d9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 07/11/2017
---
# <a name="feature-engineering-and-selection-in-azure-machine-learning"></a><span data-ttu-id="c29df-103">Azure 機器學習中的特性工程設計和選取</span><span class="sxs-lookup"><span data-stu-id="c29df-103">Feature engineering and selection in Azure Machine Learning</span></span>
<span data-ttu-id="c29df-104">本主題說明機器學習的資料增強程序中特徵工程設計和特徵選取的目的。</span><span class="sxs-lookup"><span data-stu-id="c29df-104">This topic explains the purposes of feature engineering and feature selection in the data-enhancement process of machine learning.</span></span> <span data-ttu-id="c29df-105">其使用 Azure Machine Learning Studio 提供的範例來解釋這些程序的相關事項。</span><span class="sxs-lookup"><span data-stu-id="c29df-105">It illustrates what these processes involve by using examples provided by Azure Machine Learning Studio.</span></span>

[!INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]

<span data-ttu-id="c29df-106">從收集的原始資料選取或擷取特性，通常可以增強機器學習中使用的定型資料。</span><span class="sxs-lookup"><span data-stu-id="c29df-106">The training data used in machine learning can often be enhanced by the selection or extraction of features from the raw data collected.</span></span> <span data-ttu-id="c29df-107">在了解如何將手寫字元影像分類的內容中，有一個工程設計的特徵範例是建立從原始位元分配資料建構的位元密度對應。</span><span class="sxs-lookup"><span data-stu-id="c29df-107">An example of an engineered feature in the context of learning how to classify the images of handwritten characters is a bit-density map constructed from the raw bit distribution data.</span></span> <span data-ttu-id="c29df-108">相較於原始分配，此對應有助於更有效地找出字元的界限。</span><span class="sxs-lookup"><span data-stu-id="c29df-108">This map can help locate the edges of the characters more efficiently than the raw distribution.</span></span>

<span data-ttu-id="c29df-109">工程設計和選取的特徵可提高下列定型過程的效率：嘗試擷取資料中內含的重要資訊。</span><span class="sxs-lookup"><span data-stu-id="c29df-109">Engineered and selected features increase the efficiency of the training process, which attempts to extract the key information contained in the data.</span></span> <span data-ttu-id="c29df-110">此外，還可改善這些模型的功效，正確地分類輸入資料以及更精確地預測感興趣的結果。</span><span class="sxs-lookup"><span data-stu-id="c29df-110">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</span></span> <span data-ttu-id="c29df-111">特性工程設計和選取也可結合起來，讓學習更易於以運算方式處理。</span><span class="sxs-lookup"><span data-stu-id="c29df-111">Feature engineering and selection can also combine to make the learning more computationally tractable.</span></span> <span data-ttu-id="c29df-112">其作法是提高而後減少校正或定型模型所需的特性數量。</span><span class="sxs-lookup"><span data-stu-id="c29df-112">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</span></span> <span data-ttu-id="c29df-113">從數學的角度來看，選取用來定型模型的特性是極小的一組獨立變數，可供解釋資料中的模式，然後成功地預測結果。</span><span class="sxs-lookup"><span data-stu-id="c29df-113">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</span></span>

<span data-ttu-id="c29df-114">特性的工程設計和選取屬於較大型程序的一部分，該程序通常包含下列四個步驟：</span><span class="sxs-lookup"><span data-stu-id="c29df-114">The engineering and selection of features is one part of a larger process, which typically consists of four steps:</span></span>

* <span data-ttu-id="c29df-115">資料收集</span><span class="sxs-lookup"><span data-stu-id="c29df-115">Data collection</span></span>
* <span data-ttu-id="c29df-116">資料增強</span><span class="sxs-lookup"><span data-stu-id="c29df-116">Data enhancement</span></span>
* <span data-ttu-id="c29df-117">模型建構</span><span class="sxs-lookup"><span data-stu-id="c29df-117">Model construction</span></span>
* <span data-ttu-id="c29df-118">後處理</span><span class="sxs-lookup"><span data-stu-id="c29df-118">Post-processing</span></span>

<span data-ttu-id="c29df-119">工程設計和選取構成了機器學習服務的資料增強步驟。</span><span class="sxs-lookup"><span data-stu-id="c29df-119">Engineering and selection make up the data enhancement step of machine learning.</span></span> <span data-ttu-id="c29df-120">此程序的三個層面主要有四個目的：</span><span class="sxs-lookup"><span data-stu-id="c29df-120">Three aspects of this process may be distinguished for our purposes:</span></span>

* <span data-ttu-id="c29df-121">**資料前處理**：此程序嘗試確保收集的資料乾淨而一致。</span><span class="sxs-lookup"><span data-stu-id="c29df-121">**Data pre-processing**: This process tries to ensure that the collected data is clean and consistent.</span></span> <span data-ttu-id="c29df-122">其中包含下列工作：例如整合多個資料集、處理不一致的資料，以及轉換資料類型。</span><span class="sxs-lookup"><span data-stu-id="c29df-122">It includes tasks such as integrating multiple data sets, handling missing data, handling inconsistent data, and converting data types.</span></span>
* <span data-ttu-id="c29df-123">**特性工程設計**：此程序嘗試從資料中的現有原始特性建立其他相關特性，以及增加學習演算法的預測功效。</span><span class="sxs-lookup"><span data-stu-id="c29df-123">**Feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data and to increase predictive power to the learning algorithm.</span></span>
* <span data-ttu-id="c29df-124">**特性選取**：此程序選取主要的原始資料特性子集，以縮小定型問題的維度。</span><span class="sxs-lookup"><span data-stu-id="c29df-124">**Feature selection**: This process selects the key subset of original data features to reduce the dimensionality of the training problem.</span></span>

<span data-ttu-id="c29df-125">本主題僅涵蓋資料增強程序的特徵工程設計和特徵選取層面。</span><span class="sxs-lookup"><span data-stu-id="c29df-125">This topic only covers the feature engineering and feature selection aspects of the data enhancement process.</span></span> <span data-ttu-id="c29df-126">如需資料前處理步驟的其他資訊，請參閱 [在 Azure Machine Learning Studio 中進行資料前處理](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/) 影片。</span><span class="sxs-lookup"><span data-stu-id="c29df-126">For more information on the data pre-processing step, see [Pre-processing data in Azure Machine Learning Studio](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/).</span></span>

## <a name="creating-features-from-your-data--feature-engineering"></a><span data-ttu-id="c29df-127">從您的資料建立特性 - 特性工程設計</span><span class="sxs-lookup"><span data-stu-id="c29df-127">Creating features from your data--feature engineering</span></span>
<span data-ttu-id="c29df-128">定型資料包含由範例所組成的矩陣 (資料列中儲存的記錄或 觀察)，而每個範例都有一組特性 (資料行中儲存的變數或欄位)。</span><span class="sxs-lookup"><span data-stu-id="c29df-128">The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).</span></span> <span data-ttu-id="c29df-129">在實驗設計中指定的特性預計會將資料中的模式特性化。</span><span class="sxs-lookup"><span data-stu-id="c29df-129">The features specified in the experimental design are expected to characterize the patterns in the data.</span></span> <span data-ttu-id="c29df-130">儘管許多原始資料欄位都可以直接包含在選取用來將模型定型的特性集中，但通常還是需要從原始資料中的特性建構其他 (經過工程設計的) 特性，才能產生增強的定型資料集。</span><span class="sxs-lookup"><span data-stu-id="c29df-130">Although many of the raw data fields can be directly included in the selected feature set used to train a model, additional engineered features often need to be constructed from the features in the raw data to generate an enhanced training data set.</span></span>

<span data-ttu-id="c29df-131">應建立何種特性，才能在模型定型時增強資料集？</span><span class="sxs-lookup"><span data-stu-id="c29df-131">What kind of features should be created to enhance the data set when training a model?</span></span> <span data-ttu-id="c29df-132">經過工程設計的特性可增強定型，還會提供用以區分資料中模式的資訊。</span><span class="sxs-lookup"><span data-stu-id="c29df-132">Engineered features that enhance the training provide information that better differentiates the patterns in the data.</span></span> <span data-ttu-id="c29df-133">您期望新特性可提供原始或現有特性集中未清楚擷取或顯而易見的其他資訊，但此程序是一種藝術。</span><span class="sxs-lookup"><span data-stu-id="c29df-133">You expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set, but this process is something of an art.</span></span> <span data-ttu-id="c29df-134">健全且有建設性的決策通常需要一些網域.專業知識。</span><span class="sxs-lookup"><span data-stu-id="c29df-134">Sound and productive decisions often require some domain expertise.</span></span>

<span data-ttu-id="c29df-135">從 Azure 機器學習著手時，最簡單的方式是透過 Machine Learning Studio 中提供的範例具體地領會此程序。</span><span class="sxs-lookup"><span data-stu-id="c29df-135">When starting with Azure Machine Learning, it is easiest to grasp this process concretely by using samples provided in Machine Learning Studio.</span></span> <span data-ttu-id="c29df-136">以下呈現兩個範例：</span><span class="sxs-lookup"><span data-stu-id="c29df-136">Two examples are presented here:</span></span>

* <span data-ttu-id="c29df-137">在已知目標值的監督實驗中的 ([單車租用數量預測](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)) 迴歸範例</span><span class="sxs-lookup"><span data-stu-id="c29df-137">A regression example ([Prediction of the number of bike rentals](http://gallery.cortanaintelligence.com/Experiment/Regression-Demand-estimation-4)) in a supervised experiment where the target values are known</span></span>
* <span data-ttu-id="c29df-138">使用[特性雜湊][feature-hashing]的文字採礦分類範例</span><span class="sxs-lookup"><span data-stu-id="c29df-138">A text-mining classification example using [Feature Hashing][feature-hashing]</span></span>

### <a name="example-1-adding-temporal-features-for-a-regression-model"></a><span data-ttu-id="c29df-139">範例 1：新增迴歸模型的暫時特性</span><span class="sxs-lookup"><span data-stu-id="c29df-139">Example 1: Adding temporal features for a regression model</span></span>
<span data-ttu-id="c29df-140">為了示範如何為迴歸工作的特徵進行工程設計，讓我們在 Azure Machine Learning Studio 中使用「單車的需求預測」實驗。</span><span class="sxs-lookup"><span data-stu-id="c29df-140">To demonstrate how to engineer features for a regression task, let's use the experiment "Demand forecasting of bikes" in Azure Machine Learning Studio.</span></span> <span data-ttu-id="c29df-141">這項實驗的目標在於預測單車需求，也就是再特定月份、日期或小時內單車租用的數量。</span><span class="sxs-lookup"><span data-stu-id="c29df-141">The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month, day, or hour.</span></span> <span data-ttu-id="c29df-142">資料集**單車租用 UCI 資料集**作為原始輸入資料使用。</span><span class="sxs-lookup"><span data-stu-id="c29df-142">The data set **Bike Rental UCI data set** is used as the raw input data.</span></span>

<span data-ttu-id="c29df-143">此資料集是以在美國華盛頓特區維護單車出租網路的 Capital Bikeshare 公司所提供的實際資料為基礎。</span><span class="sxs-lookup"><span data-stu-id="c29df-143">This data set is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.</span></span> <span data-ttu-id="c29df-144">此資料集代表 2011 年到 2012 年間，一天中某個特定小時內的單車租用數量，總共包含 17379 個資料列和 17 個資料行。</span><span class="sxs-lookup"><span data-stu-id="c29df-144">The data set represents the number of bike rentals within a specific hour of a day, from 2011 to 2012, and it contains 17379 rows and 17 columns.</span></span> <span data-ttu-id="c29df-145">原始特性集包含天氣條件 (溫度、溼度、風速) 和當天的類型 (假日或工作日)。</span><span class="sxs-lookup"><span data-stu-id="c29df-145">The raw feature set contains weather conditions (temperature, humidity, wind speed) and the type of the day (holiday or weekday).</span></span> <span data-ttu-id="c29df-146">要預測的欄位為 **cnt**，代表特定小時內單位租用的計數，其範圍是 1 至 977。</span><span class="sxs-lookup"><span data-stu-id="c29df-146">The field to predict is **cnt**, a count that represents the bike rentals within a specific hour and that ranges from 1 to 977.</span></span>

<span data-ttu-id="c29df-147">為了在定型資料中建構有效特性，會使用相同的演算法建立四個各有不同定型資料集的迴歸模型，</span><span class="sxs-lookup"><span data-stu-id="c29df-147">To construct effective features in the training data, four regression models are built by using the same algorithm, but with four different training data sets.</span></span> <span data-ttu-id="c29df-148">這四個資料集代表相同的原始輸入資料，但設定的特性數量增加。</span><span class="sxs-lookup"><span data-stu-id="c29df-148">The four data sets represent the same raw input data, but with an increasing number of features set.</span></span> <span data-ttu-id="c29df-149">這些特性可分為四類：</span><span class="sxs-lookup"><span data-stu-id="c29df-149">These features are grouped into four categories:</span></span>

1. <span data-ttu-id="c29df-150">A = 預測日的天氣 + 假日 + 工作日 + 週末特性</span><span class="sxs-lookup"><span data-stu-id="c29df-150">A = weather + holiday + weekday + weekend features for the predicted day</span></span>
2. <span data-ttu-id="c29df-151">B = 過去的 12 小時以來，每小時租出的單車數量</span><span class="sxs-lookup"><span data-stu-id="c29df-151">B = number of bikes that were rented in each of the previous 12 hours</span></span>
3. <span data-ttu-id="c29df-152">C = 過去的 12 天以來，每天在同一個時間租出的單車數量</span><span class="sxs-lookup"><span data-stu-id="c29df-152">C = number of bikes that were rented in each of the previous 12 days at the same hour</span></span>
4. <span data-ttu-id="c29df-153">D = 過去的 12 週以來，在同一天同一個時間租出的單車數量</span><span class="sxs-lookup"><span data-stu-id="c29df-153">D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day</span></span>

<span data-ttu-id="c29df-154">除了已存在於原先未經處理資料中的特徵集 A 以外，其他三個特徵集都是透過特徵工程設計程序來建立。</span><span class="sxs-lookup"><span data-stu-id="c29df-154">Besides feature set A, which already exists in the original raw data, the other three sets of features are created through the feature engineering process.</span></span> <span data-ttu-id="c29df-155">特徵集 B 會擷取最近的單車需求。</span><span class="sxs-lookup"><span data-stu-id="c29df-155">Feature set B captures the recent demand for the bikes.</span></span> <span data-ttu-id="c29df-156">特性集 C 會擷取某一個小時的單車需求。</span><span class="sxs-lookup"><span data-stu-id="c29df-156">Feature set C captures the demand for bikes at a particular hour.</span></span> <span data-ttu-id="c29df-157">特性集 D 會擷取一週當中某一天某一個小時的單車需求。</span><span class="sxs-lookup"><span data-stu-id="c29df-157">Feature set D captures demand for bikes at particular hour and particular day of the week.</span></span> <span data-ttu-id="c29df-158">四個定型資料集分別包含特性集 A、A+B、A+B+C 和 A+B+C+D。</span><span class="sxs-lookup"><span data-stu-id="c29df-158">Each of the four training data sets includes feature sets A, A+B, A+B+C, and A+B+C+D, respectively.</span></span>

<span data-ttu-id="c29df-159">在 Azure 機器學習實驗中，這四個定型資料集是透過預先處理的輸入資料集中的分支形成。</span><span class="sxs-lookup"><span data-stu-id="c29df-159">In the Azure Machine Learning experiment, these four training data sets are formed via four branches from the pre-processed input data set.</span></span> <span data-ttu-id="c29df-160">除了最左邊的分支以外，每個分支都包含[執行 R 指令碼][execute-r-script]模組，其中有一組衍生的特徵 (特徵集 B、C 和 D) 會分別建構並附加至匯入的資料集 。</span><span class="sxs-lookup"><span data-stu-id="c29df-160">Except for the leftmost branch, each of these branches contains an [Execute R Script][execute-r-script] module in which a set of derived features (feature sets B, C, and D) is respectively constructed and appended to the imported data set.</span></span> <span data-ttu-id="c29df-161">下圖示範左邊第二個分支中用來建立特性集 B 的 R 指令碼。</span><span class="sxs-lookup"><span data-stu-id="c29df-161">The following figure demonstrates the R script used to create feature set B in the second left branch.</span></span>

![建立功能集](./media/machine-learning-feature-selection-and-engineering/addFeature-Rscripts.png)

<span data-ttu-id="c29df-163">下表彙總四個模型的效能結果比較。</span><span class="sxs-lookup"><span data-stu-id="c29df-163">The following table summarizes the comparison of the performance results of the four models.</span></span> <span data-ttu-id="c29df-164">特性 A+B+C 所呈現的結果最理想。</span><span class="sxs-lookup"><span data-stu-id="c29df-164">The best results are shown by features A+B+C.</span></span> <span data-ttu-id="c29df-165">請注意，當定型資料中包含其他特性集時，錯誤率會降低。</span><span class="sxs-lookup"><span data-stu-id="c29df-165">Note that the error rate decreases when additional feature sets are included in the training data.</span></span> <span data-ttu-id="c29df-166">這證實了我們的推測：特性集 B 和 C 會針對迴歸工作提供其他相關資訊。</span><span class="sxs-lookup"><span data-stu-id="c29df-166">This verifies our presumption that the feature sets B and C provide additional relevant information for the regression task.</span></span> <span data-ttu-id="c29df-167">新增 D 特性集似乎不會讓錯誤率降低。</span><span class="sxs-lookup"><span data-stu-id="c29df-167">Adding the D feature set does not seem to provide any additional reduction in the error rate.</span></span>

![比較效能結果](./media/machine-learning-feature-selection-and-engineering/result1.png)

### <span data-ttu-id="c29df-169"><a name="example2"></a> 範例 2：在文字採礦中建立特性</span><span class="sxs-lookup"><span data-stu-id="c29df-169"><a name="example2"></a> Example 2: Creating features in text mining</span></span>
<span data-ttu-id="c29df-170">特性工程設計廣泛運用於文字採礦的相關工作，例如文件分類和情感分析。</span><span class="sxs-lookup"><span data-stu-id="c29df-170">Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.</span></span> <span data-ttu-id="c29df-171">例如，當您想要將文件分為數個類別時，通常會假設包含在一個文件類別中的文字或片語比較不可能出現在其他文件類別中。</span><span class="sxs-lookup"><span data-stu-id="c29df-171">For example, when you want to classify documents into several categories, a typical assumption is that the words or phrases included in one document category are less likely to occur in another document category.</span></span> <span data-ttu-id="c29df-172">換言之，文字或片語分配的次數能夠描述不同文件類別的特徵。</span><span class="sxs-lookup"><span data-stu-id="c29df-172">In other words, the frequency of the word or phrase distribution is able to characterize different document categories.</span></span> <span data-ttu-id="c29df-173">在文字採礦應用程式，建立文字或片語次數相關特性時需要特性工程設計程序中，因為個別的文字內容通常可作為輸入資料。</span><span class="sxs-lookup"><span data-stu-id="c29df-173">In text mining applications, the feature engineering process is needed to create the features involving word or phrase frequencies because individual pieces of text-contents usually serve as the input data.</span></span>

<span data-ttu-id="c29df-174">為了達成此工作，會套用名為特性雜湊的技術，有效地將任意文字特性變成索引。</span><span class="sxs-lookup"><span data-stu-id="c29df-174">To achieve this task, a technique called *feature hashing* is applied to efficiently turn arbitrary text features into indices.</span></span> <span data-ttu-id="c29df-175">此方法不會將每個文字特性 (文字或片語) 關聯至特定索引，而是將雜湊函數套用至特性並直接使用其雜湊值作為索引。</span><span class="sxs-lookup"><span data-stu-id="c29df-175">Instead of associating each text feature (words or phrases) to a particular index, this method functions by applying a hash function to the features and by using their hash values as indices directly.</span></span>

<span data-ttu-id="c29df-176">Azure 機器學習中有一個[特性雜湊][feature-hashing]模組，會建立這些文字或片語特性。</span><span class="sxs-lookup"><span data-stu-id="c29df-176">In Azure Machine Learning, there is a [Feature Hashing][feature-hashing] module that creates these word or phrase features.</span></span> <span data-ttu-id="c29df-177">下圖顯示使用此模組的範例。</span><span class="sxs-lookup"><span data-stu-id="c29df-177">The following figure shows an example of using this module.</span></span> <span data-ttu-id="c29df-178">輸入資料集包含兩個資料行：1 至 5 的書籍評比，以及實際評論內容。</span><span class="sxs-lookup"><span data-stu-id="c29df-178">The input data set contains two columns: the book rating ranging from 1 to 5 and the actual review content.</span></span> <span data-ttu-id="c29df-179">此[特性雜湊][feature-hashing]模組的目標在於擷取新特性，以顯示特定書籍評論中對應文字或片語的發生次數。</span><span class="sxs-lookup"><span data-stu-id="c29df-179">The goal of this [Feature Hashing][feature-hashing] module is to retrieve new features that show the occurrence frequency of the corresponding words or phrases within the particular book review.</span></span> <span data-ttu-id="c29df-180">若要使用此模組，您必須完成下列步驟：</span><span class="sxs-lookup"><span data-stu-id="c29df-180">To use this module, you need to complete the following steps:</span></span>

1. <span data-ttu-id="c29df-181">選取包含輸入文字的資料行 (此例中的 **Col2**)。</span><span class="sxs-lookup"><span data-stu-id="c29df-181">Select the column that contains the input text (**Col2** in this example).</span></span>
2. <span data-ttu-id="c29df-182">將 Hashing bitsize 設定為 8，表示將建立 2^8=256 個特徵。</span><span class="sxs-lookup"><span data-stu-id="c29df-182">Set *Hashing bitsize* to 8, which means 2^8=256 features are created.</span></span> <span data-ttu-id="c29df-183">所有文字中的文字或片語接著會雜湊至 256 個索引。</span><span class="sxs-lookup"><span data-stu-id="c29df-183">The word or phrase in the text is then hashed to 256 indices.</span></span> <span data-ttu-id="c29df-184">Hashing bitsize 參數的範圍是 1 至 31。</span><span class="sxs-lookup"><span data-stu-id="c29df-184">The parameter *Hashing bitsize* ranges from 1 to 31.</span></span> <span data-ttu-id="c29df-185">如果參數設定為較大的數字，則單字或片語較不會雜湊至相同的索引。</span><span class="sxs-lookup"><span data-stu-id="c29df-185">If the parameter is set to a larger number, the words or phrases are less likely to be hashed into the same index.</span></span>
3. <span data-ttu-id="c29df-186">將 N-grams 參數設定為 2。</span><span class="sxs-lookup"><span data-stu-id="c29df-186">Set the parameter *N-grams* to 2.</span></span> <span data-ttu-id="c29df-187">這麼做可從輸入文字中擷取 unigrams (適用於每一個文字的特性) 和 bigrams (適用於每一對相鄰文字的特性) 的發生次數。</span><span class="sxs-lookup"><span data-stu-id="c29df-187">This retrieves the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.</span></span> <span data-ttu-id="c29df-188">N-grams 參數的範圍是 0 至 10，這表示要包含在一個特性中的循序文字數目上限。</span><span class="sxs-lookup"><span data-stu-id="c29df-188">The parameter *N-grams* ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.</span></span>  

![特性雜湊模組](./media/machine-learning-feature-selection-and-engineering/feature-Hashing1.png)

<span data-ttu-id="c29df-190">下圖顯示這些新特徵的外觀。</span><span class="sxs-lookup"><span data-stu-id="c29df-190">The following figure shows what these new features look like.</span></span>

![特性雜湊範例](./media/machine-learning-feature-selection-and-engineering/feature-Hashing2.png)

## <a name="filtering-features-from-your-data--feature-selection"></a><span data-ttu-id="c29df-192">從您的資料篩選特性 - 特性選取</span><span class="sxs-lookup"><span data-stu-id="c29df-192">Filtering features from your data--feature selection</span></span>
<span data-ttu-id="c29df-193">特性選取程序通常適用於定型資料集的建構，以便進行預測性建模工作，例如分類或迴歸工作。</span><span class="sxs-lookup"><span data-stu-id="c29df-193">*Feature selection* is a process that is commonly applied to the construction of training data sets for predictive modeling tasks such as classification or regression tasks.</span></span> <span data-ttu-id="c29df-194">其目的在於從原始資料集中選取一小組特性，使用極小一組的特性來代表資料中的最大變異量，藉此縮小其維度。</span><span class="sxs-lookup"><span data-stu-id="c29df-194">The goal is to select a subset of the features from the original data set that reduces its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</span></span> <span data-ttu-id="c29df-195">這個特徵的子集只會包含要用於訓練模型的特徵。</span><span class="sxs-lookup"><span data-stu-id="c29df-195">This subset of features contains the only features to be included to train the model.</span></span> <span data-ttu-id="c29df-196">特性選取有兩個主要目的：</span><span class="sxs-lookup"><span data-stu-id="c29df-196">Feature selection serves two main purposes:</span></span>

* <span data-ttu-id="c29df-197">特性選取通常會排除不相關、多餘或高度相關的特性，進而提高分類正確性。</span><span class="sxs-lookup"><span data-stu-id="c29df-197">Feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</span></span>
* <span data-ttu-id="c29df-198">特性選取會減少特性數目，讓模型定型程序更有效率。</span><span class="sxs-lookup"><span data-stu-id="c29df-198">Feature selection decreases the number of features, which makes the model training process more efficient.</span></span> <span data-ttu-id="c29df-199">對於定型代價昂貴的學習者 (例如支援向量機器) 而言，這格外重要。</span><span class="sxs-lookup"><span data-stu-id="c29df-199">This is particularly important for learners that are expensive to train such as support vector machines.</span></span>

<span data-ttu-id="c29df-200">雖然特性選取設法要減少資料集中用於定型模型的特性數目，但通常不是指「維度縮減」。</span><span class="sxs-lookup"><span data-stu-id="c29df-200">Although feature selection seeks to reduce the number of features in the data set used to train the model, it is not usually referred to by the term *dimensionality reduction.*</span></span> <span data-ttu-id="c29df-201">特性選取方法會擷取資料中的原始特性子集，但不會加以變更。</span><span class="sxs-lookup"><span data-stu-id="c29df-201">Feature selection methods extract a subset of original features in the data without changing them.</span></span>  <span data-ttu-id="c29df-202">維度縮減方法會運用經過工程設計的特性，轉換原始特性並加以修改。</span><span class="sxs-lookup"><span data-stu-id="c29df-202">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</span></span> <span data-ttu-id="c29df-203">維度縮減方法的範例包含主成分分析、典型相關分析和奇異值分解。</span><span class="sxs-lookup"><span data-stu-id="c29df-203">Examples of dimensionality reduction methods include principal component analysis, canonical correlation analysis, and singular value decomposition.</span></span>

<span data-ttu-id="c29df-204">監督環境中有一個廣泛應用的特性選取方法類別為以篩選為基礎的特性選取。</span><span class="sxs-lookup"><span data-stu-id="c29df-204">One widely applied category of feature selection methods in a supervised context is filter-based feature selection.</span></span> <span data-ttu-id="c29df-205">這些方法會評估每個特性與目標屬性之間的相關性，套用統計量值以將評分指派給每個特性。</span><span class="sxs-lookup"><span data-stu-id="c29df-205">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</span></span> <span data-ttu-id="c29df-206">接著會依分數將特性排名，而分數可供您用來設定保留或排除特定特性的臨界值。</span><span class="sxs-lookup"><span data-stu-id="c29df-206">The features are then ranked by the score, which you can use to set the threshold for keeping or eliminating a specific feature.</span></span> <span data-ttu-id="c29df-207">這些方法中使用的統計量值範例包含皮耳森相關、相互資訊和卡方檢定。</span><span class="sxs-lookup"><span data-stu-id="c29df-207">Examples of the statistical measures used in these methods include Pearson Correlation, mutual information, and the Chi-squared test.</span></span>

<span data-ttu-id="c29df-208">Azure Machine Learning Studio 針對特性選取提供模組。</span><span class="sxs-lookup"><span data-stu-id="c29df-208">Azure Machine Learning Studio provides modules for feature selection.</span></span> <span data-ttu-id="c29df-209">如下圖所示，這些模組包含[以篩選為基礎的特性選取][filter-based-feature-selection]和[費雪線性判別分析][fisher-linear-discriminant-analysis]。</span><span class="sxs-lookup"><span data-stu-id="c29df-209">As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</span></span>

![特性選取範例](./media/machine-learning-feature-selection-and-engineering/feature-Selection.png)

<span data-ttu-id="c29df-211">例如，使用[篩選為基礎的特徵選取][filter-based-feature-selection]模組與先前所述的文字採礦範例。</span><span class="sxs-lookup"><span data-stu-id="c29df-211">For example, use the [Filter-Based Feature Selection][filter-based-feature-selection] module with the text mining example outlined previously.</span></span> <span data-ttu-id="c29df-212">假設在透過[特徵雜湊][feature-hashing]模組建立一組 256 個特性之後，您想要建立一個迴歸模型，其應變數為 **Col1** 並代表 1 至 5 的書籍評論評比。</span><span class="sxs-lookup"><span data-stu-id="c29df-212">Assume that you want to build a regression model after a set of 256 features is created through the [Feature Hashing][feature-hashing] module, and that the response variable is **Col1** and represents a book review rating ranging from 1 to 5.</span></span> <span data-ttu-id="c29df-213">將 [特性評分方法] 設定為 [皮耳森相關]，則 [目標欄] 會是 **Col1**，而 [所需的特性數] 會是 **50**。</span><span class="sxs-lookup"><span data-stu-id="c29df-213">Set **Feature scoring method** to **Pearson Correlation**, **Target column** to **Col1**, and **Number of desired features** to **50**.</span></span> <span data-ttu-id="c29df-214">然後，[以篩選器為基礎的特徵選取][filter-based-feature-selection]模組會產生一個包含 50 個特徵且目標屬性為 **Col1** 的資料集。</span><span class="sxs-lookup"><span data-stu-id="c29df-214">The module [Filter-Based Feature Selection][filter-based-feature-selection] then produces a data set containing 50 features together with the target attribute **Col1**.</span></span> <span data-ttu-id="c29df-215">下圖顯示此實驗的流程以及輸入參數。</span><span class="sxs-lookup"><span data-stu-id="c29df-215">The following figure shows the flow of this experiment and the input parameters.</span></span>

![特性選取範例](./media/machine-learning-feature-selection-and-engineering/feature-Selection1.png)

<span data-ttu-id="c29df-217">下圖顯示結果產生的資料集。</span><span class="sxs-lookup"><span data-stu-id="c29df-217">The following figure shows the resulting data sets.</span></span> <span data-ttu-id="c29df-218">每個特性都是根據本身與目標屬性 **Col1** 之間的皮耳森相關進行評分。</span><span class="sxs-lookup"><span data-stu-id="c29df-218">Each feature is scored based on the Pearson Correlation between itself and the target attribute **Col1**.</span></span> <span data-ttu-id="c29df-219">系統會保留最高分的特性。</span><span class="sxs-lookup"><span data-stu-id="c29df-219">The features with top scores are kept.</span></span>

![篩選器為基礎的特性選取資料集](./media/machine-learning-feature-selection-and-engineering/feature-Selection2.png)

<span data-ttu-id="c29df-221">下圖顯示所選特性的對應評分。</span><span class="sxs-lookup"><span data-stu-id="c29df-221">The following figure shows the corresponding scores of the selected features.</span></span>

![選取的特性分數](./media/machine-learning-feature-selection-and-engineering/feature-Selection3.png)

<span data-ttu-id="c29df-223">套用這個[以篩選為基礎的特性選取][filter-based-feature-selection]模組，則會選取 256 個特性中的 50 個特性，因為根據**皮耳森相關**評分方法，其具有最多個與目標變數 **Col1** 相互關聯的特性。</span><span class="sxs-lookup"><span data-stu-id="c29df-223">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most features correlated with the target variable **Col1** based on the scoring method **Pearson Correlation**.</span></span>

## <a name="conclusion"></a><span data-ttu-id="c29df-224">結論</span><span class="sxs-lookup"><span data-stu-id="c29df-224">Conclusion</span></span>
<span data-ttu-id="c29df-225">建立機器學習模型時，常會執行特性工程設計和特性選取這兩個步驟來預備定型資料。</span><span class="sxs-lookup"><span data-stu-id="c29df-225">Feature engineering and feature selection are two steps commonly performed to prepare the training data when building a machine learning model.</span></span> <span data-ttu-id="c29df-226">通常會先套用特性工程設計以產生其他特定，然後執行特性選取步驟以排除不相關、多餘或高度相關的特性。</span><span class="sxs-lookup"><span data-stu-id="c29df-226">Normally, feature engineering is applied first to generate additional features, and then the feature selection step is performed to eliminate irrelevant, redundant, or highly correlated features.</span></span>

<span data-ttu-id="c29df-227">您不一定要執行特徵工程設計或特徵選取。</span><span class="sxs-lookup"><span data-stu-id="c29df-227">It is not always necessarily to perform feature engineering or feature selection.</span></span> <span data-ttu-id="c29df-228">需要與否取決於您所擁有或收集的資料、您挑選的演算法，以及實驗的目標。</span><span class="sxs-lookup"><span data-stu-id="c29df-228">Whether it is needed depends on the data you have or collect, the algorithm you pick, and the objective of the experiment.</span></span>

<!-- Module References -->
[execute-r-script]: https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/
[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/
