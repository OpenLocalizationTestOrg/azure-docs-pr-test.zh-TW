---
title: "複製活動的效能及微調指南 | Microsoft Docs"
description: "了解當您使用複製活動時，Azure Data Factory 中的資料移動效能會受到哪些主要因素的影響。"
services: data-factory
documentationcenter: 
author: linda33wj
manager: jhubbard
editor: monicar
ms.assetid: 4b9a6a4f-8cf5-4e0a-a06f-8133a2b7bc58
ms.service: data-factory
ms.workload: data-services
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 08/10/2017
ms.author: jingwang
ms.openlocfilehash: 2779655aee3af3a351b30f18b4c9d9918e9f2210
ms.sourcegitcommit: 18ad9bc049589c8e44ed277f8f43dcaa483f3339
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 08/29/2017
---
# <a name="copy-activity-performance-and-tuning-guide"></a><span data-ttu-id="ebc12-103">複製活動的效能及微調指南</span><span class="sxs-lookup"><span data-stu-id="ebc12-103">Copy Activity performance and tuning guide</span></span>
<span data-ttu-id="ebc12-104">Azure Data Factory 複製活動會提供安全、可靠、高效能的頂級資料載入解決方案。</span><span class="sxs-lookup"><span data-stu-id="ebc12-104">Azure Data Factory Copy Activity delivers a first-class secure, reliable, and high-performance data loading solution.</span></span> <span data-ttu-id="ebc12-105">它可讓您複製每天在各式各樣雲端和內部部署資料存放區上數十 TB 的資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-105">It enables you to copy tens of terabytes of data every day across a rich variety of cloud and on-premises data stores.</span></span> <span data-ttu-id="ebc12-106">超快的資料載入效能是可確保您能夠專注於核心「巨量資料」問題的關鍵︰建置進階的分析解決方案，並從該所有資料獲得深入解析。</span><span class="sxs-lookup"><span data-stu-id="ebc12-106">Blazing-fast data loading performance is key to ensure you can focus on the core “big data” problem: building advanced analytics solutions and getting deep insights from all that data.</span></span>

<span data-ttu-id="ebc12-107">Azure 提供一組企業級資料儲存與資料倉儲解決方案，而「複製活動」則提供一個容易設定的高度最佳化資料載入體驗。</span><span class="sxs-lookup"><span data-stu-id="ebc12-107">Azure provides a set of enterprise-grade data storage and data warehouse solutions, and Copy Activity offers a highly optimized data loading experience that is easy to configure and set up.</span></span> <span data-ttu-id="ebc12-108">只要使用單一的複製活動，您便可以達成下列目的︰</span><span class="sxs-lookup"><span data-stu-id="ebc12-108">With just a single copy activity, you can achieve:</span></span>

* <span data-ttu-id="ebc12-109">以 **1.2 GBps** 的速度將資料載入 **Azure SQL 資料倉儲**。</span><span class="sxs-lookup"><span data-stu-id="ebc12-109">Loading data into **Azure SQL Data Warehouse** at **1.2 GBps**.</span></span> <span data-ttu-id="ebc12-110">如需使用案例的逐步解說，請參閱[使用 Azure Data Factory 在 15 分鐘內將 1 TB 載入至 Azure SQL 資料倉儲](data-factory-load-sql-data-warehouse.md)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-110">For a walkthrough with a use case, see [Load 1 TB into Azure SQL Data Warehouse under 15 minutes with Azure Data Factory](data-factory-load-sql-data-warehouse.md).</span></span>
* <span data-ttu-id="ebc12-111">以 **1.0 GBps** 的速度將資料載入 **Azure Blob 儲存體**</span><span class="sxs-lookup"><span data-stu-id="ebc12-111">Loading data into **Azure Blob storage** at **1.0 GBps**</span></span>
* <span data-ttu-id="ebc12-112">以 **1.0 GBps** 的速度將資料載入 **Azure Data Lake Store**</span><span class="sxs-lookup"><span data-stu-id="ebc12-112">Loading data into **Azure Data Lake Store** at **1.0 GBps**</span></span>

<span data-ttu-id="ebc12-113">本文章說明：</span><span class="sxs-lookup"><span data-stu-id="ebc12-113">This article describes:</span></span>

* <span data-ttu-id="ebc12-114">[效能參考數字](#performance-reference) ；</span><span class="sxs-lookup"><span data-stu-id="ebc12-114">[Performance reference numbers](#performance-reference) for supported source and sink data stores to help you plan your project;</span></span>
* <span data-ttu-id="ebc12-115">可在不同情況下 (包括[雲端資料移動單位](#cloud-data-movement-units)、[平行複製](#parallel-copy)及[分段複製](#staged-copy)) 大幅提升複製輸送量的功能；</span><span class="sxs-lookup"><span data-stu-id="ebc12-115">Features that can boost the copy throughput in different scenarios, including [cloud data movement units](#cloud-data-movement-units), [parallel copy](#parallel-copy), and [staged Copy](#staged-copy);</span></span>
* <span data-ttu-id="ebc12-116">[效能微調指導方針](#performance-tuning-steps) 。</span><span class="sxs-lookup"><span data-stu-id="ebc12-116">[Performance tuning guidance](#performance-tuning-steps) on how to tune the performance and the key factors that can impact copy performance.</span></span>

> [!NOTE]
> <span data-ttu-id="ebc12-117">如果您大致來說並不熟悉複製活動，請先參閱 [使用複製活動來移動資料](data-factory-data-movement-activities.md) 再閱讀本文。</span><span class="sxs-lookup"><span data-stu-id="ebc12-117">If you are not familiar with Copy Activity in general, see [Move data by using Copy Activity](data-factory-data-movement-activities.md) before reading this article.</span></span>
>

## <a name="performance-reference"></a><span data-ttu-id="ebc12-118">效能參考</span><span class="sxs-lookup"><span data-stu-id="ebc12-118">Performance reference</span></span>

<span data-ttu-id="ebc12-119">下表顯示根據內部測試之給定來源與接收配對的複製輸送量數字 (以 MBps 為單位)，以供參考。</span><span class="sxs-lookup"><span data-stu-id="ebc12-119">As a reference, below table shows the copy throughput number in MBps for the given source and sink pairs based on in-house testing.</span></span> <span data-ttu-id="ebc12-120">為了進行比較，該表格也會示範[雲端資料移動單位](#cloud-data-movement-units)或[資料管理閘道延展性](data-factory-data-management-gateway-high-availability-scalability.md) (多個閘道節點) 的不同設定如何協助複製效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-120">For comparison, it also demonstrates how different settings of [cloud data movement units](#cloud-data-movement-units) or [Data Management Gateway scalability](data-factory-data-management-gateway-high-availability-scalability.md) (multiple gateway nodes) can help on copy performance.</span></span>

![效能矩陣](./media/data-factory-copy-activity-performance/CopyPerfRef.png)


<span data-ttu-id="ebc12-122">**注意事項：**</span><span class="sxs-lookup"><span data-stu-id="ebc12-122">**Points to note:**</span></span>
* <span data-ttu-id="ebc12-123">輸送量的計算公式如下：[從來源讀取的資料大小]/[複製活動執行持續時間]。</span><span class="sxs-lookup"><span data-stu-id="ebc12-123">Throughput is calculated by using the following formula: [size of data read from source]/[Copy Activity run duration].</span></span>
* <span data-ttu-id="ebc12-124">表中的效能參考數字是使用單一複製活動執行裡的 [TPC-H](http://www.tpc.org/tpch/) 資料集來測量的。</span><span class="sxs-lookup"><span data-stu-id="ebc12-124">The performance reference numbers in the table were measured using [TPC-H](http://www.tpc.org/tpch/) data set in a single copy activity run.</span></span>
* <span data-ttu-id="ebc12-125">在 Azure 資料存放區中，來源和接收位於相同 Azure 區域中。</span><span class="sxs-lookup"><span data-stu-id="ebc12-125">In Azure data stores, the source and sink are in the same Azure region.</span></span>
* <span data-ttu-id="ebc12-126">對於內部部署和雲端資料存放區之間的混合式複製，每個閘道節點都在與內部部署資料存放區分開的機器上執行，該機器具有以下規格。</span><span class="sxs-lookup"><span data-stu-id="ebc12-126">For hybrid copy between on-premises and cloud data stores, each gateway node was running on a machine that was separate from the on-premises data store with below specification.</span></span> <span data-ttu-id="ebc12-127">當閘道上僅執行單一活動，複製作業將只會取用測試電腦一小部分的 CPU、記憶體或網路頻寬。</span><span class="sxs-lookup"><span data-stu-id="ebc12-127">When a single activity was running on gateway, the copy operation consumed only a small portion of the test machine's CPU, memory, or network bandwidth.</span></span> <span data-ttu-id="ebc12-128">深入了解[資料管理閘道的考量](#considerations-for-data-management-gateway)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-128">Learn more from [consideration for Data Management Gateway](#considerations-for-data-management-gateway).</span></span>
    <table>
    <tr>
        <td><span data-ttu-id="ebc12-129">CPU</span><span class="sxs-lookup"><span data-stu-id="ebc12-129">CPU</span></span></td>
        <td><span data-ttu-id="ebc12-130">32 核心 2.20 GHz Intel Xeon E5-2660 v2</span><span class="sxs-lookup"><span data-stu-id="ebc12-130">32 cores 2.20 GHz Intel Xeon E5-2660 v2</span></span></td>
    </tr>
    <tr>
        <td><span data-ttu-id="ebc12-131">記憶體</span><span class="sxs-lookup"><span data-stu-id="ebc12-131">Memory</span></span></td>
        <td><span data-ttu-id="ebc12-132">128 GB</span><span class="sxs-lookup"><span data-stu-id="ebc12-132">128 GB</span></span></td>
    </tr>
    <tr>
        <td><span data-ttu-id="ebc12-133">網路</span><span class="sxs-lookup"><span data-stu-id="ebc12-133">Network</span></span></td>
        <td><span data-ttu-id="ebc12-134">網際網路介面：10 Gbps；內部網路介面：40 Gbps</span><span class="sxs-lookup"><span data-stu-id="ebc12-134">Internet interface: 10 Gbps; intranet interface: 40 Gbps</span></span></td>
    </tr>
    </table>


> [!TIP]
> <span data-ttu-id="ebc12-135">您可以利用比預設最大 DMU 更多的資料移動單位 (DMU)，也就是對雲端到雲端複製活動執行使用 32 單位，以達到更高的輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-135">You can achieve higher throughput by leveraging more data movement units (DMUs) than the default maximum DMUs, which is 32 for a cloud-to-cloud copy activity run.</span></span> <span data-ttu-id="ebc12-136">比方說，使用 100 DMU，您就可以用 **1.0GBps** 的速率將資料從 Azure Blob 複製到 Azure Data Lake Store。</span><span class="sxs-lookup"><span data-stu-id="ebc12-136">For example, with 100 DMUs, you can achieve copying data from Azure Blob into Azure Data Lake Store at **1.0GBps**.</span></span> <span data-ttu-id="ebc12-137">如需此功能和支援案例的詳細資訊，請參閱[雲端資料移動單位](#cloud-data-movement-units)一節。</span><span class="sxs-lookup"><span data-stu-id="ebc12-137">See the [Cloud data movement units](#cloud-data-movement-units) section for details about this feature and the supported scenario.</span></span> <span data-ttu-id="ebc12-138">請連絡 [Azure 支援](https://azure.microsoft.com/support/)來要求更多 DMU。</span><span class="sxs-lookup"><span data-stu-id="ebc12-138">Contact [Azure support](https://azure.microsoft.com/support/) to request more DMUs.</span></span>

## <a name="parallel-copy"></a><span data-ttu-id="ebc12-139">平行複製</span><span class="sxs-lookup"><span data-stu-id="ebc12-139">Parallel copy</span></span>
<span data-ttu-id="ebc12-140">您可以 **在複製活動執行內以平行方式**從來源讀取資料或將資料寫入目的地。</span><span class="sxs-lookup"><span data-stu-id="ebc12-140">You can read data from the source or write data to the destination **in parallel within a Copy Activity run**.</span></span> <span data-ttu-id="ebc12-141">這項功能可增強複製作業的輸送量並減少所需的資料移動時間。</span><span class="sxs-lookup"><span data-stu-id="ebc12-141">This feature enhances the throughput of a copy operation and reduces the time it takes to move data.</span></span>

<span data-ttu-id="ebc12-142">此設定不同於活動定義中的 **並行** 屬性。</span><span class="sxs-lookup"><span data-stu-id="ebc12-142">This setting is different from the **concurrency** property in the activity definition.</span></span> <span data-ttu-id="ebc12-143">**並行**屬性可決定**並行複製活動執行**的數目，以處理來自不同活動時段 (1 AM 至 2 AM、2 AM 至 3 AM、3 AM 至 4 AM，依此類推) 的資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-143">The **concurrency** property determines the number of **concurrent Copy Activity runs** to process data from different activity windows (1 AM to 2 AM, 2 AM to 3 AM, 3 AM to 4 AM, and so on).</span></span> <span data-ttu-id="ebc12-144">在執行歷程載入時，這個功能非常有用。</span><span class="sxs-lookup"><span data-stu-id="ebc12-144">This capability is helpful when you perform a historical load.</span></span> <span data-ttu-id="ebc12-145">平行複製功能適用於 **單一活動執行**。</span><span class="sxs-lookup"><span data-stu-id="ebc12-145">The parallel copy capability applies to a **single activity run**.</span></span>

<span data-ttu-id="ebc12-146">讓我們看一下範例案例。</span><span class="sxs-lookup"><span data-stu-id="ebc12-146">Let's look at a sample scenario.</span></span> <span data-ttu-id="ebc12-147">在下列範例中，需要處理多個過往配量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-147">In the following example, multiple slices from the past need to be processed.</span></span> <span data-ttu-id="ebc12-148">Data Factory 會對每個配量執行一個複製活動執行個體 (活動執行)：</span><span class="sxs-lookup"><span data-stu-id="ebc12-148">Data Factory runs an instance of Copy Activity (an activity run) for each slice:</span></span>

* <span data-ttu-id="ebc12-149">第 1 個活動時段 (1 AM 至 2 AM) 的資料配量 ==> 活動執行 1</span><span class="sxs-lookup"><span data-stu-id="ebc12-149">The data slice from the first activity window (1 AM to 2 AM) ==> Activity run 1</span></span>
* <span data-ttu-id="ebc12-150">第 2 個活動時段 (2 AM 至 3 AM) 的資料配量 ==> 活動執行 2</span><span class="sxs-lookup"><span data-stu-id="ebc12-150">The data slice from the second activity window (2 AM to 3 AM) ==> Activity run 2</span></span>
* <span data-ttu-id="ebc12-151">第 3 個活動時段 (3 AM 至 4 AM) 的資料配量 ==> 活動執行 3</span><span class="sxs-lookup"><span data-stu-id="ebc12-151">The data slice from the second activity window (3 AM to 4 AM) ==> Activity run 3</span></span>

<span data-ttu-id="ebc12-152">依此類推。</span><span class="sxs-lookup"><span data-stu-id="ebc12-152">And so on.</span></span>

<span data-ttu-id="ebc12-153">在此範例中，當**並行**值設定為 2，**活動執行 1** 和**活動執行 2** 會**並行**複製兩個活動時段的資料，以改善資料移動效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-153">In this example, when the **concurrency** value is set to 2, **Activity run 1** and **Activity run 2** copy data from two activity windows **concurrently** to improve data movement performance.</span></span> <span data-ttu-id="ebc12-154">不過，如果有多個檔案與活動執行 1 相關聯，則資料移動服務一次只會從來源複製一個檔案到目的地。</span><span class="sxs-lookup"><span data-stu-id="ebc12-154">However, if multiple files are associated with Activity run 1, the data movement service copies files from the source to the destination one file at a time.</span></span>

### <a name="cloud-data-movement-units"></a><span data-ttu-id="ebc12-155">雲端資料移動單位</span><span class="sxs-lookup"><span data-stu-id="ebc12-155">Cloud data movement units</span></span>
<span data-ttu-id="ebc12-156">**雲端資料移動單位 (DMU)** 是一項量值，代表 Data Factory 中單一單位的能力 (CPU、記憶體和網路資源配置的組合)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-156">A **cloud data movement unit (DMU)** is a measure that represents the power (a combination of CPU, memory, and network resource allocation) of a single unit in Data Factory.</span></span> <span data-ttu-id="ebc12-157">DMU 可用於雲端到雲端的複製作業，但不可用於混合式複製。</span><span class="sxs-lookup"><span data-stu-id="ebc12-157">A DMU might be used in a cloud-to-cloud copy operation, but not in a hybrid copy.</span></span>

<span data-ttu-id="ebc12-158">根據預設，Data Factory 會使用單一個雲端 DMU 來執行單一個複製活動執行。</span><span class="sxs-lookup"><span data-stu-id="ebc12-158">By default, Data Factory uses a single cloud DMU to perform a single Copy Activity run.</span></span> <span data-ttu-id="ebc12-159">若要覆寫此預設值，請如下所示指定 **cloudDataMovementUnits** 屬性的值。</span><span class="sxs-lookup"><span data-stu-id="ebc12-159">To override this default, specify a value for the **cloudDataMovementUnits** property as follows.</span></span> <span data-ttu-id="ebc12-160">如需在為特定複製來源和接收設定更多單位時可能獲得之效能增益水準的相關資訊，請參閱 [效能參考](#performance-reference)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-160">For information about the level of performance gain you might get when you configure more units for a specific copy source and sink, see the [performance reference](#performance-reference).</span></span>

```json
"activities":[  
    {
        "name": "Sample copy activity",
        "description": "",
        "type": "Copy",
        "inputs": [{ "name": "InputDataset" }],
        "outputs": [{ "name": "OutputDataset" }],
        "typeProperties": {
            "source": {
                "type": "BlobSource",
            },
            "sink": {
                "type": "AzureDataLakeStoreSink"
            },
            "cloudDataMovementUnits": 32
        }
    }
]
```
<span data-ttu-id="ebc12-161">cloudDataMovementUnits 屬性的允許值是 1 (預設值)、2、4、8、16 和 32。</span><span class="sxs-lookup"><span data-stu-id="ebc12-161">The **allowed values** for the **cloudDataMovementUnits** property are 1 (default), 2, 4, 8, 16, 32.</span></span> <span data-ttu-id="ebc12-162">根據您的資料模式，複製作業會在執行階段使用的 **實際雲端 DMU 數目** 等於或小於所設定的值。</span><span class="sxs-lookup"><span data-stu-id="ebc12-162">The **actual number of cloud DMUs** that the copy operation uses at run time is equal to or less than the configured value, depending on your data pattern.</span></span>

> [!NOTE]
> <span data-ttu-id="ebc12-163">如果您需要更多雲端 DMU 以提高輸送量，請連絡 [Azure 支援](https://azure.microsoft.com/support/)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-163">If you need more cloud DMUs for a higher throughput, contact [Azure support](https://azure.microsoft.com/support/).</span></span> <span data-ttu-id="ebc12-164">目前只有當您是將多個檔案從 Blob 儲存體/Data Lake Store/Amazon S3/雲端 FTP/雲端 SFTP 複製到 Blob 儲存體/Data Lake Store/Azure SQL Database 時，設定 8 及 8 以上的值才有作用。</span><span class="sxs-lookup"><span data-stu-id="ebc12-164">Setting of 8 and above currently works only when you **copy multiple files from Blob storage/Data Lake Store/Amazon S3/cloud FTP/cloud SFTP to Blob storage/Data Lake Store/Azure SQL Database**.</span></span>
>

### <a name="parallelcopies"></a><span data-ttu-id="ebc12-165">parallelCopies</span><span class="sxs-lookup"><span data-stu-id="ebc12-165">parallelCopies</span></span>
<span data-ttu-id="ebc12-166">您可以使用 **parallelCopies** 屬性，來指出您想要複製活動使用的平行處理原則。</span><span class="sxs-lookup"><span data-stu-id="ebc12-166">You can use the **parallelCopies** property to indicate the parallelism that you want Copy Activity to use.</span></span> <span data-ttu-id="ebc12-167">您可以將此屬性視為複製活動內，可透過平行方式從來源讀取或寫入至接收資料存放區中的執行緒數目上限。</span><span class="sxs-lookup"><span data-stu-id="ebc12-167">You can think of this property as the maximum number of threads within Copy Activity that can read from your source or write to your sink data stores in parallel.</span></span>

<span data-ttu-id="ebc12-168">對於每個複製活動執行，Data Factory 會決定要用來從來源資料存放區複製資料到目的地資料存放區的平行複製數目。</span><span class="sxs-lookup"><span data-stu-id="ebc12-168">For each Copy Activity run, Data Factory determines the number of parallel copies to use to copy data from the source data store and to the destination data store.</span></span> <span data-ttu-id="ebc12-169">它會使用的預設平行複製數目取決於您所使用的來源和接收類型。</span><span class="sxs-lookup"><span data-stu-id="ebc12-169">The default number of parallel copies that it uses depends on the type of source and sink that you are using.</span></span>  

| <span data-ttu-id="ebc12-170">來源和接收</span><span class="sxs-lookup"><span data-stu-id="ebc12-170">Source and sink</span></span> | <span data-ttu-id="ebc12-171">由服務決定的預設平行複製計數</span><span class="sxs-lookup"><span data-stu-id="ebc12-171">Default parallel copy count determined by service</span></span> |
| --- | --- |
| <span data-ttu-id="ebc12-172">在檔案型存放區 (Blob 儲存體、Data Lake Store、Amazon S3、內部部署檔案系統、內部部署 HDFS) 之間複製資料</span><span class="sxs-lookup"><span data-stu-id="ebc12-172">Copy data between file-based stores (Blob storage; Data Lake Store; Amazon S3; an on-premises file system; an on-premises HDFS)</span></span> |<span data-ttu-id="ebc12-173">介於 1 到 32。</span><span class="sxs-lookup"><span data-stu-id="ebc12-173">Between 1 and 32.</span></span> <span data-ttu-id="ebc12-174">取決於檔案大小和用來在兩個雲端資料存放區之間複製資料的雲端資料移動單位數 (DMU)，或用於混合式複製的實體閘道器電腦組態 (複製資料至內部部署資料存放區或從內部部署資料存放區複製資料)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-174">Depends on the size of the files and the number of cloud data movement units (DMUs) used to copy data between two cloud data stores, or the physical configuration of the Gateway machine used for a hybrid copy (to copy data to or from an on-premises data store).</span></span> |
| <span data-ttu-id="ebc12-175">將資料從 **任何來源資料存放區複製到 Azure 表格儲存體**</span><span class="sxs-lookup"><span data-stu-id="ebc12-175">Copy data from **any source data store to Azure Table storage**</span></span> |<span data-ttu-id="ebc12-176">4</span><span class="sxs-lookup"><span data-stu-id="ebc12-176">4</span></span> |
| <span data-ttu-id="ebc12-177">所有其他來源和接收組</span><span class="sxs-lookup"><span data-stu-id="ebc12-177">All other source and sink pairs</span></span> |<span data-ttu-id="ebc12-178">1</span><span class="sxs-lookup"><span data-stu-id="ebc12-178">1</span></span> |

<span data-ttu-id="ebc12-179">通常，預設行為應可提供最佳輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-179">Usually, the default behavior should give you the best throughput.</span></span> <span data-ttu-id="ebc12-180">不過，若要控制裝載資料存放區之電腦上的負載或是調整複製效能，您可以選擇覆寫預設值並指定 **parallelCopies** 屬性的值。</span><span class="sxs-lookup"><span data-stu-id="ebc12-180">However, to control the load on machines that host your data stores, or to tune copy performance, you may choose to override the default value and specify a value for the **parallelCopies** property.</span></span> <span data-ttu-id="ebc12-181">該值必須介於 1 (含) 到 32 (含)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-181">The value must be between 1 and 32 (both inclusive).</span></span> <span data-ttu-id="ebc12-182">在執行階段，為獲得最佳效能，複製活動會使用小於或等於設定值的值。</span><span class="sxs-lookup"><span data-stu-id="ebc12-182">At run time, for the best performance, Copy Activity uses a value that is less than or equal to the value that you set.</span></span>

```json
"activities":[  
    {
        "name": "Sample copy activity",
        "description": "",
        "type": "Copy",
        "inputs": [{ "name": "InputDataset" }],
        "outputs": [{ "name": "OutputDataset" }],
        "typeProperties": {
            "source": {
                "type": "BlobSource",
            },
            "sink": {
                "type": "AzureDataLakeStoreSink"
            },
            "parallelCopies": 8
        }
    }
]
```
<span data-ttu-id="ebc12-183">注意事項：</span><span class="sxs-lookup"><span data-stu-id="ebc12-183">Points to note:</span></span>

* <span data-ttu-id="ebc12-184">在檔案型存放區之間複製資料時，**parallelCopies** 決定檔案層級的平行處理原則。</span><span class="sxs-lookup"><span data-stu-id="ebc12-184">When you copy data between file-based stores, the **parallelCopies** determine the parallelism at the file level.</span></span> <span data-ttu-id="ebc12-185">在底層則會自動且直接在單一檔案中進行區塊化，而這是設計來為指定的來源資料存放區類型使用最適合的區塊大小，以載入平行於和垂直於 parallelCopies 的資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-185">The chunking within a single file would happen underneath automatically and transparently, and it's designed to use the best suitable chunk size for a given source data store type to load data in parallel and orthogonal to parallelCopies.</span></span> <span data-ttu-id="ebc12-186">資料移動服務在執行階段用於複製作業的實際平行複製數目不會超過您擁有的檔案數目。</span><span class="sxs-lookup"><span data-stu-id="ebc12-186">The actual number of parallel copies the data movement service uses for the copy operation at run time is no more than the number of files you have.</span></span> <span data-ttu-id="ebc12-187">如果複製行為是 **mergeFile**，則複製活動無法利用檔案層級的平行處理原則。</span><span class="sxs-lookup"><span data-stu-id="ebc12-187">If the copy behavior is **mergeFile**, Copy Activity cannot take advantage of file-level parallelism.</span></span>
* <span data-ttu-id="ebc12-188">在為 **parallelCopies** 屬性指定值時，請考慮來源和接收資料存放區的負載會增加，而如果是混合式複製，則閘道器的負載會增加。</span><span class="sxs-lookup"><span data-stu-id="ebc12-188">When you specify a value for the **parallelCopies** property, consider the load increase on your source and sink data stores, and to gateway if it is a hybrid copy.</span></span> <span data-ttu-id="ebc12-189">當您有多個活動或是會對相同資料存放區執行相同活動的並行執行時，尤其如此。</span><span class="sxs-lookup"><span data-stu-id="ebc12-189">This happens especially when you have multiple activities or concurrent runs of the same activities that run against the same data store.</span></span> <span data-ttu-id="ebc12-190">如果您注意到資料存放區或閘道器已無法應付負載，請減少 **parallelCopies** 值以減輕負載。</span><span class="sxs-lookup"><span data-stu-id="ebc12-190">If you notice that either the data store or Gateway is overwhelmed with the load, decrease the **parallelCopies** value to relieve the load.</span></span>
* <span data-ttu-id="ebc12-191">將資料從非檔案型存放區複製到檔案型存放區時，資料移動服務會忽略 **parallelCopies** 屬性。</span><span class="sxs-lookup"><span data-stu-id="ebc12-191">When you copy data from stores that are not file-based to stores that are file-based, the data movement service ignores the **parallelCopies** property.</span></span> <span data-ttu-id="ebc12-192">即使已指定平行處理原則，也不會套用於此案例。</span><span class="sxs-lookup"><span data-stu-id="ebc12-192">Even if parallelism is specified, it's not applied in this case.</span></span>

> [!NOTE]
> <span data-ttu-id="ebc12-193">您必須使用 1.11 版或更新版本的資料管理閘道，才能在進行混合式複製時使用 **parallelCopies** 功能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-193">You must use Data Management Gateway version 1.11 or later to use the **parallelCopies** feature when you do a hybrid copy.</span></span>
>
>

<span data-ttu-id="ebc12-194">若要更妥善地使用這兩個屬性，以及增強您的資料移動輸送量，請參閱 [範例使用案例](#case-study-use-parallel-copy)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-194">To better use these two properties, and to enhance your data movement throughput, see the [sample use cases](#case-study-use-parallel-copy).</span></span> <span data-ttu-id="ebc12-195">您不需要設定 **parallelCopies** 就能利用預設行為。</span><span class="sxs-lookup"><span data-stu-id="ebc12-195">You don't need to configure **parallelCopies** to take advantage of the default behavior.</span></span> <span data-ttu-id="ebc12-196">如果您有設定且 **parallelCopies** 太小，將可能無法充分利用多個雲端 DMU。</span><span class="sxs-lookup"><span data-stu-id="ebc12-196">If you do configure and **parallelCopies** is too small, multiple cloud DMUs might not be fully utilized.</span></span>  

### <a name="billing-impact"></a><span data-ttu-id="ebc12-197">計費影響</span><span class="sxs-lookup"><span data-stu-id="ebc12-197">Billing impact</span></span>
<span data-ttu-id="ebc12-198">請 **務必** 要記住，您必須根據複製作業的總時間付費。</span><span class="sxs-lookup"><span data-stu-id="ebc12-198">It's **important** to remember that you are charged based on the total time of the copy operation.</span></span> <span data-ttu-id="ebc12-199">若過去某複製作業使用 1 個雲端單位花費 1 小時，現在使用 4 個雲端單位花費 15 分鐘，則兩者的整體費用幾乎相同。</span><span class="sxs-lookup"><span data-stu-id="ebc12-199">If a copy job used to take one hour with one cloud unit and now it takes 15 minutes with four cloud units, the overall bill remains almost the same.</span></span> <span data-ttu-id="ebc12-200">例如，您使用 4 個雲端單位。</span><span class="sxs-lookup"><span data-stu-id="ebc12-200">For example, you use four cloud units.</span></span> <span data-ttu-id="ebc12-201">第 1 個雲端單位費時 10 分鐘、第 2 個單位費時 10 分鐘、第 3 個單位費時 5 分鐘、第 4 個單位費時 5 分鐘，以上全都在一個複製活動執行內。</span><span class="sxs-lookup"><span data-stu-id="ebc12-201">The first cloud unit spends 10 minutes, the second one, 10 minutes, the third one, 5 minutes, and the fourth one, 5 minutes, all in one Copy Activity run.</span></span> <span data-ttu-id="ebc12-202">您必須支付總複製 (資料移動) 時間的費用，亦即 10 + 10 + 5 + 5 = 30 分鐘。</span><span class="sxs-lookup"><span data-stu-id="ebc12-202">You are charged for the total copy (data movement) time, which is 10 + 10 + 5 + 5 = 30 minutes.</span></span> <span data-ttu-id="ebc12-203">是否使用 **parallelCopies** 對計費沒有任何影響。</span><span class="sxs-lookup"><span data-stu-id="ebc12-203">Using **parallelCopies** does not affect billing.</span></span>

## <a name="staged-copy"></a><span data-ttu-id="ebc12-204">分段複製</span><span class="sxs-lookup"><span data-stu-id="ebc12-204">Staged copy</span></span>
<span data-ttu-id="ebc12-205">從來源資料存放區將資料複製到接收資料存放區時，您可以選擇使用 Blob 儲存體做為過渡暫存存放區。</span><span class="sxs-lookup"><span data-stu-id="ebc12-205">When you copy data from a source data store to a sink data store, you might choose to use Blob storage as an interim staging store.</span></span> <span data-ttu-id="ebc12-206">暫存在下列情況下特別有用︰</span><span class="sxs-lookup"><span data-stu-id="ebc12-206">Staging is especially useful in the following cases:</span></span>

1. <span data-ttu-id="ebc12-207">**您想要透過 PolyBase 將資料從各種資料存放區內嵌至 SQL 資料倉儲**。</span><span class="sxs-lookup"><span data-stu-id="ebc12-207">**You want to ingest data from various data stores into SQL Data Warehouse via PolyBase**.</span></span> <span data-ttu-id="ebc12-208">SQL 資料倉儲使用 PolyBase 做為高輸送量機制，將大量資料載入 SQL 資料倉儲。</span><span class="sxs-lookup"><span data-stu-id="ebc12-208">SQL Data Warehouse uses PolyBase as a high-throughput mechanism to load a large amount of data into SQL Data Warehouse.</span></span> <span data-ttu-id="ebc12-209">不過，來源資料必須位於 Blob 儲存體，並符合額外的條件。</span><span class="sxs-lookup"><span data-stu-id="ebc12-209">However, the source data must be in Blob storage, and it must meet additional criteria.</span></span> <span data-ttu-id="ebc12-210">當您從 Blob 儲存體以外的資料存放區載入資料時，您可以啟用透過過渡暫存 Blob 儲存體的資料複製。</span><span class="sxs-lookup"><span data-stu-id="ebc12-210">When you load data from a data store other than Blob storage, you can activate data copying via interim staging Blob storage.</span></span> <span data-ttu-id="ebc12-211">在該情況下，Data Factory 會執行必要的資料轉換，以確保它符合 PolyBase 的需求。</span><span class="sxs-lookup"><span data-stu-id="ebc12-211">In that case, Data Factory performs the required data transformations to ensure that it meets the requirements of PolyBase.</span></span> <span data-ttu-id="ebc12-212">然後，它會使用 PolyBase 將資料載入 SQL 資料倉儲。</span><span class="sxs-lookup"><span data-stu-id="ebc12-212">Then it uses PolyBase to load data into SQL Data Warehouse.</span></span> <span data-ttu-id="ebc12-213">如需詳細資訊，請參閱 [使用 PolyBase 將資料載入 Azure SQL 資料倉儲](data-factory-azure-sql-data-warehouse-connector.md#use-polybase-to-load-data-into-azure-sql-data-warehouse)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-213">For more details, see [Use PolyBase to load data into Azure SQL Data Warehouse](data-factory-azure-sql-data-warehouse-connector.md#use-polybase-to-load-data-into-azure-sql-data-warehouse).</span></span> <span data-ttu-id="ebc12-214">如需使用案例的逐步解說，請參閱[使用 Azure Data Factory 在 15 分鐘內將 1 TB 載入至 Azure SQL 資料倉儲](data-factory-load-sql-data-warehouse.md)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-214">For a walkthrough with a use case, see [Load 1 TB into Azure SQL Data Warehouse under 15 minutes with Azure Data Factory](data-factory-load-sql-data-warehouse.md).</span></span>
2. <span data-ttu-id="ebc12-215">**有時透過慢速網路連接執行混合式資料移動 (也就是，在內部部署資料存放區和雲端資料存放區之間複製資料)**，需要一段時間。</span><span class="sxs-lookup"><span data-stu-id="ebc12-215">**Sometimes it takes a while to perform a hybrid data movement (that is, to copy between an on-premises data store and a cloud data store) over a slow network connection**.</span></span> <span data-ttu-id="ebc12-216">為了提升效能，您可以壓縮內部部署資料，減少移動資料到雲端中的暫存資料存放區所需的時間。</span><span class="sxs-lookup"><span data-stu-id="ebc12-216">To improve performance, you can compress the data on-premises so that it takes less time to move data to the staging data store in the cloud.</span></span> <span data-ttu-id="ebc12-217">然後，您可以先在暫存存放區中解壓縮資料，再將其載入至目的地資料存放區。</span><span class="sxs-lookup"><span data-stu-id="ebc12-217">Then you can decompress the data in the staging store before you load it into the destination data store.</span></span>
3. <span data-ttu-id="ebc12-218">**由於公司 IT 原則，您不想要在您的防火牆中開啟 80 和 443 以外的連接埠**。</span><span class="sxs-lookup"><span data-stu-id="ebc12-218">**You don't want to open ports other than port 80 and port 443 in your firewall, because of corporate IT policies**.</span></span> <span data-ttu-id="ebc12-219">例如，從內部部署資料存放區將資料複製到 Azure SQL Database 接收或 Azure SQL 資料倉儲接收時，您必須針對 Windows 防火牆和公司防火牆啟用連接埠 1433 上的輸出 TCP 通訊。</span><span class="sxs-lookup"><span data-stu-id="ebc12-219">For example, when you copy data from an on-premises data store to an Azure SQL Database sink or an Azure SQL Data Warehouse sink, you need to activate outbound TCP communication on port 1433 for both the Windows firewall and your corporate firewall.</span></span> <span data-ttu-id="ebc12-220">在此案例中，請利用閘道先在連接埠 443 上透過 HTTP 或 HTTPS 將資料複製到 Blob 儲存體暫存執行個體。</span><span class="sxs-lookup"><span data-stu-id="ebc12-220">In this scenario, take advantage of the gateway to first copy data to a Blob storage staging instance over HTTP or HTTPS on port 443.</span></span> <span data-ttu-id="ebc12-221">接著，將資料從 Blob 儲存體暫存載入到 SQL Database 或 SQL 資料倉儲。</span><span class="sxs-lookup"><span data-stu-id="ebc12-221">Then, load the data into SQL Database or SQL Data Warehouse from Blob storage staging.</span></span> <span data-ttu-id="ebc12-222">在此流程中，您不需要啟用連接埠 1433。</span><span class="sxs-lookup"><span data-stu-id="ebc12-222">In this flow, you don't need to enable port 1433.</span></span>

### <a name="how-staged-copy-works"></a><span data-ttu-id="ebc12-223">分段複製的運作方式</span><span class="sxs-lookup"><span data-stu-id="ebc12-223">How staged copy works</span></span>
<span data-ttu-id="ebc12-224">當您啟用暫存功能時，會先從來源資料存放區複製資料到暫存資料存放區 (自備)，</span><span class="sxs-lookup"><span data-stu-id="ebc12-224">When you activate the staging feature, first the data is copied from the source data store to the staging data store (bring your own).</span></span> <span data-ttu-id="ebc12-225">接著再從暫存資料存放區複製資料到接收資料存放區。</span><span class="sxs-lookup"><span data-stu-id="ebc12-225">Next, the data is copied from the staging data store to the sink data store.</span></span> <span data-ttu-id="ebc12-226">Data Factory 會自動為您管理 2 階段流程，</span><span class="sxs-lookup"><span data-stu-id="ebc12-226">Data Factory automatically manages the two-stage flow for you.</span></span> <span data-ttu-id="ebc12-227">也會在資料移動完成之後，清除暫存儲存體中的暫存資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-227">Data Factory also cleans up temporary data from the staging storage after the data movement is complete.</span></span>

<span data-ttu-id="ebc12-228">在雲端複製案例中 (來源與接收的資料存放區皆在雲端)，不會使用閘道。</span><span class="sxs-lookup"><span data-stu-id="ebc12-228">In the cloud copy scenario (both source and sink data stores are in the cloud), gateway is not used.</span></span> <span data-ttu-id="ebc12-229">Data Factory 服務會執行複製作業。</span><span class="sxs-lookup"><span data-stu-id="ebc12-229">The Data Factory service performs the copy operations.</span></span>

![分段複製：雲端案例](media/data-factory-copy-activity-performance/staged-copy-cloud-scenario.png)

<span data-ttu-id="ebc12-231">在混合式複製案例中 (來源是在內部部署而接收是在雲端)，閘道會從來源資料存放區將資料移動至暫存資料存放區。</span><span class="sxs-lookup"><span data-stu-id="ebc12-231">In the hybrid copy scenario (source is on-premises and sink is in the cloud), the gateway moves data from the source data store to a staging data store.</span></span> <span data-ttu-id="ebc12-232">Data Factory 服務會從暫存資料存放區將資料移動至接收資料存放區。</span><span class="sxs-lookup"><span data-stu-id="ebc12-232">Data Factory service moves data from the staging data store to the sink data store.</span></span> <span data-ttu-id="ebc12-233">反轉流程也支援透過暫存將資料從雲端資料存放區複製到內部部署資料存放區。</span><span class="sxs-lookup"><span data-stu-id="ebc12-233">Copying data from a cloud data store to an on-premises data store via staging also is supported with the reversed flow.</span></span>

![分段複製：混合式案例](media/data-factory-copy-activity-performance/staged-copy-hybrid-scenario.png)

<span data-ttu-id="ebc12-235">當您使用暫存存放區啟用資料移動時，您可以指定是否要在從來源資料存放區將資料移動至過渡或暫存資料存放區之前壓縮資料，然後在從過渡或暫存資料存放區將資料移動至接收資料存放區之前解壓縮資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-235">When you activate data movement by using a staging store, you can specify whether you want the data to be compressed before moving data from the source data store to an interim or staging data store, and then decompressed before moving data from an interim or staging data store to the sink data store.</span></span>

<span data-ttu-id="ebc12-236">目前您還無法使用暫存存放區在兩個內部部署資料存放區之間複製資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-236">Currently, you can't copy data between two on-premises data stores by using a staging store.</span></span> <span data-ttu-id="ebc12-237">我們預計此選項很快就會推出。</span><span class="sxs-lookup"><span data-stu-id="ebc12-237">We expect this option to be available soon.</span></span>

### <a name="configuration"></a><span data-ttu-id="ebc12-238">組態</span><span class="sxs-lookup"><span data-stu-id="ebc12-238">Configuration</span></span>
<span data-ttu-id="ebc12-239">在複製活動中設定 **enableStaging** 設定，指定您是否想要讓資料在載入至目的地資料存放區之前，暫存在 Blob 儲存體中。</span><span class="sxs-lookup"><span data-stu-id="ebc12-239">Configure the **enableStaging** setting in Copy Activity to specify whether you want the data to be staged in Blob storage before you load it into a destination data store.</span></span> <span data-ttu-id="ebc12-240">當您將 **enableStaging** 設定為 TRUE 時，請指定下一份資料表所列出的其他屬性。</span><span class="sxs-lookup"><span data-stu-id="ebc12-240">When you set **enableStaging** to TRUE, specify the additional properties listed in the next table.</span></span> <span data-ttu-id="ebc12-241">如果還未指定，您也需要建立 Azure 儲存體或儲存體共用存取簽章連結服務以供暫存使用。</span><span class="sxs-lookup"><span data-stu-id="ebc12-241">If you don’t have one, you also need to create an Azure Storage or Storage shared access signature-linked service for staging.</span></span>

| <span data-ttu-id="ebc12-242">屬性</span><span class="sxs-lookup"><span data-stu-id="ebc12-242">Property</span></span> | <span data-ttu-id="ebc12-243">說明</span><span class="sxs-lookup"><span data-stu-id="ebc12-243">Description</span></span> | <span data-ttu-id="ebc12-244">預設值</span><span class="sxs-lookup"><span data-stu-id="ebc12-244">Default value</span></span> | <span data-ttu-id="ebc12-245">必要</span><span class="sxs-lookup"><span data-stu-id="ebc12-245">Required</span></span> |
| --- | --- | --- | --- |
| <span data-ttu-id="ebc12-246">**enableStaging**</span><span class="sxs-lookup"><span data-stu-id="ebc12-246">**enableStaging**</span></span> |<span data-ttu-id="ebc12-247">指定您是否要透過過渡暫存存放區複製資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-247">Specify whether you want to copy data via an interim staging store.</span></span> |<span data-ttu-id="ebc12-248">False</span><span class="sxs-lookup"><span data-stu-id="ebc12-248">False</span></span> |<span data-ttu-id="ebc12-249">否</span><span class="sxs-lookup"><span data-stu-id="ebc12-249">No</span></span> |
| <span data-ttu-id="ebc12-250">**linkedServiceName**</span><span class="sxs-lookup"><span data-stu-id="ebc12-250">**linkedServiceName**</span></span> |<span data-ttu-id="ebc12-251">指定 [AzureStorage](data-factory-azure-blob-connector.md#azure-storage-linked-service) 或 [AzureStorageSas](data-factory-azure-blob-connector.md#azure-storage-sas-linked-service) 連結服務的名稱，以代表您用來做為過渡暫存存放區的儲存體執行個體。</span><span class="sxs-lookup"><span data-stu-id="ebc12-251">Specify the name of an [AzureStorage](data-factory-azure-blob-connector.md#azure-storage-linked-service) or [AzureStorageSas](data-factory-azure-blob-connector.md#azure-storage-sas-linked-service) linked service, which refers to the instance of Storage that you use as an interim staging store.</span></span> <br/><br/> <span data-ttu-id="ebc12-252">您無法使用具有共用存取簽章的儲存體來透過 PolyBase 將資料載入至 SQL 資料倉儲。</span><span class="sxs-lookup"><span data-stu-id="ebc12-252">You cannot use Storage with a shared access signature to load data into SQL Data Warehouse via PolyBase.</span></span> <span data-ttu-id="ebc12-253">您可以將它用於其他所有案例。</span><span class="sxs-lookup"><span data-stu-id="ebc12-253">You can use it in all other scenarios.</span></span> |<span data-ttu-id="ebc12-254">N/A</span><span class="sxs-lookup"><span data-stu-id="ebc12-254">N/A</span></span> |<span data-ttu-id="ebc12-255">是，當 **enableStaging** 設為 TRUE</span><span class="sxs-lookup"><span data-stu-id="ebc12-255">Yes, when **enableStaging** is set to TRUE</span></span> |
| <span data-ttu-id="ebc12-256">**路徑**</span><span class="sxs-lookup"><span data-stu-id="ebc12-256">**path**</span></span> |<span data-ttu-id="ebc12-257">指定要包含分段資料的 Blob 儲存體路徑。</span><span class="sxs-lookup"><span data-stu-id="ebc12-257">Specify the Blob storage path that you want to contain the staged data.</span></span> <span data-ttu-id="ebc12-258">如果未提供路徑，服務會建立容器來儲存暫存資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-258">If you do not provide a path, the service creates a container to store temporary data.</span></span> <br/><br/> <span data-ttu-id="ebc12-259">只有在使用具有共用存取簽章的儲存體時，或需要讓暫存資料位於特定位置時，才指定路徑。</span><span class="sxs-lookup"><span data-stu-id="ebc12-259">Specify a path only if you use Storage with a shared access signature, or you require temporary data to be in a specific location.</span></span> |<span data-ttu-id="ebc12-260">N/A</span><span class="sxs-lookup"><span data-stu-id="ebc12-260">N/A</span></span> |<span data-ttu-id="ebc12-261">否</span><span class="sxs-lookup"><span data-stu-id="ebc12-261">No</span></span> |
| <span data-ttu-id="ebc12-262">**enableCompression**</span><span class="sxs-lookup"><span data-stu-id="ebc12-262">**enableCompression**</span></span> |<span data-ttu-id="ebc12-263">指定將資料複製到目的地之前，是否應該壓縮資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-263">Specifies whether data should be compressed before it is copied to the destination.</span></span> <span data-ttu-id="ebc12-264">此設定可減少傳輸的資料量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-264">This setting reduces the volume of data being transferred.</span></span> |<span data-ttu-id="ebc12-265">False</span><span class="sxs-lookup"><span data-stu-id="ebc12-265">False</span></span> |<span data-ttu-id="ebc12-266">否</span><span class="sxs-lookup"><span data-stu-id="ebc12-266">No</span></span> |

<span data-ttu-id="ebc12-267">以下是具有上表所述屬性的「複製活動」的範例定義︰</span><span class="sxs-lookup"><span data-stu-id="ebc12-267">Here's a sample definition of Copy Activity with the properties that are described in the preceding table:</span></span>

```json
"activities":[  
{
    "name": "Sample copy activity",
    "type": "Copy",
    "inputs": [{ "name": "OnpremisesSQLServerInput" }],
    "outputs": [{ "name": "AzureSQLDBOutput" }],
    "typeProperties": {
        "source": {
            "type": "SqlSource",
        },
        "sink": {
            "type": "SqlSink"
        },
        "enableStaging": true,
        "stagingSettings": {
            "linkedServiceName": "MyStagingBlob",
            "path": "stagingcontainer/path",
            "enableCompression": true
        }
    }
}
]
```

### <a name="billing-impact"></a><span data-ttu-id="ebc12-268">計費影響</span><span class="sxs-lookup"><span data-stu-id="ebc12-268">Billing impact</span></span>
<span data-ttu-id="ebc12-269">我們將會根據兩個步驟向您收費：複製持續時間和複製類型。</span><span class="sxs-lookup"><span data-stu-id="ebc12-269">You are charged based on two steps: copy duration and copy type.</span></span>

* <span data-ttu-id="ebc12-270">在雲端複製期間使用暫存時 (從雲端資料存放區將資料複製到其他雲端資料存放區)，所收費用為 [步驟 1 和步驟 2 的複製持續時間總和] x [雲端複製單位價格]。</span><span class="sxs-lookup"><span data-stu-id="ebc12-270">When you use staging during a cloud copy (copying data from a cloud data store to another cloud data store), you are charged the [sum of copy duration for step 1 and step 2] x [cloud copy unit price].</span></span>
* <span data-ttu-id="ebc12-271">在混合式複製期間使用暫存時 (從內部部署資料存放區將資料複製到雲端資料存放區)，所收費用為 [混合式複製持續時間] x [混合式複製單位價格] + [雲端複製持續時間] x [雲端複製單位價格]。</span><span class="sxs-lookup"><span data-stu-id="ebc12-271">When you use staging during a hybrid copy (copying data from an on-premises data store to a cloud data store), you are charged for [hybrid copy duration] x [hybrid copy unit price] + [cloud copy duration] x [cloud copy unit price].</span></span>

## <a name="performance-tuning-steps"></a><span data-ttu-id="ebc12-272">效能微調步驟</span><span class="sxs-lookup"><span data-stu-id="ebc12-272">Performance tuning steps</span></span>
<span data-ttu-id="ebc12-273">建議您採取下列步驟來微調 Data Factory 服務搭配使用複製活動時的效能︰</span><span class="sxs-lookup"><span data-stu-id="ebc12-273">We suggest that you take these steps to tune the performance of your Data Factory service with Copy Activity:</span></span>

1. <span data-ttu-id="ebc12-274">**建立基準**。</span><span class="sxs-lookup"><span data-stu-id="ebc12-274">**Establish a baseline**.</span></span> <span data-ttu-id="ebc12-275">在開發階段，對具有代表性的資料範例使用複製活動來測試您的管線。</span><span class="sxs-lookup"><span data-stu-id="ebc12-275">During the development phase, test your pipeline by using Copy Activity against a representative data sample.</span></span> <span data-ttu-id="ebc12-276">您可以使用 Data Factory 的 [切割模型](data-factory-scheduling-and-execution.md) 來限制您所使用的資料量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-276">You can use the Data Factory [slicing model](data-factory-scheduling-and-execution.md) to limit the amount of data you work with.</span></span>

   <span data-ttu-id="ebc12-277">藉由使用 **監視及管理應用程式**來收集執行時間和效能特性。</span><span class="sxs-lookup"><span data-stu-id="ebc12-277">Collect execution time and performance characteristics by using the **Monitoring and Management App**.</span></span> <span data-ttu-id="ebc12-278">在 Data Factory 首頁上選擇 [監視及管理]。</span><span class="sxs-lookup"><span data-stu-id="ebc12-278">Choose **Monitor & Manage** on your Data Factory home page.</span></span> <span data-ttu-id="ebc12-279">在樹狀檢視中選擇 [輸出資料集] 。</span><span class="sxs-lookup"><span data-stu-id="ebc12-279">In the tree view, choose the **output dataset**.</span></span> <span data-ttu-id="ebc12-280">在 [活動時段]  清單中選擇 [複製活動執行]。</span><span class="sxs-lookup"><span data-stu-id="ebc12-280">In the **Activity Windows** list, choose the Copy Activity run.</span></span> <span data-ttu-id="ebc12-281"> 會列出複製活動的持續時間和所複製資料的大小。</span><span class="sxs-lookup"><span data-stu-id="ebc12-281">**Activity Windows** lists the Copy Activity duration and the size of the data that's copied.</span></span> <span data-ttu-id="ebc12-282">輸送量會列在 [活動時段總管] 中。</span><span class="sxs-lookup"><span data-stu-id="ebc12-282">The throughput is listed in **Activity Window Explorer**.</span></span> <span data-ttu-id="ebc12-283">若要深入了解該應用程式，請參閱 [使用監視及管理應用程式來監視及管理 Azure Data Factory 管線](data-factory-monitor-manage-app.md)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-283">To learn more about the app, see [Monitor and manage Azure Data Factory pipelines by using the Monitoring and Management App](data-factory-monitor-manage-app.md).</span></span>

   ![活動執行詳細資料](./media/data-factory-copy-activity-performance/mmapp-activity-run-details.png)

   <span data-ttu-id="ebc12-285">在本文稍後，您可以將您案例的效能和組態與我們的測試中複製活動的 [效能參考](#performance-reference) 進行比較。</span><span class="sxs-lookup"><span data-stu-id="ebc12-285">Later in the article, you can compare the performance and configuration of your scenario to Copy Activity’s [performance reference](#performance-reference) from our tests.</span></span>
2. <span data-ttu-id="ebc12-286">**效能診斷與最佳化**。</span><span class="sxs-lookup"><span data-stu-id="ebc12-286">**Diagnose and optimize performance**.</span></span> <span data-ttu-id="ebc12-287">如果您觀察到的效能不符預期，您必須找出效能瓶頸。</span><span class="sxs-lookup"><span data-stu-id="ebc12-287">If the performance you observe doesn't meet your expectations, you need to identify performance bottlenecks.</span></span> <span data-ttu-id="ebc12-288">然後將效能最佳化，以消除或減少瓶頸的影響。</span><span class="sxs-lookup"><span data-stu-id="ebc12-288">Then, optimize performance to remove or reduce the effect of bottlenecks.</span></span> <span data-ttu-id="ebc12-289">效能診斷的完整說明不在本文的討論之列，但以下是一些常見的考量：</span><span class="sxs-lookup"><span data-stu-id="ebc12-289">A full description of performance diagnosis is beyond the scope of this article, but here are some common considerations:</span></span>

   * <span data-ttu-id="ebc12-290">效能功能︰</span><span class="sxs-lookup"><span data-stu-id="ebc12-290">Performance features:</span></span>
     * [<span data-ttu-id="ebc12-291">平行複製</span><span class="sxs-lookup"><span data-stu-id="ebc12-291">Parallel copy</span></span>](#parallel-copy)
     * [<span data-ttu-id="ebc12-292">雲端資料移動單位</span><span class="sxs-lookup"><span data-stu-id="ebc12-292">Cloud data movement units</span></span>](#cloud-data-movement-units)
     * [<span data-ttu-id="ebc12-293">分段複製</span><span class="sxs-lookup"><span data-stu-id="ebc12-293">Staged copy</span></span>](#staged-copy)
     * [<span data-ttu-id="ebc12-294">資料管理閘道延展性</span><span class="sxs-lookup"><span data-stu-id="ebc12-294">Data Management Gateway scalability</span></span>](data-factory-data-management-gateway-high-availability-scalability.md)
   * [<span data-ttu-id="ebc12-295">資料管理閘道</span><span class="sxs-lookup"><span data-stu-id="ebc12-295">Data Management Gateway</span></span>](#considerations-for-data-management-gateway)
   * [<span data-ttu-id="ebc12-296">來源</span><span class="sxs-lookup"><span data-stu-id="ebc12-296">Source</span></span>](#considerations-for-the-source)
   * [<span data-ttu-id="ebc12-297">接收</span><span class="sxs-lookup"><span data-stu-id="ebc12-297">Sink</span></span>](#considerations-for-the-sink)
   * [<span data-ttu-id="ebc12-298">序列化和還原序列化</span><span class="sxs-lookup"><span data-stu-id="ebc12-298">Serialization and deserialization</span></span>](#considerations-for-serialization-and-deserialization)
   * [<span data-ttu-id="ebc12-299">壓縮</span><span class="sxs-lookup"><span data-stu-id="ebc12-299">Compression</span></span>](#considerations-for-compression)
   * [<span data-ttu-id="ebc12-300">資料行對應</span><span class="sxs-lookup"><span data-stu-id="ebc12-300">Column mapping</span></span>](#considerations-for-column-mapping)
   * [<span data-ttu-id="ebc12-301">其他考量</span><span class="sxs-lookup"><span data-stu-id="ebc12-301">Other considerations</span></span>](#other-considerations)
3. <span data-ttu-id="ebc12-302">**將組態擴展至整個資料集**。</span><span class="sxs-lookup"><span data-stu-id="ebc12-302">**Expand the configuration to your entire data set**.</span></span> <span data-ttu-id="ebc12-303">當您對執行結果及效能感到滿意時，您可以將定義和管線作用期間擴展為涵蓋整個資料集。</span><span class="sxs-lookup"><span data-stu-id="ebc12-303">When you're satisfied with the execution results and performance, you can expand the definition and pipeline active period to cover your entire data set.</span></span>

## <a name="considerations-for-data-management-gateway"></a><span data-ttu-id="ebc12-304">資料管理閘道的考量</span><span class="sxs-lookup"><span data-stu-id="ebc12-304">Considerations for Data Management Gateway</span></span>
<span data-ttu-id="ebc12-305">**閘道設定**：建議您使用專用的電腦來裝載資料管理閘道。</span><span class="sxs-lookup"><span data-stu-id="ebc12-305">**Gateway setup**: We recommend that you use a dedicated machine to host Data Management Gateway.</span></span> <span data-ttu-id="ebc12-306">請參閱[使用資料管理閘道的考量](data-factory-data-management-gateway.md#considerations-for-using-gateway)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-306">See [Considerations for using Data Management Gateway](data-factory-data-management-gateway.md#considerations-for-using-gateway).</span></span>  

<span data-ttu-id="ebc12-307">**閘道監控與相應增加/相應放大**：具有一或多個閘道節點的單一邏輯閘道可同時提供多個複製活動進行。</span><span class="sxs-lookup"><span data-stu-id="ebc12-307">**Gateway monitoring and scale-up/out**: A single logical gateway with one or more gateway nodes can serve multiple Copy Activity runs at the same time concurrently.</span></span> <span data-ttu-id="ebc12-308">您可以在 Azure 入口網站中，檢視閘道機器近乎即時的資訊使用率 (CPU、記憶體、網路 (輸入/輸出) 等) 快照集，以及執行的並行作業數目與限制，請參閱[在入口網站中監視閘道](data-factory-data-management-gateway.md#monitor-gateway-in-the-portal)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-308">You can view near-real time snapshot of resource utilization (CPU, memory, network(in/out), etc.) on a gateway machine as well as the number of concurrent jobs running versus limit in the Azure portal, see [Monitor gateway in the portal](data-factory-data-management-gateway.md#monitor-gateway-in-the-portal).</span></span> <span data-ttu-id="ebc12-309">如果您對於混合式資料移動 (包含大量的並行複製活動執行或需要複製的大量資料) 有很大的需求，請考慮[相應增加或相應放大閘道](data-factory-data-management-gateway-high-availability-scalability.md#scale-considerations)，以充份利用您的資源，或佈建更多資源以賦予複製能力。</span><span class="sxs-lookup"><span data-stu-id="ebc12-309">If you have heavy need on hybrid data movement either with large number of concurrent copy activity runs or with large volume of data to copy, consider to [scale up or scale out gateway](data-factory-data-management-gateway-high-availability-scalability.md#scale-considerations) so as to better utilize your resource or to provision more resource to empower copy.</span></span> 

## <a name="considerations-for-the-source"></a><span data-ttu-id="ebc12-310">來源的考量</span><span class="sxs-lookup"><span data-stu-id="ebc12-310">Considerations for the source</span></span>
### <a name="general"></a><span data-ttu-id="ebc12-311">一般</span><span class="sxs-lookup"><span data-stu-id="ebc12-311">General</span></span>
<span data-ttu-id="ebc12-312">請確定基礎資料存放區未被執行於其上或對其執行的其他工作負載全面佔據。</span><span class="sxs-lookup"><span data-stu-id="ebc12-312">Be sure that the underlying data store is not overwhelmed by other workloads that are running on or against it.</span></span>

<span data-ttu-id="ebc12-313">針對 Microsoft 資料存放區，請參閱資料存放區特定的 [監視和微調主題](#performance-reference) ，以協助您了解資料存放區的效能特性、最小化回應時間和最大化輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-313">For Microsoft data stores, see [monitoring and tuning topics](#performance-reference) that are specific to data stores, and help you understand data store performance characteristics, minimize response times, and maximize throughput.</span></span>

<span data-ttu-id="ebc12-314">如果您要從 Blob 儲存體複製資料到 SQL 資料倉儲，請考慮使用 **PolyBase** 以提升效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-314">If you copy data from Blob storage to SQL Data Warehouse, consider using **PolyBase** to boost performance.</span></span> <span data-ttu-id="ebc12-315">如需詳細資訊，請參閱 [使用 PolyBase 將資料載入 Azure SQL 資料倉儲](data-factory-azure-sql-data-warehouse-connector.md#use-polybase-to-load-data-into-azure-sql-data-warehouse) 。</span><span class="sxs-lookup"><span data-stu-id="ebc12-315">See [Use PolyBase to load data into Azure SQL Data Warehouse](data-factory-azure-sql-data-warehouse-connector.md#use-polybase-to-load-data-into-azure-sql-data-warehouse) for details.</span></span> <span data-ttu-id="ebc12-316">如需使用案例的逐步解說，請參閱[使用 Azure Data Factory 在 15 分鐘內將 1 TB 載入至 Azure SQL 資料倉儲](data-factory-load-sql-data-warehouse.md)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-316">For a walkthrough with a use case, see [Load 1 TB into Azure SQL Data Warehouse under 15 minutes with Azure Data Factory](data-factory-load-sql-data-warehouse.md).</span></span>

### <a name="file-based-data-stores"></a><span data-ttu-id="ebc12-317">以檔案為基礎的資料存放區</span><span class="sxs-lookup"><span data-stu-id="ebc12-317">File-based data stores</span></span>
<span data-ttu-id="ebc12-318">*(包括 Blob 儲存體、Data Lake Store、Amazon S3、內部部署檔案系統及內部部署 HDFS)*</span><span class="sxs-lookup"><span data-stu-id="ebc12-318">*(Includes Blob storage, Data Lake Store, Amazon S3, on-premises file systems, and on-premises HDFS)*</span></span>

* <span data-ttu-id="ebc12-319">**平均檔案大小和檔案計數**：複製活動會一次傳送一個檔案的資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-319">**Average file size and file count**: Copy Activity transfers data one file at a time.</span></span> <span data-ttu-id="ebc12-320">在要移動的資料量相同的前提下，如果資料包含許多個小型檔案而非少數幾個大型檔案，其整體輸送量會較低，因為每個檔案都需要啟動程序階段。</span><span class="sxs-lookup"><span data-stu-id="ebc12-320">With the same amount of data to be moved, the overall throughput is lower if the data consists of many small files rather than a few large files due to the bootstrap phase for each file.</span></span> <span data-ttu-id="ebc12-321">因此，可能的話，請將小型檔案合併為較大的檔案，以提高輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-321">Therefore, if possible, combine small files into larger files to gain higher throughput.</span></span>
* <span data-ttu-id="ebc12-322">**檔案格式和壓縮**：如需可改善效能的其他方法，請參閱[序列化和還原序列化的考量](#considerations-for-serialization-and-deserialization)和[壓縮的考量](#considerations-for-compression)小節。</span><span class="sxs-lookup"><span data-stu-id="ebc12-322">**File format and compression**: For more ways to improve performance, see the [Considerations for serialization and deserialization](#considerations-for-serialization-and-deserialization) and [Considerations for compression](#considerations-for-compression) sections.</span></span>
* <span data-ttu-id="ebc12-323">對於必須使用**資料管理閘道**的**內部部署檔案系統**案例，請參閱[資料管理閘道的考量](#considerations-for-data-management-gateway)一節。</span><span class="sxs-lookup"><span data-stu-id="ebc12-323">For the **on-premises file system** scenario, in which **Data Management Gateway** is required, see the [Considerations for Data Management Gateway](#considerations-for-data-management-gateway) section.</span></span>

### <a name="relational-data-stores"></a><span data-ttu-id="ebc12-324">關聯式資料存放區</span><span class="sxs-lookup"><span data-stu-id="ebc12-324">Relational data stores</span></span>
<span data-ttu-id="ebc12-325">*(包括 SQL Database、SQL 資料倉儲、Amazon Redshift、SQL Server 資料庫，以及 Oracle、MySQL、DB2、Teradata、Sybase 和 PostgreSQL 資料庫等)*</span><span class="sxs-lookup"><span data-stu-id="ebc12-325">*(Includes SQL Database; SQL Data Warehouse; Amazon Redshift; SQL Server databases; and Oracle, MySQL, DB2, Teradata, Sybase, and PostgreSQL databases, etc.)*</span></span>

* <span data-ttu-id="ebc12-326">**資料模式**︰資料表結構描述對複製輸送量會有影響。</span><span class="sxs-lookup"><span data-stu-id="ebc12-326">**Data pattern**: Your table schema affects copy throughput.</span></span> <span data-ttu-id="ebc12-327">若要複製相同的資料量，較大的資料列大小會有優於較小資料列大小的效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-327">A large row size gives you a better performance than small row size, to copy the same amount of data.</span></span> <span data-ttu-id="ebc12-328">這是因為資料庫可以更有效率地擷取包含較少資料列的較少資料批次。</span><span class="sxs-lookup"><span data-stu-id="ebc12-328">The reason is that the database can more efficiently retrieve fewer batches of data that contain fewer rows.</span></span>
* <span data-ttu-id="ebc12-329">**查詢或預存程序**：最佳化您在複製活動來源中指定的查詢或預存程序邏輯，以更有效率地擷取資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-329">**Query or stored procedure**: Optimize the logic of the query or stored procedure you specify in the Copy Activity source to fetch data more efficiently.</span></span>
* <span data-ttu-id="ebc12-330">對於必須使用**資料管理閘道**的**內部部署關聯式資料庫** (例如 SQL Server 和 Oracle)，請參閱[資料管理閘道的考量](#considerations-on-data-management-gateway)一節。</span><span class="sxs-lookup"><span data-stu-id="ebc12-330">For **on-premises relational databases**, such as SQL Server and Oracle, which require the use of **Data Management Gateway**, see the [Considerations for Data Management Gateway](#considerations-on-data-management-gateway) section.</span></span>

## <a name="considerations-for-the-sink"></a><span data-ttu-id="ebc12-331">接收的考量</span><span class="sxs-lookup"><span data-stu-id="ebc12-331">Considerations for the sink</span></span>
### <a name="general"></a><span data-ttu-id="ebc12-332">一般</span><span class="sxs-lookup"><span data-stu-id="ebc12-332">General</span></span>
<span data-ttu-id="ebc12-333">請確定基礎資料存放區未被執行於其上或對其執行的其他工作負載全面佔據。</span><span class="sxs-lookup"><span data-stu-id="ebc12-333">Be sure that the underlying data store is not overwhelmed by other workloads that are running on or against it.</span></span>

<span data-ttu-id="ebc12-334">針對 Microsoft 資料存放區，請參閱資料存放區特定的 [監視和微調主題](#performance-reference) 。</span><span class="sxs-lookup"><span data-stu-id="ebc12-334">For Microsoft data stores, refer to [monitoring and tuning topics](#performance-reference) that are specific to data stores.</span></span> <span data-ttu-id="ebc12-335">這些主題可協助您了解資料存放區的效能特性，以及如何最小化回應時間和最大化輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-335">These topics can help you understand data store performance characteristics and how to minimize response times and maximize throughput.</span></span>

<span data-ttu-id="ebc12-336">如果您要從 **Blob 儲存體**複製資料到 **SQL 資料倉儲**，請考慮使用 **PolyBase** 以提升效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-336">If you are copying data from **Blob storage** to **SQL Data Warehouse**, consider using **PolyBase** to boost performance.</span></span> <span data-ttu-id="ebc12-337">如需詳細資訊，請參閱 [使用 PolyBase 將資料載入 Azure SQL 資料倉儲](data-factory-azure-sql-data-warehouse-connector.md#use-polybase-to-load-data-into-azure-sql-data-warehouse) 。</span><span class="sxs-lookup"><span data-stu-id="ebc12-337">See [Use PolyBase to load data into Azure SQL Data Warehouse](data-factory-azure-sql-data-warehouse-connector.md#use-polybase-to-load-data-into-azure-sql-data-warehouse) for details.</span></span> <span data-ttu-id="ebc12-338">如需使用案例的逐步解說，請參閱[使用 Azure Data Factory 在 15 分鐘內將 1 TB 載入至 Azure SQL 資料倉儲](data-factory-load-sql-data-warehouse.md)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-338">For a walkthrough with a use case, see [Load 1 TB into Azure SQL Data Warehouse under 15 minutes with Azure Data Factory](data-factory-load-sql-data-warehouse.md).</span></span>

### <a name="file-based-data-stores"></a><span data-ttu-id="ebc12-339">以檔案為基礎的資料存放區</span><span class="sxs-lookup"><span data-stu-id="ebc12-339">File-based data stores</span></span>
<span data-ttu-id="ebc12-340">*(包括 Blob 儲存體、Data Lake Store、Amazon S3、內部部署檔案系統及內部部署 HDFS)*</span><span class="sxs-lookup"><span data-stu-id="ebc12-340">*(Includes Blob storage, Data Lake Store, Amazon S3, on-premises file systems, and on-premises HDFS)*</span></span>

* <span data-ttu-id="ebc12-341">**複製行為**：如果您從其他以檔案為基礎的資料存放區複製資料，複製活動會透過 **copyBehavior** 屬性提供三個選項。</span><span class="sxs-lookup"><span data-stu-id="ebc12-341">**Copy behavior**: If you copy data from a different file-based data store, Copy Activity has three options via the **copyBehavior** property.</span></span> <span data-ttu-id="ebc12-342">它會保留階層、扁平化階層或合併檔案。</span><span class="sxs-lookup"><span data-stu-id="ebc12-342">It preserves hierarchy, flattens hierarchy, or merges files.</span></span> <span data-ttu-id="ebc12-343">保留或扁平化階層幾乎不會造成效能負荷，但合併檔案則會導致效能負荷增加。</span><span class="sxs-lookup"><span data-stu-id="ebc12-343">Either preserving or flattening hierarchy has little or no performance overhead, but merging files causes performance overhead to increase.</span></span>
* <span data-ttu-id="ebc12-344">**檔案格式和壓縮**：如需可改善效能的其他方法，請參閱[序列化和還原序列化的考量](#considerations-for-serialization-and-deserialization)和[壓縮的考量](#considerations-for-compression)小節。</span><span class="sxs-lookup"><span data-stu-id="ebc12-344">**File format and compression**: See the [Considerations for serialization and deserialization](#considerations-for-serialization-and-deserialization) and [Considerations for compression](#considerations-for-compression) sections for more ways to improve performance.</span></span>
* <span data-ttu-id="ebc12-345">**Blob 儲存體**：Blob 儲存體目前只支援以區塊 Blob 來最佳化資料傳送和輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-345">**Blob storage**: Currently, Blob storage supports only block blobs for optimized data transfer and throughput.</span></span>
* <span data-ttu-id="ebc12-346">對於必須使用**資料管理閘道**的**內部部署檔案系統**案例，請參閱[資料管理閘道的考量](#considerations-for-data-management-gateway)一節。</span><span class="sxs-lookup"><span data-stu-id="ebc12-346">For **on-premises file systems** scenarios that require the use of **Data Management Gateway**, see the [Considerations for Data Management Gateway](#considerations-for-data-management-gateway) section.</span></span>

### <a name="relational-data-stores"></a><span data-ttu-id="ebc12-347">關聯式資料存放區</span><span class="sxs-lookup"><span data-stu-id="ebc12-347">Relational data stores</span></span>
<span data-ttu-id="ebc12-348">*(包括 SQL Database、SQL 資料倉儲、SQL Server 資料庫及 Oracle 資料庫)*</span><span class="sxs-lookup"><span data-stu-id="ebc12-348">*(Includes SQL Database, SQL Data Warehouse, SQL Server databases, and Oracle databases)*</span></span>

* <span data-ttu-id="ebc12-349">**複製行為**：根據為 **sqlSink** 設定的屬性，複製活動會以不同的方式將資料寫入目的地資料庫中。</span><span class="sxs-lookup"><span data-stu-id="ebc12-349">**Copy behavior**: Depending on the properties you've set for **sqlSink**, Copy Activity writes data to the destination database in different ways.</span></span>
  * <span data-ttu-id="ebc12-350">根據預設，資料移動服務會使用大量複製 API 以附加模式插入資料，而提供最佳效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-350">By default, the data movement service uses the Bulk Copy API to insert data in append mode, which provides the best performance.</span></span>
  * <span data-ttu-id="ebc12-351">如果您在接收中設定了預存程序，資料庫會一次套用一個資料列的資料 (而不是大量載入)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-351">If you configure a stored procedure in the sink, the database applies the data one row at a time instead of as a bulk load.</span></span> <span data-ttu-id="ebc12-352">因此效能會大幅降低。</span><span class="sxs-lookup"><span data-stu-id="ebc12-352">Performance drops significantly.</span></span> <span data-ttu-id="ebc12-353">如果資料集較大，在適用的情況下，請考慮改為使用 **sqlWriterCleanupScript** 屬性。</span><span class="sxs-lookup"><span data-stu-id="ebc12-353">If your data set is large, when applicable, consider switching to using the **sqlWriterCleanupScript** property.</span></span>
  * <span data-ttu-id="ebc12-354">如果您設定了每個複製活動執行的 **sqlWriterCleanupScript** 屬性，服務會觸發指令碼，然後使用大量複製 API 插入資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-354">If you configure the **sqlWriterCleanupScript** property for each Copy Activity run, the service triggers the script, and then you use the Bulk Copy API to insert the data.</span></span> <span data-ttu-id="ebc12-355">例如，若要以最新的資料覆寫整個資料表，您可以先指定指令碼以刪除所有記錄，再從來源大量載入新資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-355">For example, to overwrite the entire table with the latest data, you can specify a script to first delete all records before bulk-loading the new data from the source.</span></span>
* <span data-ttu-id="ebc12-356">**資料模式和批次大小**：</span><span class="sxs-lookup"><span data-stu-id="ebc12-356">**Data pattern and batch size**:</span></span>
  * <span data-ttu-id="ebc12-357">資料表結構描述對複製輸送量會有影響。</span><span class="sxs-lookup"><span data-stu-id="ebc12-357">Your table schema affects copy throughput.</span></span> <span data-ttu-id="ebc12-358">若要複製相同的資料量，較大的資料列大小會有優於較小資料列大小的效能，因為資料庫可以更有效率地認可較少的資料批次。</span><span class="sxs-lookup"><span data-stu-id="ebc12-358">To copy the same amount of data, a large row size gives you better performance than a small row size because the database can more efficiently commit fewer batches of data.</span></span>
  * <span data-ttu-id="ebc12-359">複製活動會以一系列的批次插入資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-359">Copy Activity inserts data in a series of batches.</span></span> <span data-ttu-id="ebc12-360">您可以使用 **writeBatchSize** 屬性來設定批次中包含的資料列數。</span><span class="sxs-lookup"><span data-stu-id="ebc12-360">You can set the number of rows in a batch by using the **writeBatchSize** property.</span></span> <span data-ttu-id="ebc12-361">如果您的資料具有較小的資料列，您可以將 **writeBatchSize** 屬性設為較高的值，以減少批次的額外負荷，並增加輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-361">If your data has small rows, you can set the **writeBatchSize** property with a higher value to benefit from lower batch overhead and higher throughput.</span></span> <span data-ttu-id="ebc12-362">如果您的資料的資料列大小較大，在增加 **writeBatchSize**時請多加留意。</span><span class="sxs-lookup"><span data-stu-id="ebc12-362">If the row size of your data is large, be careful when you increase **writeBatchSize**.</span></span> <span data-ttu-id="ebc12-363">較大的值可能會導致複製因資料庫的超載而失敗。</span><span class="sxs-lookup"><span data-stu-id="ebc12-363">A high value might lead to a copy failure caused by overloading the database.</span></span>
* <span data-ttu-id="ebc12-364">對於必須使用**資料管理閘道**的**內部部署關聯式資料庫** (如 SQL Server 和 Oracle)，請參閱[資料管理閘道的考量](#considerations-for-data-management-gateway)一節。</span><span class="sxs-lookup"><span data-stu-id="ebc12-364">For **on-premises relational databases** like SQL Server and Oracle, which require the use of **Data Management Gateway**, see the [Considerations for Data Management Gateway](#considerations-for-data-management-gateway) section.</span></span>

### <a name="nosql-stores"></a><span data-ttu-id="ebc12-365">NoSQL 存放區</span><span class="sxs-lookup"><span data-stu-id="ebc12-365">NoSQL stores</span></span>
<span data-ttu-id="ebc12-366">(包括表格儲存體和 Azure Cosmos DB)</span><span class="sxs-lookup"><span data-stu-id="ebc12-366">*(Includes Table storage and Azure Cosmos DB )*</span></span>

* <span data-ttu-id="ebc12-367">針對 **表格儲存體**：</span><span class="sxs-lookup"><span data-stu-id="ebc12-367">For **Table storage**:</span></span>
  * <span data-ttu-id="ebc12-368">**資料分割**：將資料寫入至交錯的資料分割會大幅降低效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-368">**Partition**: Writing data to interleaved partitions dramatically degrades performance.</span></span> <span data-ttu-id="ebc12-369">請依資料分割索引鍵來排序來源資料，使資料能有效率地依序插入資料分割中，或者，調整邏輯以將資料寫入單一資料分割中。</span><span class="sxs-lookup"><span data-stu-id="ebc12-369">Sort your source data by partition key so that the data is inserted efficiently into one partition after another, or adjust the logic to write the data to a single partition.</span></span>
* <span data-ttu-id="ebc12-370">針對 **Azure Cosmos DB**：</span><span class="sxs-lookup"><span data-stu-id="ebc12-370">For **Azure Cosmos DB**:</span></span>
  * <span data-ttu-id="ebc12-371">**批次大小**：**writeBatchSize** 屬性會設定對 Azure Cosmos DB 服務提出建立文件的平行要求數目。</span><span class="sxs-lookup"><span data-stu-id="ebc12-371">**Batch size**: The **writeBatchSize** property sets the number of parallel requests to the Azure Cosmos DB service to create documents.</span></span> <span data-ttu-id="ebc12-372">增加 **writeBatchSize** 時，您可預期有更好的效能，因為對 Azure Cosmos DB 傳送了更多的平行要求。</span><span class="sxs-lookup"><span data-stu-id="ebc12-372">You can expect better performance when you increase **writeBatchSize** because more parallel requests are sent to Azure Cosmos DB.</span></span> <span data-ttu-id="ebc12-373">不過，在寫入至 Azure Cosmos DB 時請注意節流問題 (錯誤訊息為「要求率偏高」)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-373">However, watch for throttling when you write to Azure Cosmos DB (the error message is "Request rate is large").</span></span> <span data-ttu-id="ebc12-374">有各種因素會導致發生節流，包括文件大小、文件中的詞彙數目，以及目標集合的索引編製原則。</span><span class="sxs-lookup"><span data-stu-id="ebc12-374">Various factors can cause throttling, including document size, the number of terms in the documents, and the target collection's indexing policy.</span></span> <span data-ttu-id="ebc12-375">若要達到更高的複製輸送量，請考慮使用更好的集合 (例如 S3)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-375">To achieve higher copy throughput, consider using a better collection, for example, S3.</span></span>

## <a name="considerations-for-serialization-and-deserialization"></a><span data-ttu-id="ebc12-376">序列化和還原序列化的考量</span><span class="sxs-lookup"><span data-stu-id="ebc12-376">Considerations for serialization and deserialization</span></span>
<span data-ttu-id="ebc12-377">如果您的輸入資料集或輸出資料集是檔案，就可能發生序列化和還原序列化。</span><span class="sxs-lookup"><span data-stu-id="ebc12-377">Serialization and deserialization can occur when your input data set or output data set is a file.</span></span> <span data-ttu-id="ebc12-378">請參閱[支援的檔案和壓縮格式](data-factory-supported-file-and-compression-formats.md)，其中具有關於複製活動支援檔案格式的詳細資訊。</span><span class="sxs-lookup"><span data-stu-id="ebc12-378">See [Supported file and compression formats](data-factory-supported-file-and-compression-formats.md) with details on supported file formats by Copy Activity.</span></span>

<span data-ttu-id="ebc12-379">**複製行為**：</span><span class="sxs-lookup"><span data-stu-id="ebc12-379">**Copy behavior**:</span></span>

* <span data-ttu-id="ebc12-380">在以檔案為基礎的資料存放區之間複製檔案：</span><span class="sxs-lookup"><span data-stu-id="ebc12-380">Copying files between file-based data stores:</span></span>
  * <span data-ttu-id="ebc12-381">如果輸入和輸出資料集同時具有相同的檔案格式設定，或者都沒有這些設定，資料移動服務會執行二進位複製，而不執行任何序列化或還原序列化。</span><span class="sxs-lookup"><span data-stu-id="ebc12-381">When input and output data sets both have the same or no file format settings, the data movement service executes a binary copy without any serialization or deserialization.</span></span> <span data-ttu-id="ebc12-382">此情況的輸送量會高於來源和接收檔案格式設定彼此不同的案例。</span><span class="sxs-lookup"><span data-stu-id="ebc12-382">You see a higher throughput compared to the scenario, in which the source and sink file format settings are different from each other.</span></span>
  * <span data-ttu-id="ebc12-383">如果輸入和輸出資料集都是文字格式，只有編碼類型不同，則資料移動服務只會執行編碼轉換，</span><span class="sxs-lookup"><span data-stu-id="ebc12-383">When input and output data sets both are in text format and only the encoding type is different, the data movement service only does encoding conversion.</span></span> <span data-ttu-id="ebc12-384">而不會執行任何序列化和還原序列化，因此和二進位複製相較之下，會產生一些效能負荷。</span><span class="sxs-lookup"><span data-stu-id="ebc12-384">It doesn't do any serialization and deserialization, which causes some performance overhead compared to a binary copy.</span></span>
  * <span data-ttu-id="ebc12-385">如果輸入和輸出資料集皆有不同的檔案格式或不同的組態 (例如分隔符號)，則資料移動服務會將來源資料還原序列化，以進行串流、轉換然後再序列化為您所指出的輸出格式。</span><span class="sxs-lookup"><span data-stu-id="ebc12-385">When input and output data sets both have different file formats or different configurations, like delimiters, the data movement service deserializes source data to stream, transform, and then serialize it into the output format you indicated.</span></span> <span data-ttu-id="ebc12-386">此作業會導致遠高於其他案例的效能負荷。</span><span class="sxs-lookup"><span data-stu-id="ebc12-386">This operation results in a much more significant performance overhead compared to other scenarios.</span></span>
* <span data-ttu-id="ebc12-387">對 (從) 不是以檔案為基礎的資料存放區複製檔案時 (例如，從以檔案為基礎的存放區複製到關聯式存放區)，必須執行序列化或還原序列化步驟。</span><span class="sxs-lookup"><span data-stu-id="ebc12-387">When you copy files to/from a data store that is not file-based (for example, from a file-based store to a relational store), the serialization or deserialization step is required.</span></span> <span data-ttu-id="ebc12-388">此步驟會導致很高的效能負荷。</span><span class="sxs-lookup"><span data-stu-id="ebc12-388">This step results in significant performance overhead.</span></span>

<span data-ttu-id="ebc12-389">**檔案格式**︰您選擇的檔案格式可能會影響複製效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-389">**File format**: The file format you choose might affect copy performance.</span></span> <span data-ttu-id="ebc12-390">例如，Avro 是一種壓縮二進位格式，可將中繼資料和資料儲存在一起。</span><span class="sxs-lookup"><span data-stu-id="ebc12-390">For example, Avro is a compact binary format that stores metadata with data.</span></span> <span data-ttu-id="ebc12-391">它廣泛支援在 Hadoop 生態系統中進行處理和查詢。</span><span class="sxs-lookup"><span data-stu-id="ebc12-391">It has broad support in the Hadoop ecosystem for processing and querying.</span></span> <span data-ttu-id="ebc12-392">不過，Avro 的序列化和還原序列化代價較高，因為它會導致低於文字格式的複製輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-392">However, Avro is more expensive for serialization and deserialization, which results in lower copy throughput compared to text format.</span></span> <span data-ttu-id="ebc12-393">在選擇整個處理流程中所使用的檔案格式時，應有整體考量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-393">Make your choice of file format throughout the processing flow holistically.</span></span> <span data-ttu-id="ebc12-394">首先要考量資料的儲存形式是來源資料存放區或是要從外部系統擷取，再考量最理想的儲存、分析處理和查詢格式，以及資料應以何種格式匯出到資料超市中，以供報告和視覺化工具使用。</span><span class="sxs-lookup"><span data-stu-id="ebc12-394">Start with what form the data is stored in, source data stores or to be extracted from external systems; the best format for storage, analytical processing, and querying; and in what format the data should be exported into data marts for reporting and visualization tools.</span></span> <span data-ttu-id="ebc12-395">有些時候，在考量整體分析程序時，讀取和寫入效能次佳的檔案格式，可能會是較好的選擇。</span><span class="sxs-lookup"><span data-stu-id="ebc12-395">Sometimes a file format that is suboptimal for read and write performance might be a good choice when you consider the overall analytical process.</span></span>

## <a name="considerations-for-compression"></a><span data-ttu-id="ebc12-396">壓縮的考量</span><span class="sxs-lookup"><span data-stu-id="ebc12-396">Considerations for compression</span></span>
<span data-ttu-id="ebc12-397">如果您的輸入或輸出資料集是檔案，您可以將複製活動設定為在資料寫入至目的地時執行壓縮或解壓縮。</span><span class="sxs-lookup"><span data-stu-id="ebc12-397">When your input or output data set is a file, you can set Copy Activity to perform compression or decompression as it writes data to the destination.</span></span> <span data-ttu-id="ebc12-398">當您選擇壓縮時，您必須在輸入/輸出 (I/O) 與 CPU 之間進行取捨。</span><span class="sxs-lookup"><span data-stu-id="ebc12-398">When you choose compression, you make a tradeoff between input/output (I/O) and CPU.</span></span> <span data-ttu-id="ebc12-399">壓縮資料須耗用額外的計算資源。</span><span class="sxs-lookup"><span data-stu-id="ebc12-399">Compressing the data costs extra in compute resources.</span></span> <span data-ttu-id="ebc12-400">但另一方面卻可降低網路 I/O 和儲存體用量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-400">But in return, it reduces network I/O and storage.</span></span> <span data-ttu-id="ebc12-401">根據您的資料，您可能會看到整體複製輸送量有所提升。</span><span class="sxs-lookup"><span data-stu-id="ebc12-401">Depending on your data, you may see a boost in overall copy throughput.</span></span>

<span data-ttu-id="ebc12-402">**轉碼器**︰複製活動支援 gzip、bzip2 和 Deflate 壓縮類型。</span><span class="sxs-lookup"><span data-stu-id="ebc12-402">**Codec**: Copy Activity supports gzip, bzip2, and Deflate compression types.</span></span> <span data-ttu-id="ebc12-403">這三種類型都可供 Azure HDInsight 進行處理。</span><span class="sxs-lookup"><span data-stu-id="ebc12-403">Azure HDInsight can consume all three types for processing.</span></span> <span data-ttu-id="ebc12-404">每種壓縮轉碼器各有優點。</span><span class="sxs-lookup"><span data-stu-id="ebc12-404">Each compression codec has advantages.</span></span> <span data-ttu-id="ebc12-405">例如，bzip2 的複製輸送量最低，但您卻可以在使用 bzip2 時獲得最佳的 Hive 查詢效能，因為可將其劃分來進行處理。</span><span class="sxs-lookup"><span data-stu-id="ebc12-405">For example, bzip2 has the lowest copy throughput, but you get the best Hive query performance with bzip2 because you can split it for processing.</span></span> <span data-ttu-id="ebc12-406">Gzip 是最均衡的選項，也最常被使用。</span><span class="sxs-lookup"><span data-stu-id="ebc12-406">Gzip is the most balanced option, and it is used the most often.</span></span> <span data-ttu-id="ebc12-407">請選擇最適合您的端對端案例使用的轉碼器。</span><span class="sxs-lookup"><span data-stu-id="ebc12-407">Choose the codec that best suits your end-to-end scenario.</span></span>

<span data-ttu-id="ebc12-408">**層級**：對於每個壓縮轉碼器，您可以從兩個選項中做選擇：最快速的壓縮和最佳化的壓縮。</span><span class="sxs-lookup"><span data-stu-id="ebc12-408">**Level**: You can choose from two options for each compression codec: fastest compressed and optimally compressed.</span></span> <span data-ttu-id="ebc12-409">最快速的壓縮選項能以最快速度壓縮資料，但產生的檔案不一定經過最理想的壓縮。</span><span class="sxs-lookup"><span data-stu-id="ebc12-409">The fastest compressed option compresses the data as quickly as possible, even if the resulting file is not optimally compressed.</span></span> <span data-ttu-id="ebc12-410">最佳化的壓縮選項會花費較長的壓縮時間，並產生最少量的資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-410">The optimally compressed option spends more time on compression and yields a minimal amount of data.</span></span> <span data-ttu-id="ebc12-411">您可以測試這兩個選項，以查看何者在您的案例中可提供更好的整體效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-411">You can test both options to see which provides better overall performance in your case.</span></span>

<span data-ttu-id="ebc12-412">**考量事項**：若要在內部部署存放區與雲端之間複製大量資料，請考慮使用過渡 Blob 儲存體來搭配壓縮。</span><span class="sxs-lookup"><span data-stu-id="ebc12-412">**A consideration**: To copy a large amount of data between an on-premises store and the cloud, consider using interim blob storage with compression.</span></span> <span data-ttu-id="ebc12-413">當公司網路與 Azure 服務的頻寬是限制因素，而且您想要讓輸入資料集和輸出資料集都是未壓縮的形式時，使用過渡儲存體會有所幫助。</span><span class="sxs-lookup"><span data-stu-id="ebc12-413">Using interim storage is helpful when the bandwidth of your corporate network and your Azure services is the limiting factor, and you want the input data set and output data set both to be in uncompressed form.</span></span> <span data-ttu-id="ebc12-414">更具體來說，您可以將單一複製活動分成兩個複製活動。</span><span class="sxs-lookup"><span data-stu-id="ebc12-414">More specifically, you can break a single copy activity into two copy activities.</span></span> <span data-ttu-id="ebc12-415">第一個複製活動以壓縮形式從來源複製到過渡或暫存 Blob。</span><span class="sxs-lookup"><span data-stu-id="ebc12-415">The first copy activity copies from the source to an interim or staging blob in compressed form.</span></span> <span data-ttu-id="ebc12-416">第二個複製活動則從暫存複製壓縮的資料，然後在寫入至接收時加以解壓縮。</span><span class="sxs-lookup"><span data-stu-id="ebc12-416">The second copy activity copies the compressed data from staging, and then decompresses while it writes to the sink.</span></span>

## <a name="considerations-for-column-mapping"></a><span data-ttu-id="ebc12-417">資料行對應的考量</span><span class="sxs-lookup"><span data-stu-id="ebc12-417">Considerations for column mapping</span></span>
<span data-ttu-id="ebc12-418">您可以在複製活動中設定 **columnMappings** 屬性以將所有或部分的輸入資料行對應至輸出資料行。</span><span class="sxs-lookup"><span data-stu-id="ebc12-418">You can set the **columnMappings** property in Copy Activity to map all or a subset of the input columns to the output columns.</span></span> <span data-ttu-id="ebc12-419">資料移動服務從來源讀取資料之後，必須先對資料執行資料行對應，再將資料寫入至接收。</span><span class="sxs-lookup"><span data-stu-id="ebc12-419">After the data movement service reads the data from the source, it needs to perform column mapping on the data before it writes the data to the sink.</span></span> <span data-ttu-id="ebc12-420">這項額外處理會降低複製輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-420">This extra processing reduces copy throughput.</span></span>

<span data-ttu-id="ebc12-421">如果您的來源資料存放區是可查詢的 (例如，如果是 SQL Database 或 SQL Server 之類的關聯式存放區，或如果是表格儲存體或 Azure Cosmos DB 之類的 NoSQL 存放區)，請考慮將資料行篩選和重新排序邏輯推送至 **查詢** 屬性，而不使用資料行對應。</span><span class="sxs-lookup"><span data-stu-id="ebc12-421">If your source data store is queryable, for example, if it's a relational store like SQL Database or SQL Server, or if it's a NoSQL store like Table storage or Azure Cosmos DB, consider pushing the column filtering and reordering logic to the **query** property instead of using column mapping.</span></span> <span data-ttu-id="ebc12-422">如此一來，便會在資料移動服務從來源資料存放區讀取資料時發生投射，而大幅提高效率。</span><span class="sxs-lookup"><span data-stu-id="ebc12-422">This way, the projection occurs while the data movement service reads data from the source data store, where it is much more efficient.</span></span>

## <a name="other-considerations"></a><span data-ttu-id="ebc12-423">其他考量</span><span class="sxs-lookup"><span data-stu-id="ebc12-423">Other considerations</span></span>
<span data-ttu-id="ebc12-424">如果要複製的資料很大，您可以調整您的商務邏輯，使用 Data Factory 的切割機制進一步分割資料。</span><span class="sxs-lookup"><span data-stu-id="ebc12-424">If the size of data you want to copy is large, you can adjust your business logic to further partition the data using the slicing mechanism in Data Factory.</span></span> <span data-ttu-id="ebc12-425">然後，將複製活動排程為更頻繁地執行，以縮減每個複製活動執行的資料大小。</span><span class="sxs-lookup"><span data-stu-id="ebc12-425">Then, schedule Copy Activity to run more frequently to reduce the data size for each Copy Activity run.</span></span>

<span data-ttu-id="ebc12-426">請密切留意資料集數目，以及要求 Data Factory 同時連線至相同資料存放區的複製活動。</span><span class="sxs-lookup"><span data-stu-id="ebc12-426">Be cautious about the number of data sets and copy activities requiring Data Factory to connector to the same data store at the same time.</span></span> <span data-ttu-id="ebc12-427">許多並行複製作業可能會導致資料存放區出現瓶頸，並導致效能降低，複製作業內部重試，在某些情況下甚至導致執行失敗。</span><span class="sxs-lookup"><span data-stu-id="ebc12-427">Many concurrent copy jobs might throttle a data store and lead to degraded performance, copy job internal retries, and in some cases, execution failures.</span></span>

## <a name="sample-scenario-copy-from-an-on-premises-sql-server-to-blob-storage"></a><span data-ttu-id="ebc12-428">範例案例：從內部部署 SQL Server 複製到 Blob 儲存體</span><span class="sxs-lookup"><span data-stu-id="ebc12-428">Sample scenario: Copy from an on-premises SQL Server to Blob storage</span></span>
<span data-ttu-id="ebc12-429">**案例：**建置從內部部署 SQL Server 將資料以 CSV 格式複製到 Blob 儲存體的管線。</span><span class="sxs-lookup"><span data-stu-id="ebc12-429">**Scenario**: A pipeline is built to copy data from an on-premises SQL Server to Blob storage in CSV format.</span></span> <span data-ttu-id="ebc12-430">為了加快複製作業速度，CSV 檔案應該壓縮為 bzip2 格式。</span><span class="sxs-lookup"><span data-stu-id="ebc12-430">To make the copy job faster, the CSV files should be compressed into bzip2 format.</span></span>

<span data-ttu-id="ebc12-431">**測試和分析**：複製活動的輸送量小於 2 MBps，遠低於效能基準。</span><span class="sxs-lookup"><span data-stu-id="ebc12-431">**Test and analysis**: The throughput of Copy Activity is less than 2 MBps, which is much slower than the performance benchmark.</span></span>

<span data-ttu-id="ebc12-432">**效能分析和微調**：為了排解效能問題，讓我們看看資料的處理及移動方式。</span><span class="sxs-lookup"><span data-stu-id="ebc12-432">**Performance analysis and tuning**: To troubleshoot the performance issue, let’s look at how the data is processed and moved.</span></span>

1. <span data-ttu-id="ebc12-433">**讀取資料**：閘道器開啟對 SQL Server 的連接，並傳送查詢。</span><span class="sxs-lookup"><span data-stu-id="ebc12-433">**Read data**: Gateway opens a connection to SQL Server and sends the query.</span></span> <span data-ttu-id="ebc12-434">SQL Server 透過內部網路將資料流傳送至閘道器，以進行回應。</span><span class="sxs-lookup"><span data-stu-id="ebc12-434">SQL Server responds by sending the data stream to Gateway via the intranet.</span></span>
2. <span data-ttu-id="ebc12-435">**序列化和壓縮資料**︰閘道器將資料流序列化為 CSV 格式，並將資料壓縮為 bzip2 資料流。</span><span class="sxs-lookup"><span data-stu-id="ebc12-435">**Serialize and compress data**: Gateway serializes the data stream to CSV format, and compresses the data to a bzip2 stream.</span></span>
3. <span data-ttu-id="ebc12-436">**寫入資料**：閘道器透過網際網路將 bzip2 資料流上傳至 Blob 儲存體。</span><span class="sxs-lookup"><span data-stu-id="ebc12-436">**Write data**: Gateway uploads the bzip2 stream to Blob storage via the Internet.</span></span>

<span data-ttu-id="ebc12-437">如您所見，資料將會以串流序列的方式處理和移動：SQL Server -> LAN -> 閘道器 -> WAN -> Blob 儲存體。</span><span class="sxs-lookup"><span data-stu-id="ebc12-437">As you can see, the data is being processed and moved in a streaming sequential manner: SQL Server > LAN > Gateway > WAN > Blob storage.</span></span> <span data-ttu-id="ebc12-438">**整體效能受限於管線的最小輸送量**。</span><span class="sxs-lookup"><span data-stu-id="ebc12-438">**The overall performance is gated by the minimum throughput across the pipeline**.</span></span>

![資料流](./media/data-factory-copy-activity-performance/case-study-pic-1.png)

<span data-ttu-id="ebc12-440">下列一或多個因素可能會造成效能瓶頸：</span><span class="sxs-lookup"><span data-stu-id="ebc12-440">One or more of the following factors might cause the performance bottleneck:</span></span>

* <span data-ttu-id="ebc12-441">**來源**：SQL Server 本身的輸送量偏低，因為負載過重。</span><span class="sxs-lookup"><span data-stu-id="ebc12-441">**Source**: SQL Server itself has low throughput because of heavy loads.</span></span>
* <span data-ttu-id="ebc12-442">**資料管理閘道**：</span><span class="sxs-lookup"><span data-stu-id="ebc12-442">**Data Management Gateway**:</span></span>
  * <span data-ttu-id="ebc12-443">**LAN**：閘道器的位置遠離 SQL Server 電腦，且頻寬連線較低。</span><span class="sxs-lookup"><span data-stu-id="ebc12-443">**LAN**: Gateway is located far from the SQL Server machine and has a low-bandwidth connection.</span></span>
  * <span data-ttu-id="ebc12-444">**閘道器**：閘道器已達到其負載限制，而無法執行下列作業：</span><span class="sxs-lookup"><span data-stu-id="ebc12-444">**Gateway**: Gateway has reached its load limitations to perform the following operations:</span></span>
    * <span data-ttu-id="ebc12-445">**序列化**：將資料流序列化為 CSV 格式時，輸送量偏低。</span><span class="sxs-lookup"><span data-stu-id="ebc12-445">**Serialization**: Serializing the data stream to CSV format has slow throughput.</span></span>
    * <span data-ttu-id="ebc12-446">**壓縮**：您選擇了緩慢的壓縮轉碼器 (例如 bzip2，其採用 Core i7，速度為 2.8 MBps)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-446">**Compression**: You chose a slow compression codec (for example, bzip2, which is 2.8 MBps with Core i7).</span></span>
  * <span data-ttu-id="ebc12-447">**WAN**：公司網路與 Azure 服務之間的頻寬偏低 (例如，T1 = 1,544 kbps、T2 = 6,312 kbps)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-447">**WAN**: The bandwidth between the corporate network and your Azure services is low (for example, T1 = 1,544 kbps; T2 = 6,312 kbps).</span></span>
* <span data-ttu-id="ebc12-448">**接收**：Blob 儲存體的輸送量低 </span><span class="sxs-lookup"><span data-stu-id="ebc12-448">**Sink**: Blob storage has low throughput.</span></span> <span data-ttu-id="ebc12-449">(但不太可能發生，因為其 SLA 保證至少有 60 MBps)。</span><span class="sxs-lookup"><span data-stu-id="ebc12-449">(This scenario is unlikely because its SLA guarantees a minimum of 60 MBps.)</span></span>

<span data-ttu-id="ebc12-450">在此情況下，bzip2 資料壓縮可能會拖慢整個管線。</span><span class="sxs-lookup"><span data-stu-id="ebc12-450">In this case, bzip2 data compression might be slowing down the entire pipeline.</span></span> <span data-ttu-id="ebc12-451">改用 gzip 壓縮轉碼器可能會緩解此瓶頸。</span><span class="sxs-lookup"><span data-stu-id="ebc12-451">Switching to a gzip compression codec might ease this bottleneck.</span></span>

## <a name="sample-scenarios-use-parallel-copy"></a><span data-ttu-id="ebc12-452">範例案例︰使用平行複本</span><span class="sxs-lookup"><span data-stu-id="ebc12-452">Sample scenarios: Use parallel copy</span></span>
<span data-ttu-id="ebc12-453">**案例 I：** 從內部部署檔案系統複製 1,000 個 1 MB 的檔案至 Blob 儲存體。</span><span class="sxs-lookup"><span data-stu-id="ebc12-453">**Scenario I:** Copy 1,000 1-MB files from the on-premises file system to Blob storage.</span></span>

<span data-ttu-id="ebc12-454">**分析和效能微調**︰例如，如果您已在四核心電腦上安裝閘道器，Data Factory 會使用 16 個平行複製，以並行方式從檔案系統中將檔案移至 Blob 儲存體。</span><span class="sxs-lookup"><span data-stu-id="ebc12-454">**Analysis and performance tuning**: For an example, if you have installed gateway on a quad core machine, Data Factory uses 16 parallel copies to move files from the file system to Blob storage concurrently.</span></span> <span data-ttu-id="ebc12-455">此平行執行應該會導致高輸送量。</span><span class="sxs-lookup"><span data-stu-id="ebc12-455">This parallel execution should result in high throughput.</span></span> <span data-ttu-id="ebc12-456">您也可以明確指定平行複製計數。</span><span class="sxs-lookup"><span data-stu-id="ebc12-456">You also can explicitly specify the parallel copies count.</span></span> <span data-ttu-id="ebc12-457">在複製許多小型檔案時，平行複製可藉由更有效率地使用資源，而對輸送量大有幫助。</span><span class="sxs-lookup"><span data-stu-id="ebc12-457">When you copy many small files, parallel copies dramatically help throughput by using resources more effectively.</span></span>

![案例 1](./media/data-factory-copy-activity-performance/scenario-1.png)

<span data-ttu-id="ebc12-459">**案例 II**：從 Blob 儲存體複製 20 個 Blob (每個 Blob 有 500 MB) 到 Data Lake Store 分析，然後微調效能。</span><span class="sxs-lookup"><span data-stu-id="ebc12-459">**Scenario II**: Copy 20 blobs of 500 MB each from Blob storage to Data Lake Store Analytics, and then tune performance.</span></span>

<span data-ttu-id="ebc12-460">**分析和效能微調**︰在此案例中，Data Factory 會使用一個複製 (**parallelCopies** 設為 1) 以及一個雲端資料移動單位，將資料從 Blob 儲存體複製到 Data Lake Store。</span><span class="sxs-lookup"><span data-stu-id="ebc12-460">**Analysis and performance tuning**: In this scenario, Data Factory copies the data from Blob storage to Data Lake Store by using single-copy (**parallelCopies** set to 1) and single-cloud data movement units.</span></span> <span data-ttu-id="ebc12-461">您所觀察到的輸送量會接近 [效能參考](#performance-reference)一節所述。</span><span class="sxs-lookup"><span data-stu-id="ebc12-461">The throughput you observe will be close to that described in the [performance reference section](#performance-reference).</span></span>   

![案例 2](./media/data-factory-copy-activity-performance/scenario-2.png)

<span data-ttu-id="ebc12-463">**案例 III**︰個別檔案大小大於數十 MB 且總數量很大。</span><span class="sxs-lookup"><span data-stu-id="ebc12-463">**Scenario III**: Individual file size is greater than dozens of MBs and total volume is large.</span></span>

<span data-ttu-id="ebc12-464">**分析和效能微調**︰增加 **parallelCopies** 並不會提升複製效能，因為單一雲端 DMU 的資源有所限制。</span><span class="sxs-lookup"><span data-stu-id="ebc12-464">**Analysis and performance turning**: Increasing **parallelCopies** doesn't result in better copy performance because of the resource limitations of a single-cloud DMU.</span></span> <span data-ttu-id="ebc12-465">相反地，您應該指定更多個雲端 DMU，以取得更多用來執行資料移動的資源。</span><span class="sxs-lookup"><span data-stu-id="ebc12-465">Instead, you should specify more cloud DMUs to get more resources to perform the data movement.</span></span> <span data-ttu-id="ebc12-466">請不要指定 **parallelCopies** 屬性的值。</span><span class="sxs-lookup"><span data-stu-id="ebc12-466">Do not specify a value for the **parallelCopies** property.</span></span> <span data-ttu-id="ebc12-467">Data Factory 會為您處理平行處理原則。</span><span class="sxs-lookup"><span data-stu-id="ebc12-467">Data Factory handles the parallelism for you.</span></span> <span data-ttu-id="ebc12-468">在此案例中，如果您將 **cloudDataMovementUnits** 設定為 4，會讓輸送量變成大約 4 倍。</span><span class="sxs-lookup"><span data-stu-id="ebc12-468">In this case, if you set **cloudDataMovementUnits** to 4, a throughput of about four times occurs.</span></span>

![案例 3](./media/data-factory-copy-activity-performance/scenario-3.png)

## <a name="reference"></a><span data-ttu-id="ebc12-470">參考</span><span class="sxs-lookup"><span data-stu-id="ebc12-470">Reference</span></span>
<span data-ttu-id="ebc12-471">以下是幾個支援的資料存放區所適用的效能監視及調整參考：</span><span class="sxs-lookup"><span data-stu-id="ebc12-471">Here are performance monitoring and tuning references for some of the supported data stores:</span></span>

* <span data-ttu-id="ebc12-472">Azure 儲存體 (包括 Blob 儲存體和表格儲存體)：[Azure 儲存體的擴充性目標](../storage/common/storage-scalability-targets.md)和 [Azure 儲存體效能和擴充性檢查清單](../storage/common/storage-performance-checklist.md)</span><span class="sxs-lookup"><span data-stu-id="ebc12-472">Azure Storage (including Blob storage and Table storage): [Azure Storage scalability targets](../storage/common/storage-scalability-targets.md) and [Azure Storage performance and scalability checklist](../storage/common/storage-performance-checklist.md)</span></span>
* <span data-ttu-id="ebc12-473">Azure SQL Database：您可以 [監視效能](../sql-database/sql-database-single-database-monitor.md) ，並檢查資料庫交易單位 (DTU) 百分比</span><span class="sxs-lookup"><span data-stu-id="ebc12-473">Azure SQL Database: You can [monitor the performance](../sql-database/sql-database-single-database-monitor.md) and check the database transaction unit (DTU) percentage</span></span>
* <span data-ttu-id="ebc12-474">Azure SQL 資料倉儲：其能力會以資料倉儲單位 (DWU) 來測量；請參閱 [管理 Azure SQL 資料倉儲中的計算能力 (概觀)](../sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md)</span><span class="sxs-lookup"><span data-stu-id="ebc12-474">Azure SQL Data Warehouse: Its capability is measured in data warehouse units (DWUs); see [Manage compute power in Azure SQL Data Warehouse (Overview)](../sql-data-warehouse/sql-data-warehouse-manage-compute-overview.md)</span></span>
* <span data-ttu-id="ebc12-475">Azure Cosmos DB：[Azure Cosmos DB 中的效能等級](../documentdb/documentdb-performance-levels.md)</span><span class="sxs-lookup"><span data-stu-id="ebc12-475">Azure Cosmos DB: [Performance levels in Azure Cosmos DB](../documentdb/documentdb-performance-levels.md)</span></span>
* <span data-ttu-id="ebc12-476">內部部署 SQL Server： [效能的監視與微調](https://msdn.microsoft.com/library/ms189081.aspx)</span><span class="sxs-lookup"><span data-stu-id="ebc12-476">On-premises SQL Server: [Monitor and tune for performance](https://msdn.microsoft.com/library/ms189081.aspx)</span></span>
* <span data-ttu-id="ebc12-477">內部部署檔案伺服器： [檔案伺服器的效能微調](https://msdn.microsoft.com/library/dn567661.aspx)</span><span class="sxs-lookup"><span data-stu-id="ebc12-477">On-premises file server: [Performance tuning for file servers](https://msdn.microsoft.com/library/dn567661.aspx)</span></span>
