---
title: "有關 Data Lake 存放區的資料案例 | Microsoft Docs"
description: "了解用來內嵌、處理、下載及視覺化 Data Lake 存放區之資料的各種案例和工具"
services: data-lake-store
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
ms.assetid: 37409a71-a563-4bb7-bc46-2cbd426a2ece
ms.service: data-lake-store
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 05/10/2017
ms.author: nitinme
ms.openlocfilehash: 2a2801e5c506dcc8aa9ca2ecd275b52c72d5fbbf
ms.sourcegitcommit: 18ad9bc049589c8e44ed277f8f43dcaa483f3339
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 08/29/2017
---
# <a name="using-azure-data-lake-store-for-big-data-requirements"></a><span data-ttu-id="29b6d-103">使用 Azure Data Lake Store 處理巨量資料需求</span><span class="sxs-lookup"><span data-stu-id="29b6d-103">Using Azure Data Lake Store for big data requirements</span></span>
<span data-ttu-id="29b6d-104">巨量資料處理有四個主要階段︰</span><span class="sxs-lookup"><span data-stu-id="29b6d-104">There are four key stages in big data processing:</span></span>

* <span data-ttu-id="29b6d-105">即時或以批次形式將大量資料內嵌到存放區</span><span class="sxs-lookup"><span data-stu-id="29b6d-105">Ingesting large amounts of data into a data store, at real-time or in batches</span></span>
* <span data-ttu-id="29b6d-106">處理資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-106">Processing the data</span></span>
* <span data-ttu-id="29b6d-107">下載資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-107">Downloading the data</span></span>
* <span data-ttu-id="29b6d-108">將資料視覺化</span><span class="sxs-lookup"><span data-stu-id="29b6d-108">Visualizing the data</span></span>

<span data-ttu-id="29b6d-109">在本文中，我們探討這些與 Azure Data Lake 存放區有關的階段，以了解可用來滿足您巨量資料需求的選項與工具。</span><span class="sxs-lookup"><span data-stu-id="29b6d-109">In this article, we look at these stages with respect to Azure Data Lake Store to understand the options and tools available to meet your big data needs.</span></span>

## <a name="ingest-data-into-data-lake-store"></a><span data-ttu-id="29b6d-110">將資料內嵌到 Data Lake 存放區</span><span class="sxs-lookup"><span data-stu-id="29b6d-110">Ingest data into Data Lake Store</span></span>
<span data-ttu-id="29b6d-111">本章節強調不同的資料來源，以及將資料內嵌到 Data Lake 存放區帳戶的各種方式。</span><span class="sxs-lookup"><span data-stu-id="29b6d-111">This section highlights the different sources of data and the different ways in which that data can be ingested into a Data Lake Store account.</span></span>

<span data-ttu-id="29b6d-112">![將資料內嵌到 Data Lake Store](./media/data-lake-store-data-scenarios/ingest-data.png "將資料內嵌到 Data Lake Store")</span><span class="sxs-lookup"><span data-stu-id="29b6d-112">![Ingest data into Data Lake Store](./media/data-lake-store-data-scenarios/ingest-data.png "Ingest data into Data Lake Store")</span></span>

### <a name="ad-hoc-data"></a><span data-ttu-id="29b6d-113">臨機操作資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-113">Ad hoc data</span></span>
<span data-ttu-id="29b6d-114">代表用來建立巨量資料應用程式原型的較小型資料集。</span><span class="sxs-lookup"><span data-stu-id="29b6d-114">This represents smaller data sets that are used for prototyping a big data application.</span></span> <span data-ttu-id="29b6d-115">內嵌臨機操作資料的方式會因資料來源不同而有所差異。</span><span class="sxs-lookup"><span data-stu-id="29b6d-115">There are different ways of ingesting ad hoc data depending on the source of the data.</span></span>

| <span data-ttu-id="29b6d-116">資料來源</span><span class="sxs-lookup"><span data-stu-id="29b6d-116">Data Source</span></span> | <span data-ttu-id="29b6d-117">內嵌方式</span><span class="sxs-lookup"><span data-stu-id="29b6d-117">Ingest it using</span></span> |
| --- | --- |
| <span data-ttu-id="29b6d-118">本機電腦</span><span class="sxs-lookup"><span data-stu-id="29b6d-118">Local computer</span></span> |<ul> <li>[<span data-ttu-id="29b6d-119">Azure 入口網站</span><span class="sxs-lookup"><span data-stu-id="29b6d-119">Azure Portal</span></span>](/data-lake-store-get-started-portal.md)</li> <li>[<span data-ttu-id="29b6d-120">Azure PowerShell</span><span class="sxs-lookup"><span data-stu-id="29b6d-120">Azure PowerShell</span></span>](data-lake-store-get-started-powershell.md)</li> <li>[<span data-ttu-id="29b6d-121">Azure 跨平台 CLI 2.0</span><span class="sxs-lookup"><span data-stu-id="29b6d-121">Azure Cross-platform CLI 2.0</span></span>](data-lake-store-get-started-cli-2.0.md)</li> <li>[<span data-ttu-id="29b6d-122">使用適用於 Visual Studio 的 Data Lake Tools</span><span class="sxs-lookup"><span data-stu-id="29b6d-122">Using Data Lake Tools for Visual Studio</span></span>](../data-lake-analytics/data-lake-analytics-data-lake-tools-get-started.md) </li></ul> |
| <span data-ttu-id="29b6d-123">Azure 儲存體 Blob</span><span class="sxs-lookup"><span data-stu-id="29b6d-123">Azure Storage Blob</span></span> |<ul> <li>[<span data-ttu-id="29b6d-124">Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="29b6d-124">Azure Data Factory</span></span>](../data-factory/data-factory-azure-datalake-connector.md)</li> <li>[<span data-ttu-id="29b6d-125">AdlCopy 工具</span><span class="sxs-lookup"><span data-stu-id="29b6d-125">AdlCopy tool</span></span>](data-lake-store-copy-data-azure-storage-blob.md)</li><li>[<span data-ttu-id="29b6d-126">HDInsight 叢集上執行的 DistCp</span><span class="sxs-lookup"><span data-stu-id="29b6d-126">DistCp running on HDInsight cluster</span></span>](data-lake-store-copy-data-wasb-distcp.md)</li> </ul> |

### <a name="streamed-data"></a><span data-ttu-id="29b6d-127">串流資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-127">Streamed data</span></span>
<span data-ttu-id="29b6d-128">代表可能由應用程式、裝置、感應器等各種來源產生的資料。這些資料可透過多種工具內嵌到 Data Lake 存放區。</span><span class="sxs-lookup"><span data-stu-id="29b6d-128">This represents data that can be generated by various sources such as applications, devices, sensors, etc. This data can be ingested into a Data Lake Store by variety tools.</span></span> <span data-ttu-id="29b6d-129">這些工具通常能以事件為基礎即時擷取及處理資料，然後再以批次將事件寫入 Data Lake 存放區，以供進一步處理。</span><span class="sxs-lookup"><span data-stu-id="29b6d-129">These tools will usually capture and process the data on an event-by-event basis in real-time, and then write the events in batches into Data Lake Store so that they can be further processed.</span></span>

<span data-ttu-id="29b6d-130">以下是您可以使用的工具︰</span><span class="sxs-lookup"><span data-stu-id="29b6d-130">Following are tools that you can use:</span></span>

* <span data-ttu-id="29b6d-131">[Azure 串流分析](../stream-analytics/stream-analytics-data-lake-output.md) - 內嵌到「事件中樞」的事件可以透過 Azure Data Lake Store 輸出被寫入 Azure Data Lake 中。</span><span class="sxs-lookup"><span data-stu-id="29b6d-131">[Azure Stream Analytics](../stream-analytics/stream-analytics-data-lake-output.md) - Events ingested into Event Hubs can be written to Azure Data Lake using an Azure Data Lake Store output.</span></span>
* <span data-ttu-id="29b6d-132">[Azure HDInsight Storm](../hdinsight/hdinsight-storm-write-data-lake-store.md) - 您可以從 Storm 叢集將資料直接寫入 Data Lake Store 中。</span><span class="sxs-lookup"><span data-stu-id="29b6d-132">[Azure HDInsight Storm](../hdinsight/hdinsight-storm-write-data-lake-store.md) - You can write data directly to Data Lake Store from the Storm cluster.</span></span>
* <span data-ttu-id="29b6d-133">[EventProcessorHost](../event-hubs/event-hubs-dotnet-standard-getstarted-receive-eph.md) - 您可以從「事件中樞」接收事件，然後使用 [Data Lake Store .NET SDK](data-lake-store-get-started-net-sdk.md) 將事件寫入 Data Lake Store 中。</span><span class="sxs-lookup"><span data-stu-id="29b6d-133">[EventProcessorHost](../event-hubs/event-hubs-dotnet-standard-getstarted-receive-eph.md) – You can receive events from Event Hubs and then write it to Data Lake Store using the [Data Lake Store .NET SDK](data-lake-store-get-started-net-sdk.md).</span></span>

### <a name="relational-data"></a><span data-ttu-id="29b6d-134">關聯式資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-134">Relational data</span></span>
<span data-ttu-id="29b6d-135">您也可以從關聯式資料庫取得資料。</span><span class="sxs-lookup"><span data-stu-id="29b6d-135">You can also source data from relational databases.</span></span> <span data-ttu-id="29b6d-136">每經過一段時間，關聯式資料庫就會收集大量資料，在經過巨量資料管線處理後，這些資料將可提供重要情資。</span><span class="sxs-lookup"><span data-stu-id="29b6d-136">Over a period of time, relational databases collect huge amounts of data which can provide key insights if processed through a big data pipeline.</span></span> <span data-ttu-id="29b6d-137">您可以使用下列工具，將這類資料移動到 Data Lake 存放區。</span><span class="sxs-lookup"><span data-stu-id="29b6d-137">You can use the following tools to move such data into Data Lake Store.</span></span>

* [<span data-ttu-id="29b6d-138">Apache Sqoop</span><span class="sxs-lookup"><span data-stu-id="29b6d-138">Apache Sqoop</span></span>](data-lake-store-data-transfer-sql-sqoop.md)
* [<span data-ttu-id="29b6d-139">Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="29b6d-139">Azure Data Factory</span></span>](../data-factory/data-factory-data-movement-activities.md)

### <a name="web-server-log-data-upload-using-custom-applications"></a><span data-ttu-id="29b6d-140">Web 伺服器記錄資料 (使用自訂應用程式上傳)</span><span class="sxs-lookup"><span data-stu-id="29b6d-140">Web server log data (upload using custom applications)</span></span>
<span data-ttu-id="29b6d-141">我們會特別強調這類資料集的原因在於，因為 Web 伺服器記錄資料是巨量資料應用程式的常見使用案例，且需要將大量記錄檔上傳到 Data Lake 存放區。</span><span class="sxs-lookup"><span data-stu-id="29b6d-141">This type of dataset is specifically called out because analysis of web server log data is a common use case for big data applications and requires large volumes of log files to be uploaded to the Data Lake Store.</span></span> <span data-ttu-id="29b6d-142">您可以使用以下任何工具來撰寫自己的指令碼或應用程式，以便上傳這類資料。</span><span class="sxs-lookup"><span data-stu-id="29b6d-142">You can use any of the following tools to write your own scripts or applications to upload such data.</span></span>

* [<span data-ttu-id="29b6d-143">Azure 跨平台 CLI 2.0</span><span class="sxs-lookup"><span data-stu-id="29b6d-143">Azure Cross-platform CLI 2.0</span></span>](data-lake-store-get-started-cli-2.0.md)
* [<span data-ttu-id="29b6d-144">Azure PowerShell</span><span class="sxs-lookup"><span data-stu-id="29b6d-144">Azure PowerShell</span></span>](data-lake-store-get-started-powershell.md)
* [<span data-ttu-id="29b6d-145">Azure Data Lake 存放區 .NET SDK</span><span class="sxs-lookup"><span data-stu-id="29b6d-145">Azure Data Lake Store .NET SDK</span></span>](data-lake-store-get-started-net-sdk.md)
* [<span data-ttu-id="29b6d-146">Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="29b6d-146">Azure Data Factory</span></span>](../data-factory/data-factory-data-movement-activities.md)

<span data-ttu-id="29b6d-147">若要上傳 Web 伺服器記錄資料及上傳其他類型的資料 (如社交情緒資料)，撰寫自己的自訂指令碼/應用程式是個不錯方法，因為您可以彈性地將自己的資料上傳元件納入較大型的巨量資料應用程式中。</span><span class="sxs-lookup"><span data-stu-id="29b6d-147">For uploading web server log data, and also for uploading other kinds of data (e.g. social sentiments data), it is a good approach to write your own custom scripts/applications because it gives you the flexibility to include your data uploading component as part of your larger big data application.</span></span> <span data-ttu-id="29b6d-148">在某些情況下，這段程式碼可能會採用指令碼或簡易命令列公用程式的形式。</span><span class="sxs-lookup"><span data-stu-id="29b6d-148">In some cases this code may take the form of a script or simple command line utility.</span></span> <span data-ttu-id="29b6d-149">在其他情況下，程式碼可用來將巨量資料處理整合到商務應用程式或解決方案中。</span><span class="sxs-lookup"><span data-stu-id="29b6d-149">In other cases, the code may be used to integrate big data processing into a business application or solution.</span></span>

### <a name="data-associated-with-azure-hdinsight-clusters"></a><span data-ttu-id="29b6d-150">與 Azure HDInsight 叢集相關聯的資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-150">Data associated with Azure HDInsight clusters</span></span>
<span data-ttu-id="29b6d-151">大部分的 HDInsight 叢集類型 (Hadoop、HBase、Storm) 能以資料儲存存放機制的形式支援 Data Lake 存放區。</span><span class="sxs-lookup"><span data-stu-id="29b6d-151">Most HDInsight cluster types (Hadoop, HBase, Storm) support Data Lake Store as a data storage repository.</span></span> <span data-ttu-id="29b6d-152">HDInsight 叢集能從 Azure 儲存體 Blob (WASB) 存取資料。</span><span class="sxs-lookup"><span data-stu-id="29b6d-152">HDInsight clusters access data from Azure Storage Blobs (WASB).</span></span> <span data-ttu-id="29b6d-153">為了提高效能，您可以將資料從 WASB 複製到與叢集相關聯的 Data Lake 存放區帳戶。</span><span class="sxs-lookup"><span data-stu-id="29b6d-153">For better performance, you can copy the data from WASB into a Data Lake Store account associated with the cluster.</span></span> <span data-ttu-id="29b6d-154">您可以使用下列工具來複製資料。</span><span class="sxs-lookup"><span data-stu-id="29b6d-154">You can use the following tools to copy the data.</span></span>

* [<span data-ttu-id="29b6d-155">Apache DistCp</span><span class="sxs-lookup"><span data-stu-id="29b6d-155">Apache DistCp</span></span>](data-lake-store-copy-data-wasb-distcp.md)
* [<span data-ttu-id="29b6d-156">AdlCopy 服務</span><span class="sxs-lookup"><span data-stu-id="29b6d-156">AdlCopy Service</span></span>](data-lake-store-copy-data-azure-storage-blob.md)
* [<span data-ttu-id="29b6d-157">Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="29b6d-157">Azure Data Factory</span></span>](../data-factory/data-factory-azure-datalake-connector.md)

### <a name="data-stored-in-on-premises-or-iaas-hadoop-clusters"></a><span data-ttu-id="29b6d-158">儲存於內部部署環境或 IaaS Hadoop 叢集中的資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-158">Data stored in on-premises or IaaS Hadoop clusters</span></span>
<span data-ttu-id="29b6d-159">您可能會使用 HDFS，在本機電腦上將大量資料儲存於現有的 Hadoop 叢集中。</span><span class="sxs-lookup"><span data-stu-id="29b6d-159">Large amounts of data may be stored in existing Hadoop clusters, locally on machines using HDFS.</span></span> <span data-ttu-id="29b6d-160">Hadoop 叢集可能位於內部部署環境中，也可能位於 Azure 上的 IaaS 叢集內。</span><span class="sxs-lookup"><span data-stu-id="29b6d-160">The Hadoop clusters may be in an on-premises deployment or may be within an IaaS cluster on Azure.</span></span> <span data-ttu-id="29b6d-161">可能有一些需求，要以一次性方法或週期性方式來將這類資料複製到 Azure Data Lake Store。</span><span class="sxs-lookup"><span data-stu-id="29b6d-161">There could be requirements to copy such data to Azure Data Lake Store for a one-off approach or in a recurring fashion.</span></span> <span data-ttu-id="29b6d-162">有各種不同的選項可用來達到此目的。</span><span class="sxs-lookup"><span data-stu-id="29b6d-162">There are various options that you can use to achieve this.</span></span> <span data-ttu-id="29b6d-163">以下是替代項目和相關考量的清單。</span><span class="sxs-lookup"><span data-stu-id="29b6d-163">Below is a list of alternatives and the associated trade-offs.</span></span>

| <span data-ttu-id="29b6d-164">方法</span><span class="sxs-lookup"><span data-stu-id="29b6d-164">Approach</span></span> | <span data-ttu-id="29b6d-165">詳細資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-165">Details</span></span> | <span data-ttu-id="29b6d-166">優點</span><span class="sxs-lookup"><span data-stu-id="29b6d-166">Advantages</span></span> | <span data-ttu-id="29b6d-167">考量</span><span class="sxs-lookup"><span data-stu-id="29b6d-167">Considerations</span></span> |
| --- | --- | --- | --- |
| <span data-ttu-id="29b6d-168">使用 Azure Data Factory (ADF)，將資料從 Hadoop 叢集直接複製到 Azure Data Lake Store</span><span class="sxs-lookup"><span data-stu-id="29b6d-168">Use Azure Data Factory (ADF) to copy data directly from Hadoop clusters to Azure Data Lake Store</span></span> |[<span data-ttu-id="29b6d-169">ADF 支援 HDFS 做為資料來源</span><span class="sxs-lookup"><span data-stu-id="29b6d-169">ADF supports HDFS as a data source</span></span>](../data-factory/data-factory-hdfs-connector.md) |<span data-ttu-id="29b6d-170">ADF 針對 HDFS 提供全新支援，以及一流的端對端管理與監視</span><span class="sxs-lookup"><span data-stu-id="29b6d-170">ADF provides out-of-the-box support for HDFS and first class end-to-end management and monitoring</span></span> |<span data-ttu-id="29b6d-171">需要將「資料管理閘道」部署在內部部署環境或 IaaS 叢集中</span><span class="sxs-lookup"><span data-stu-id="29b6d-171">Requires Data Management Gateway to be deployed on-premises or in the IaaS cluster</span></span> |
| <span data-ttu-id="29b6d-172">從 Hadoop 將資料匯出為檔案。</span><span class="sxs-lookup"><span data-stu-id="29b6d-172">Export data from Hadoop as files.</span></span> <span data-ttu-id="29b6d-173">然後使用適當的機制，將檔案複製到 Azure Data Lake Store。</span><span class="sxs-lookup"><span data-stu-id="29b6d-173">Then copy the files to Azure Data Lake Store using appropriate mechanism.</span></span> |<span data-ttu-id="29b6d-174">您可以使用下列方法，將檔案複製到 Azure Data Lake Store︰</span><span class="sxs-lookup"><span data-stu-id="29b6d-174">You can copy files to Azure Data Lake Store using:</span></span> <ul><li>[<span data-ttu-id="29b6d-175">適用於 Windows OS 的 Azure PowerShell</span><span class="sxs-lookup"><span data-stu-id="29b6d-175">Azure PowerShell for Windows OS</span></span>](data-lake-store-get-started-powershell.md)</li><li>[<span data-ttu-id="29b6d-176">適用於非 Windows OS 的 Azure 跨平台 CLI 2.0</span><span class="sxs-lookup"><span data-stu-id="29b6d-176">Azure Cross-platform CLI 2.0 for non-Windows OS</span></span>](data-lake-store-get-started-cli-2.0.md)</li><li><span data-ttu-id="29b6d-177">使用任何 Data Lake Store SDK 的自訂應用程式</span><span class="sxs-lookup"><span data-stu-id="29b6d-177">Custom app using any Data Lake Store SDK</span></span></li></ul> |<span data-ttu-id="29b6d-178">快速開始使用。</span><span class="sxs-lookup"><span data-stu-id="29b6d-178">Quick to get started.</span></span> <span data-ttu-id="29b6d-179">可以執行自訂的上傳</span><span class="sxs-lookup"><span data-stu-id="29b6d-179">Can do customized uploads</span></span> |<span data-ttu-id="29b6d-180">牽涉到多種技術的多步驟程序。</span><span class="sxs-lookup"><span data-stu-id="29b6d-180">Multi-step process that involves multiple technologies.</span></span> <span data-ttu-id="29b6d-181">考慮到自訂的工具性質，管理和監視會在經過一段時間之後逐漸變成是一項挑戰</span><span class="sxs-lookup"><span data-stu-id="29b6d-181">Management and monitoring will grow to be a challenge over time given the customized nature of the tools</span></span> |
| <span data-ttu-id="29b6d-182">使用 Distcp，將資料從 Hadoop 複製到 Azure 儲存體。</span><span class="sxs-lookup"><span data-stu-id="29b6d-182">Use Distcp to copy data from Hadoop to Azure Storage.</span></span> <span data-ttu-id="29b6d-183">然後使用適當的機制，將資料從 Azure 儲存體複製到 Data Lake Store。</span><span class="sxs-lookup"><span data-stu-id="29b6d-183">Then copy data from Azure Storage to Data Lake Store using appropriate mechanism.</span></span> |<span data-ttu-id="29b6d-184">您可以使用下列方法，將資料從「Azure 儲存體」複製到 Data Lake Store︰</span><span class="sxs-lookup"><span data-stu-id="29b6d-184">You can copy data from Azure Storage to Data Lake Store using:</span></span> <ul><li>[<span data-ttu-id="29b6d-185">Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="29b6d-185">Azure Data Factory</span></span>](../data-factory/data-factory-data-movement-activities.md)</li><li>[<span data-ttu-id="29b6d-186">AdlCopy 工具</span><span class="sxs-lookup"><span data-stu-id="29b6d-186">AdlCopy tool</span></span>](data-lake-store-copy-data-azure-storage-blob.md)</li><li>[<span data-ttu-id="29b6d-187">HDInsight 叢集上執行的 Apache DistCp</span><span class="sxs-lookup"><span data-stu-id="29b6d-187">Apache DistCp running on HDInsight clusters</span></span>](data-lake-store-copy-data-wasb-distcp.md)</li></ul> |<span data-ttu-id="29b6d-188">您可以使用開放原始碼工具。</span><span class="sxs-lookup"><span data-stu-id="29b6d-188">You can use open-source tools.</span></span> |<span data-ttu-id="29b6d-189">牽涉到多種技術的多步驟程序</span><span class="sxs-lookup"><span data-stu-id="29b6d-189">Multi-step process that involves multiple technologies</span></span> |

### <a name="really-large-datasets"></a><span data-ttu-id="29b6d-190">大型資料集</span><span class="sxs-lookup"><span data-stu-id="29b6d-190">Really large datasets</span></span>
<span data-ttu-id="29b6d-191">若要上傳動輒數 TB 的資料集，使用上述方法有時候可能會過於緩慢且昂貴。</span><span class="sxs-lookup"><span data-stu-id="29b6d-191">For uploading datasets that range in several terabytes, using the methods described above can sometimes be slow and costly.</span></span> <span data-ttu-id="29b6d-192">此時，您可以使用下列選項。</span><span class="sxs-lookup"><span data-stu-id="29b6d-192">In such cases, you can use the options below.</span></span>

* <span data-ttu-id="29b6d-193">**使用 Azure ExpressRoute**。</span><span class="sxs-lookup"><span data-stu-id="29b6d-193">**Using Azure ExpressRoute**.</span></span> <span data-ttu-id="29b6d-194">Azure ExpressRoute 可讓您在 Azure 資料中心與內部部署的基礎結構之間建立私人連線。</span><span class="sxs-lookup"><span data-stu-id="29b6d-194">Azure ExpressRoute lets you create private connections between Azure datacenters and infrastructure on your premises.</span></span> <span data-ttu-id="29b6d-195">這是傳輸大量資料的可靠選項。</span><span class="sxs-lookup"><span data-stu-id="29b6d-195">This provides a reliable option for transferring large amounts of data.</span></span> <span data-ttu-id="29b6d-196">如需詳細資訊，請參閱 [Azure ExpressRoute 文件](../expressroute/expressroute-introduction.md)。</span><span class="sxs-lookup"><span data-stu-id="29b6d-196">For more information, see [Azure ExpressRoute documentation](../expressroute/expressroute-introduction.md).</span></span>
* <span data-ttu-id="29b6d-197">**「離線」上傳資料**。</span><span class="sxs-lookup"><span data-stu-id="29b6d-197">**"Offline" upload of data**.</span></span> <span data-ttu-id="29b6d-198">如果因為任何原因而無法使用 Azure ExpressRoute，您可以使用 [Azure 匯入/匯出服務](../storage/common/storage-import-export-service.md) ，將含有您資料的硬碟送到 Azure 資料中心。</span><span class="sxs-lookup"><span data-stu-id="29b6d-198">If using Azure ExpressRoute is not feasible for any reason, you can use [Azure Import/Export service](../storage/common/storage-import-export-service.md) to ship hard disk drives with your data to an Azure data center.</span></span> <span data-ttu-id="29b6d-199">您的資料會先上傳到 Azure 儲存體 Blob。</span><span class="sxs-lookup"><span data-stu-id="29b6d-199">Your data is first uploaded to Azure Storage Blobs.</span></span> <span data-ttu-id="29b6d-200">接下來，您可以使用 [Azure Data Factory](../data-factory/data-factory-azure-datalake-connector.md) 或 [AdlCopy 工具](data-lake-store-copy-data-azure-storage-blob.md)，將資料從 Azure 儲存體 Blob 複製到 Data Lake Store。</span><span class="sxs-lookup"><span data-stu-id="29b6d-200">You can then use [Azure Data Factory](../data-factory/data-factory-azure-datalake-connector.md) or [AdlCopy tool](data-lake-store-copy-data-azure-storage-blob.md) to copy data from Azure Storage Blobs to Data Lake Store.</span></span>

  > [!NOTE]
  > <span data-ttu-id="29b6d-201">使用「匯入/匯出」服務時，運送到 Azure 資料中心之磁碟上的檔案大小應不大於 195 GB。</span><span class="sxs-lookup"><span data-stu-id="29b6d-201">While using the Import/Export service, the file sizes on the disks that you ship to Azure data center should not be greater than 195 GB.</span></span>
  >
  >

## <a name="process-data-stored-in-data-lake-store"></a><span data-ttu-id="29b6d-202">處理儲存在 Data Lake 存放區中的資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-202">Process data stored in Data Lake Store</span></span>
<span data-ttu-id="29b6d-203">一旦可以取用 Data Lake 存放區中的資料後，您就可以使用支援的巨量資料應用程式來針對這些資料執行分析。</span><span class="sxs-lookup"><span data-stu-id="29b6d-203">Once the data is available in Data Lake Store you can run analysis on that data using the supported big data applications.</span></span> <span data-ttu-id="29b6d-204">目前，您可以使用 Azure HDInsight 和 Azure Data Lake 分析來針對儲存在 Data Lake 存放區中的資料執行資料分析工作。</span><span class="sxs-lookup"><span data-stu-id="29b6d-204">Currently, you can use Azure HDInsight and Azure Data Lake Analytics to run data analysis jobs on the data stored in Data Lake Store.</span></span>

<span data-ttu-id="29b6d-205">![分析 Data Lake Store 中的資料](./media/data-lake-store-data-scenarios/analyze-data.png "分析 Data Lake Store 中的資料")</span><span class="sxs-lookup"><span data-stu-id="29b6d-205">![Analyze data in Data Lake Store](./media/data-lake-store-data-scenarios/analyze-data.png "Analyze data in Data Lake Store")</span></span>

<span data-ttu-id="29b6d-206">您可以查看下列範例。</span><span class="sxs-lookup"><span data-stu-id="29b6d-206">You can look at the following examples.</span></span>

* [<span data-ttu-id="29b6d-207">建立以 Data Lake 存放區當做儲存體的 HDInsight 叢集</span><span class="sxs-lookup"><span data-stu-id="29b6d-207">Create an HDInsight cluster with Data Lake Store as storage</span></span>](data-lake-store-hdinsight-hadoop-use-portal.md)
* [<span data-ttu-id="29b6d-208">搭配 Data Lake 存放區使用 Azure Data Lake 分析</span><span class="sxs-lookup"><span data-stu-id="29b6d-208">Use Azure Data Lake Analytics with Data Lake Store</span></span>](../data-lake-analytics/data-lake-analytics-get-started-portal.md)

## <a name="download-data-from-data-lake-store"></a><span data-ttu-id="29b6d-209">從 Data Lake 存放區下載資料</span><span class="sxs-lookup"><span data-stu-id="29b6d-209">Download data from Data Lake Store</span></span>
<span data-ttu-id="29b6d-210">在以下案例中，您可能也會想要從 Azure Data Lake 存放區下載資料或移動資料：</span><span class="sxs-lookup"><span data-stu-id="29b6d-210">You might also want to download or move data from Azure Data Lake Store for scenarios such as:</span></span>

* <span data-ttu-id="29b6d-211">將資料移動到其他儲存機制，以便與現有的資料處理管線連結。</span><span class="sxs-lookup"><span data-stu-id="29b6d-211">Move data to other repositories to interface with your existing data processing pipelines.</span></span> <span data-ttu-id="29b6d-212">例如，您可能會想要將資料從 Data Lake 存放區移動到 Azure SQL Database 或內部部署 SQL Server。</span><span class="sxs-lookup"><span data-stu-id="29b6d-212">For example, you might want to move data from Data Lake Store to Azure SQL Database or on-premises SQL Server.</span></span>
* <span data-ttu-id="29b6d-213">在建置應用程式原型時，將資料下載到本機電腦，以便在 IDE 環境中處理。</span><span class="sxs-lookup"><span data-stu-id="29b6d-213">Download data to your local computer for processing in IDE environments while building application prototypes.</span></span>

<span data-ttu-id="29b6d-214">![從 Data Lake Store 輸出資料](./media/data-lake-store-data-scenarios/egress-data.png "從 Data Lake Store 輸出資料")</span><span class="sxs-lookup"><span data-stu-id="29b6d-214">![Egress data from Data Lake Store](./media/data-lake-store-data-scenarios/egress-data.png "Egress data from Data Lake Store")</span></span>

<span data-ttu-id="29b6d-215">在這些案例中，您可以使用下列任何選項。</span><span class="sxs-lookup"><span data-stu-id="29b6d-215">In such cases, you can use any of the following options:</span></span>

* [<span data-ttu-id="29b6d-216">Apache Sqoop</span><span class="sxs-lookup"><span data-stu-id="29b6d-216">Apache Sqoop</span></span>](data-lake-store-data-transfer-sql-sqoop.md)
* [<span data-ttu-id="29b6d-217">Azure Data Factory</span><span class="sxs-lookup"><span data-stu-id="29b6d-217">Azure Data Factory</span></span>](../data-factory/data-factory-data-movement-activities.md)
* [<span data-ttu-id="29b6d-218">Apache DistCp</span><span class="sxs-lookup"><span data-stu-id="29b6d-218">Apache DistCp</span></span>](data-lake-store-copy-data-wasb-distcp.md)

<span data-ttu-id="29b6d-219">您也可以使用下列方法來撰寫自己的指令碼/應用程式，以便從 Data Lake 存放區下載資料。</span><span class="sxs-lookup"><span data-stu-id="29b6d-219">You can also use the following methods to write your own script/application to download data from Data Lake Store.</span></span>

* [<span data-ttu-id="29b6d-220">Azure 跨平台 CLI 2.0</span><span class="sxs-lookup"><span data-stu-id="29b6d-220">Azure Cross-platform CLI 2.0</span></span>](data-lake-store-get-started-cli-2.0.md)
* [<span data-ttu-id="29b6d-221">Azure PowerShell</span><span class="sxs-lookup"><span data-stu-id="29b6d-221">Azure PowerShell</span></span>](data-lake-store-get-started-powershell.md)
* [<span data-ttu-id="29b6d-222">Azure Data Lake 存放區 .NET SDK</span><span class="sxs-lookup"><span data-stu-id="29b6d-222">Azure Data Lake Store .NET SDK</span></span>](data-lake-store-get-started-net-sdk.md)

## <a name="visualize-data-in-data-lake-store"></a><span data-ttu-id="29b6d-223">將 Data Lake 存放區中的資料視覺化</span><span class="sxs-lookup"><span data-stu-id="29b6d-223">Visualize data in Data Lake Store</span></span>
<span data-ttu-id="29b6d-224">您可以混合使用多種服務，利用視覺化的方式呈現儲存在 Data Lake 存放區中的資料。</span><span class="sxs-lookup"><span data-stu-id="29b6d-224">You can use a mix of services to create visual representations of data stored in Data Lake Store.</span></span>

<span data-ttu-id="29b6d-225">![將 Data Lake Store 中的資料視覺化](./media/data-lake-store-data-scenarios/visualize-data.png "將 Data Lake Store 中的資料視覺化")</span><span class="sxs-lookup"><span data-stu-id="29b6d-225">![Visualize data in Data Lake Store](./media/data-lake-store-data-scenarios/visualize-data.png "Visualize data in Data Lake Store")</span></span>

* <span data-ttu-id="29b6d-226">您可以從使用 [Azure Data Factory 將資料從 Data Lake Store 移到 Azure SQL 資料倉儲](../data-factory/data-factory-data-movement-activities.md#supported-data-stores-and-formats)</span><span class="sxs-lookup"><span data-stu-id="29b6d-226">You can start by using [Azure Data Factory to move data from Data Lake Store to Azure SQL Data Warehouse](../data-factory/data-factory-data-movement-activities.md#supported-data-stores-and-formats)</span></span>
* <span data-ttu-id="29b6d-227">之後，您可以 [將 Power BI 與 Azure SQL 資料倉儲整合](../sql-data-warehouse/sql-data-warehouse-integrate-power-bi.md) ，以視覺化方式呈現資料。</span><span class="sxs-lookup"><span data-stu-id="29b6d-227">After that, you can [integrate Power BI with Azure SQL Data Warehouse](../sql-data-warehouse/sql-data-warehouse-integrate-power-bi.md) to create visual representation of the data.</span></span>
