---
title: "使用 Spark 分析 Application Insights 記錄 - Azure HDInsight | Microsoft Docs"
description: "了解如何將 Application Insights 記錄檔匯出至 blob 儲存體，並接著使用 HDInsight 上的 Spark 分析記錄檔。"
services: hdinsight
documentationcenter: 
author: Blackmist
manager: jhubbard
editor: cgronlun
ms.assetid: 883beae6-9839-45b5-94f7-7eb0f4534ad5
ms.service: hdinsight
ms.custom: hdinsightactive
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 08/15/2017
ms.author: larryfr
ms.openlocfilehash: d98e403683618ef6115372f99e4949af87af4490
ms.sourcegitcommit: 50e23e8d3b1148ae2d36dad3167936b4e52c8a23
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 08/18/2017
---
# <a name="analyze-application-insights-telemetry-logs-with-spark-on-hdinsight"></a><span data-ttu-id="0914c-103">使用 HDInsight 上的 Spark 分析 Application Insights 遙測記錄檔</span><span class="sxs-lookup"><span data-stu-id="0914c-103">Analyze Application Insights telemetry logs with Spark on HDInsight</span></span>

<span data-ttu-id="0914c-104">了解如何在 HDInsight 上使用 Spark 來分析 Application Insights 遙測資料。</span><span class="sxs-lookup"><span data-stu-id="0914c-104">Learn how to use Spark on HDInsight to analyze Application Insight telemetry data.</span></span>

<span data-ttu-id="0914c-105">[Visual Studio Application Insights](../application-insights/app-insights-overview.md) 是一項分析服務，可監視您的 Web 應用程式。</span><span class="sxs-lookup"><span data-stu-id="0914c-105">[Visual Studio Application Insights](../application-insights/app-insights-overview.md) is an analytics service that monitors your web applications.</span></span> <span data-ttu-id="0914c-106">可以將 Application Insights 產生的遙測資料匯出至 Azure 儲存體。</span><span class="sxs-lookup"><span data-stu-id="0914c-106">Telemetry data generated by Application Insights can be exported to Azure Storage.</span></span> <span data-ttu-id="0914c-107">資料一旦位於 Azure 儲存體中，HDInsight 便可用於進行分析。</span><span class="sxs-lookup"><span data-stu-id="0914c-107">Once the data is in Azure Storage, HDInsight can be used to analyze it.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="0914c-108">必要條件</span><span class="sxs-lookup"><span data-stu-id="0914c-108">Prerequisites</span></span>

* <span data-ttu-id="0914c-109">設定要使用 Application Insights 的應用程式。</span><span class="sxs-lookup"><span data-stu-id="0914c-109">An application that is configured to use Application Insights.</span></span>

* <span data-ttu-id="0914c-110">熟悉以 Linux 為基礎的 HDInsight 叢集的建立程序。</span><span class="sxs-lookup"><span data-stu-id="0914c-110">Familiarity with creating a Linux-based HDInsight cluster.</span></span> <span data-ttu-id="0914c-111">如需詳細資訊，請參閱[在 HDInsight 中建立 Spark](hdinsight-apache-spark-jupyter-spark-sql.md)。</span><span class="sxs-lookup"><span data-stu-id="0914c-111">For more information, see [Create Spark on HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

  > [!IMPORTANT]
  > <span data-ttu-id="0914c-112">此文件中的步驟需要使用 Linux 的 HDInsight 叢集。</span><span class="sxs-lookup"><span data-stu-id="0914c-112">The steps in this document require an HDInsight cluster that uses Linux.</span></span> <span data-ttu-id="0914c-113">Linux 是唯一使用於 HDInsight 3.4 版或更新版本的作業系統。</span><span class="sxs-lookup"><span data-stu-id="0914c-113">Linux is the only operating system used on HDInsight version 3.4 or greater.</span></span> <span data-ttu-id="0914c-114">如需詳細資訊，請參閱 [Windows 上的 HDInsight 淘汰](hdinsight-component-versioning.md#hdinsight-windows-retirement)。</span><span class="sxs-lookup"><span data-stu-id="0914c-114">For more information, see [HDInsight retirement on Windows](hdinsight-component-versioning.md#hdinsight-windows-retirement).</span></span>

* <span data-ttu-id="0914c-115">網頁瀏覽器。</span><span class="sxs-lookup"><span data-stu-id="0914c-115">A web browser.</span></span>

<span data-ttu-id="0914c-116">開發和測試本文件時使用了以下資源：</span><span class="sxs-lookup"><span data-stu-id="0914c-116">The following resources were used in developing and testing this document:</span></span>

* <span data-ttu-id="0914c-117">Application Insights 遙測資料，由 [設定為使用 Application Insights 的 Node.js Web 應用程式](../application-insights/app-insights-nodejs.md)產生。</span><span class="sxs-lookup"><span data-stu-id="0914c-117">Application Insights telemetry data was generated using a [Node.js web app configured to use Application Insights](../application-insights/app-insights-nodejs.md).</span></span>

* <span data-ttu-id="0914c-118">在 Linux 上使用 HDInsight 叢集版本 3.5 上的 Spark 分析資料。</span><span class="sxs-lookup"><span data-stu-id="0914c-118">A Linux-based Spark on HDInsight cluster version 3.5 was used to analyze the data.</span></span>

## <a name="architecture-and-planning"></a><span data-ttu-id="0914c-119">架構與規劃</span><span class="sxs-lookup"><span data-stu-id="0914c-119">Architecture and planning</span></span>

<span data-ttu-id="0914c-120">下圖說明此範例的服務架構：</span><span class="sxs-lookup"><span data-stu-id="0914c-120">The following diagram illustrates the service architecture of this example:</span></span>

![顯示資料從 Application Insights 傳輸至 blob 儲存體，然後由 HDInsight 上的 Spark 處理的圖](./media/hdinsight-spark-analyze-application-insight-logs/appinsightshdinsight.png)

### <a name="azure-storage"></a><span data-ttu-id="0914c-122">Azure 儲存體</span><span class="sxs-lookup"><span data-stu-id="0914c-122">Azure storage</span></span>

<span data-ttu-id="0914c-123">Application Insights 可以設定為持續將遙測資訊匯出到 blob。</span><span class="sxs-lookup"><span data-stu-id="0914c-123">Application Insights can be configured to continuously export telemetry information to blobs.</span></span> <span data-ttu-id="0914c-124">HDInsight 接著便可讀取儲存在 blob 中的資料。</span><span class="sxs-lookup"><span data-stu-id="0914c-124">HDInsight can then read data stored in the blobs.</span></span> <span data-ttu-id="0914c-125">不過，有一些您必須遵守的需求︰</span><span class="sxs-lookup"><span data-stu-id="0914c-125">However, there are some requirements that you must follow:</span></span>

* <span data-ttu-id="0914c-126">**位置**︰如果儲存體帳戶和 HDInsight 位於不同位置，可能就會增加延遲。</span><span class="sxs-lookup"><span data-stu-id="0914c-126">**Location**: If the Storage Account and HDInsight are in different locations, it may increase latency.</span></span> <span data-ttu-id="0914c-127">此外，將輸出費用套用到在區域間移動的資料時也會增加成本。</span><span class="sxs-lookup"><span data-stu-id="0914c-127">It also increases cost, as egress charges are applied to data moving between regions.</span></span>

    > [!WARNING]
    > <span data-ttu-id="0914c-128">不支援在與 HDInsight 不同的位置使用儲存體帳戶。</span><span class="sxs-lookup"><span data-stu-id="0914c-128">Using a Storage Account in a different location than HDInsight is not supported.</span></span>

* <span data-ttu-id="0914c-129">**blob 類型**：HDInsight 僅支援區塊 blob。</span><span class="sxs-lookup"><span data-stu-id="0914c-129">**Blob type**: HDInsight only supports block blobs.</span></span> <span data-ttu-id="0914c-130">Application Insights 預設為使用區塊 blob，因此應該使用預設項目來搭配 HDInsight。</span><span class="sxs-lookup"><span data-stu-id="0914c-130">Application Insights defaults to using block blobs, so should work by default with HDInsight.</span></span>

<span data-ttu-id="0914c-131">如需將其他儲存體新增至現有 HDInsight 叢集的資訊，請參閱[新增其他儲存體帳戶](hdinsight-hadoop-add-storage.md)文件。</span><span class="sxs-lookup"><span data-stu-id="0914c-131">For information on adding additional storage to an existing HDInsight cluster, see the [Add additional storage accounts](hdinsight-hadoop-add-storage.md) document.</span></span>

### <a name="data-schema"></a><span data-ttu-id="0914c-132">資料結構描述</span><span class="sxs-lookup"><span data-stu-id="0914c-132">Data schema</span></span>

<span data-ttu-id="0914c-133">Application Insights 提供 [匯出資料模型](../application-insights/app-insights-export-data-model.md) 資訊，做為匯出至 blob 之遙測資料的格式依據。</span><span class="sxs-lookup"><span data-stu-id="0914c-133">Application Insights provides [export data model](../application-insights/app-insights-export-data-model.md) information for the telemetry data format exported to blobs.</span></span> <span data-ttu-id="0914c-134">這份文件中的步驟使用 Spark SQL 來處理資料。</span><span class="sxs-lookup"><span data-stu-id="0914c-134">The steps in this document use Spark SQL to work with the data.</span></span> <span data-ttu-id="0914c-135">Spark SQL 可以為 Application Insights 所記錄的 JSON 資料結構自動產生結構描述。</span><span class="sxs-lookup"><span data-stu-id="0914c-135">Spark SQL can automatically generate a schema for the JSON data structure logged by Application Insights.</span></span>

## <a name="export-telemetry-data"></a><span data-ttu-id="0914c-136">匯出遙測資料</span><span class="sxs-lookup"><span data-stu-id="0914c-136">Export telemetry data</span></span>

<span data-ttu-id="0914c-137">依照 [設定連續匯出](../application-insights/app-insights-export-telemetry.md) 中的步驟設定您的 Application Insights 將遙測資訊匯出到 Azure 儲存體 blob。</span><span class="sxs-lookup"><span data-stu-id="0914c-137">Follow the steps in [Configure Continuous Export](../application-insights/app-insights-export-telemetry.md) to configure your Application Insights to export telemetry information to an Azure storage blob.</span></span>

## <a name="configure-hdinsight-to-access-the-data"></a><span data-ttu-id="0914c-138">設定 HDInsight 來存取資料</span><span class="sxs-lookup"><span data-stu-id="0914c-138">Configure HDInsight to access the data</span></span>

<span data-ttu-id="0914c-139">如果您要建立 HDInsight 叢集，請在叢集建立期間新增儲存體帳戶。</span><span class="sxs-lookup"><span data-stu-id="0914c-139">If you are creating an HDInsight cluster, add the storage account during cluster creation.</span></span>

<span data-ttu-id="0914c-140">若要將 Azure 儲存體帳戶新增至現有的叢集，請使用[新增其他儲存體帳戶](hdinsight-hadoop-add-storage.md)文件中的資訊。</span><span class="sxs-lookup"><span data-stu-id="0914c-140">To add the Azure Storage Account to an existing cluster, use the information in the [Add additional Storage Accounts](hdinsight-hadoop-add-storage.md) document.</span></span>

## <a name="analyze-the-data-pyspark"></a><span data-ttu-id="0914c-141">分析資料︰PySpark</span><span class="sxs-lookup"><span data-stu-id="0914c-141">Analyze the data: PySpark</span></span>

1. <span data-ttu-id="0914c-142">在 [Azure 入口網站](https://portal.azure.com)中選取您 HDInsight 叢集上的 Spark。</span><span class="sxs-lookup"><span data-stu-id="0914c-142">From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster.</span></span> <span data-ttu-id="0914c-143">在 [快速連結] 區段中，選取 [叢集儀表板]，然後選取 [叢集儀表板] 刀鋒視窗中的 [Jupyter Notebook]。</span><span class="sxs-lookup"><span data-stu-id="0914c-143">From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.</span></span>

    ![叢集儀表板](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)

2. <span data-ttu-id="0914c-145">在 Jupyter 頁面右上角依序選取 [新增]、[PySpark]。</span><span class="sxs-lookup"><span data-stu-id="0914c-145">In the upper right corner of the Jupyter page, select **New**, and then **PySpark**.</span></span> <span data-ttu-id="0914c-146">隨即開啟新的瀏覽器索引標籤，其中包含以 Python 為基礎的 Jupyter Notebook。</span><span class="sxs-lookup"><span data-stu-id="0914c-146">A new browser tab containing a Python-based Jupyter Notebook opens.</span></span>

3. <span data-ttu-id="0914c-147">在頁面的第一個欄位 (稱為**儲存格**) 中，輸入下列文字：</span><span class="sxs-lookup"><span data-stu-id="0914c-147">In the first field (called a **cell**) on the page, enter the following text:</span></span>

   ```python
   sc._jsc.hadoopConfiguration().set('mapreduce.input.fileinputformat.input.dir.recursive', 'true')
   ```

    <span data-ttu-id="0914c-148">這個程式碼會設定 Spark 以遞迴方式存取輸入資料的目錄結構。</span><span class="sxs-lookup"><span data-stu-id="0914c-148">This code configures Spark to recursively access the directory structure for the input data.</span></span> <span data-ttu-id="0914c-149">Application Insights 遙測會記錄到類似 `/{telemetry type}/YYYY-MM-DD/{##}/` 的目錄結構中。</span><span class="sxs-lookup"><span data-stu-id="0914c-149">Application Insights telemetry is logged to a directory structure similar to the `/{telemetry type}/YYYY-MM-DD/{##}/`.</span></span>

4. <span data-ttu-id="0914c-150">使用 **SHIFT + ENTER** 執行程式碼。</span><span class="sxs-lookup"><span data-stu-id="0914c-150">Use **SHIFT+ENTER** to run the code.</span></span> <span data-ttu-id="0914c-151">在儲存格左邊，方括號之間出現 '\*' 即表示正在執行此儲存格中的程式碼。</span><span class="sxs-lookup"><span data-stu-id="0914c-151">On the left side of the cell, an '\*' appears between the brackets to indicate that the code in this cell is being executed.</span></span> <span data-ttu-id="0914c-152">當執行完成之後，'\*' 就會變成數字，而儲存格下方會顯示類似以下文字的輸出：</span><span class="sxs-lookup"><span data-stu-id="0914c-152">Once it completes, the '\*' changes to a number, and output similar to the following text is displayed below the cell:</span></span>

        Creating SparkContext as 'sc'

        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    pyspark    idle    Link    Link    ✔

        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. <span data-ttu-id="0914c-153">新的儲存格會建立在第一個儲存格之下。</span><span class="sxs-lookup"><span data-stu-id="0914c-153">A new cell is created below the first one.</span></span> <span data-ttu-id="0914c-154">在新的儲存格中輸入下列文字。</span><span class="sxs-lookup"><span data-stu-id="0914c-154">Enter the following text in the new cell.</span></span> <span data-ttu-id="0914c-155">將 `CONTAINER` 和 `STORAGEACCOUNT` 取代為 Azure 儲存體帳戶名稱和包含 Application Insights 資料的 Blob 容器名稱。</span><span class="sxs-lookup"><span data-stu-id="0914c-155">Replace `CONTAINER` and `STORAGEACCOUNT` with the Azure storage account name and blob container name that contains Application Insights data.</span></span>

   ```python
   %%bash
   hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   ```

    <span data-ttu-id="0914c-156">使用 **SHIFT + ENTER** 執行程此儲存格。</span><span class="sxs-lookup"><span data-stu-id="0914c-156">Use **SHIFT+ENTER** to execute this cell.</span></span> <span data-ttu-id="0914c-157">您會看到類似以下文字的結果：</span><span class="sxs-lookup"><span data-stu-id="0914c-157">You see a result similar to the following text:</span></span>

        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831

    <span data-ttu-id="0914c-158">傳回的 wasb 路徑是 Application Insights 遙測資料的位置。</span><span class="sxs-lookup"><span data-stu-id="0914c-158">The wasb path returned is the location of the Application Insights telemetry data.</span></span> <span data-ttu-id="0914c-159">將儲存格中的 `hdfs dfs -ls` 這一行變更為使用傳回的 wasb 路徑，然後使用 **SHIFT + ENTER** 再執行一次儲存格。</span><span class="sxs-lookup"><span data-stu-id="0914c-159">Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again.</span></span> <span data-ttu-id="0914c-160">此時，結果應該會顯示包含遙測資料的目錄。</span><span class="sxs-lookup"><span data-stu-id="0914c-160">This time, the results should display the directories that contain telemetry data.</span></span>

   > [!NOTE]
   > <span data-ttu-id="0914c-161">本節中步驟的其餘部分使用 `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` 目錄。</span><span class="sxs-lookup"><span data-stu-id="0914c-161">For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used.</span></span> <span data-ttu-id="0914c-162">您的目錄結構可能不同。</span><span class="sxs-lookup"><span data-stu-id="0914c-162">Your directory structure may be different.</span></span>

6. <span data-ttu-id="0914c-163">在下一個儲存格中，輸入下列程式碼︰將 `WASB_PATH` 取代為前一個步驟中的路徑。</span><span class="sxs-lookup"><span data-stu-id="0914c-163">In the next cell, enter the following code: Replace `WASB_PATH` with the path from the previous step.</span></span>

   ```python
   jsonFiles = sc.textFile('WASB_PATH')
   jsonData = sqlContext.read.json(jsonFiles)
   ```

    <span data-ttu-id="0914c-164">這個程式碼會從連續匯出程序匯出的 JSON 檔案建立資料框架。</span><span class="sxs-lookup"><span data-stu-id="0914c-164">This code creates a dataframe from the JSON files exported by the continuous export process.</span></span> <span data-ttu-id="0914c-165">使用 **SHIFT + ENTER** 執行此儲存格。</span><span class="sxs-lookup"><span data-stu-id="0914c-165">Use **SHIFT+ENTER** to run this cell.</span></span>
7. <span data-ttu-id="0914c-166">在下一個儲存格中輸入並執行下列命令，以檢視 Spark 為 JSON 檔案建立的結構描述︰</span><span class="sxs-lookup"><span data-stu-id="0914c-166">In the next cell, enter and run the following to view the schema that Spark created for the JSON files:</span></span>

   ```python
   jsonData.printSchema()
   ```

    <span data-ttu-id="0914c-167">每種類型的遙測結構描述皆不同。</span><span class="sxs-lookup"><span data-stu-id="0914c-167">The schema for each type of telemetry is different.</span></span> <span data-ttu-id="0914c-168">以下範例是針對 Web 要求產生的結構描述 (資料儲存在 `Requests` 子目錄中)：</span><span class="sxs-lookup"><span data-stu-id="0914c-168">The following example is the schema that is generated for web requests (data stored in the `Requests` subdirectory):</span></span>

        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)
8. <span data-ttu-id="0914c-169">使用下列命令將資料框架註冊為暫存資料表，並針對資料執行查詢︰</span><span class="sxs-lookup"><span data-stu-id="0914c-169">Use the following to register the dataframe as a temporary table and run a query against the data:</span></span>

   ```python
   jsonData.registerTempTable("requests")
   df = sqlContext.sql("select context.location.city from requests where context.location.city is not null")
   df.show()
   ```

    <span data-ttu-id="0914c-170">此查詢會傳回 context.location.city 不是 null 的前 20 筆記錄的 city 資訊。</span><span class="sxs-lookup"><span data-stu-id="0914c-170">This query returns the city information for the top 20 records where context.location.city is not null.</span></span>

   > [!NOTE]
   > <span data-ttu-id="0914c-171">context 結構會出現在 Application Insights 記錄的所有遙測中。</span><span class="sxs-lookup"><span data-stu-id="0914c-171">The context structure is present in all telemetry logged by Application Insights.</span></span> <span data-ttu-id="0914c-172">您的記錄中可能不會填入 city 元素。</span><span class="sxs-lookup"><span data-stu-id="0914c-172">The city element may not be populated in your logs.</span></span> <span data-ttu-id="0914c-173">使用結構描述找出您可以查詢可能包含您的記錄檔資料的其他元素。</span><span class="sxs-lookup"><span data-stu-id="0914c-173">Use the schema to identify other elements that you can query that may contain data for your logs.</span></span>

    <span data-ttu-id="0914c-174">此查詢會傳回類似以下文字的資訊：</span><span class="sxs-lookup"><span data-stu-id="0914c-174">This query returns information similar to the following text:</span></span>

        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## <a name="analyze-the-data-scala"></a><span data-ttu-id="0914c-175">分析資料︰Scala</span><span class="sxs-lookup"><span data-stu-id="0914c-175">Analyze the data: Scala</span></span>

1. <span data-ttu-id="0914c-176">在 [Azure 入口網站](https://portal.azure.com)中選取您 HDInsight 叢集上的 Spark。</span><span class="sxs-lookup"><span data-stu-id="0914c-176">From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster.</span></span> <span data-ttu-id="0914c-177">在 [快速連結] 區段中，選取 [叢集儀表板]，然後選取 [叢集儀表板] 刀鋒視窗中的 [Jupyter Notebook]。</span><span class="sxs-lookup"><span data-stu-id="0914c-177">From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.</span></span>

    ![叢集儀表板](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)
2. <span data-ttu-id="0914c-179">在 Jupyter 頁面右上角依序選取 [新增]、[Scala]。</span><span class="sxs-lookup"><span data-stu-id="0914c-179">In the upper right corner of the Jupyter page, select **New**, and then **Scala**.</span></span> <span data-ttu-id="0914c-180">新的瀏覽器索引標籤隨即出現，其中包含以 Scala 為基礎的 Jupyter Notebook。</span><span class="sxs-lookup"><span data-stu-id="0914c-180">A new browser tab containing a Scala-based Jupyter Notebook appears.</span></span>
3. <span data-ttu-id="0914c-181">在頁面的第一個欄位 (稱為**儲存格**) 中，輸入下列文字：</span><span class="sxs-lookup"><span data-stu-id="0914c-181">In the first field (called a **cell**) on the page, enter the following text:</span></span>

   ```scala
   sc.hadoopConfiguration.set("mapreduce.input.fileinputformat.input.dir.recursive", "true")
   ```

    <span data-ttu-id="0914c-182">這個程式碼會設定 Spark 以遞迴方式存取輸入資料的目錄結構。</span><span class="sxs-lookup"><span data-stu-id="0914c-182">This code configures Spark to recursively access the directory structure for the input data.</span></span> <span data-ttu-id="0914c-183">Application Insights 遙測會記錄到類似 `/{telemetry type}/YYYY-MM-DD/{##}/` 的目錄結構中。</span><span class="sxs-lookup"><span data-stu-id="0914c-183">Application Insights telemetry is logged to a directory structure similar to `/{telemetry type}/YYYY-MM-DD/{##}/`.</span></span>

4. <span data-ttu-id="0914c-184">使用 **SHIFT + ENTER** 執行程式碼。</span><span class="sxs-lookup"><span data-stu-id="0914c-184">Use **SHIFT+ENTER** to run the code.</span></span> <span data-ttu-id="0914c-185">在儲存格左邊，方括號之間出現 '\*' 即表示正在執行此儲存格中的程式碼。</span><span class="sxs-lookup"><span data-stu-id="0914c-185">On the left side of the cell, an '\*' appears between the brackets to indicate that the code in this cell is being executed.</span></span> <span data-ttu-id="0914c-186">當執行完成之後，'\*' 就會變成數字，而儲存格下方會顯示類似以下文字的輸出：</span><span class="sxs-lookup"><span data-stu-id="0914c-186">Once it completes, the '\*' changes to a number, and output similar to the following text is displayed below the cell:</span></span>

        Creating SparkContext as 'sc'

        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    spark    idle    Link    Link    ✔

        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. <span data-ttu-id="0914c-187">新的儲存格會建立在第一個儲存格之下。</span><span class="sxs-lookup"><span data-stu-id="0914c-187">A new cell is created below the first one.</span></span> <span data-ttu-id="0914c-188">在新的儲存格中輸入下列文字。</span><span class="sxs-lookup"><span data-stu-id="0914c-188">Enter the following text in the new cell.</span></span> <span data-ttu-id="0914c-189">將 `CONTAINER` 和 `STORAGEACCOUNT` 取代為 Azure 儲存體帳戶名稱和包含 Application Insights 記錄的 Blob 容器名稱。</span><span class="sxs-lookup"><span data-stu-id="0914c-189">Replace `CONTAINER` and `STORAGEACCOUNT` with the Azure storage account name and blob container name that contains Application Insights logs.</span></span>

   ```scala
   %%bash
   hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   ```

    <span data-ttu-id="0914c-190">使用 **SHIFT + ENTER** 執行程此儲存格。</span><span class="sxs-lookup"><span data-stu-id="0914c-190">Use **SHIFT+ENTER** to execute this cell.</span></span> <span data-ttu-id="0914c-191">您會看到類似以下文字的結果：</span><span class="sxs-lookup"><span data-stu-id="0914c-191">You see a result similar to the following text:</span></span>

        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831

    <span data-ttu-id="0914c-192">傳回的 wasb 路徑是 Application Insights 遙測資料的位置。</span><span class="sxs-lookup"><span data-stu-id="0914c-192">The wasb path returned is the location of the Application Insights telemetry data.</span></span> <span data-ttu-id="0914c-193">將儲存格中的 `hdfs dfs -ls` 這一行變更為使用傳回的 wasb 路徑，然後使用 **SHIFT + ENTER** 再執行一次儲存格。</span><span class="sxs-lookup"><span data-stu-id="0914c-193">Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again.</span></span> <span data-ttu-id="0914c-194">此時，結果應該會顯示包含遙測資料的目錄。</span><span class="sxs-lookup"><span data-stu-id="0914c-194">This time, the results should display the directories that contain telemetry data.</span></span>

   > [!NOTE]
   > <span data-ttu-id="0914c-195">本節中步驟的其餘部分使用 `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` 目錄。</span><span class="sxs-lookup"><span data-stu-id="0914c-195">For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used.</span></span> <span data-ttu-id="0914c-196">這個目錄可能不存在，除非您的遙測資料是用於 Web 應用程式。</span><span class="sxs-lookup"><span data-stu-id="0914c-196">This directory may not exist unless your telemetry data is for a web app.</span></span>

6. <span data-ttu-id="0914c-197">在下一個儲存格中，輸入下列程式碼︰將 `WASB\_PATH` 取代為前一個步驟中的路徑。</span><span class="sxs-lookup"><span data-stu-id="0914c-197">In the next cell, enter the following code: Replace `WASB\_PATH` with the path from the previous step.</span></span>

   ```scala
   var jsonFiles = sc.textFile('WASB_PATH')
   val sqlContext = new org.apache.spark.sql.SQLContext(sc)
   var jsonData = sqlContext.read.json(jsonFiles)
   ```

    <span data-ttu-id="0914c-198">這個程式碼會從連續匯出程序匯出的 JSON 檔案建立資料框架。</span><span class="sxs-lookup"><span data-stu-id="0914c-198">This code creates a dataframe from the JSON files exported by the continuous export process.</span></span> <span data-ttu-id="0914c-199">使用 **SHIFT + ENTER** 執行此儲存格。</span><span class="sxs-lookup"><span data-stu-id="0914c-199">Use **SHIFT+ENTER** to run this cell.</span></span>

7. <span data-ttu-id="0914c-200">在下一個儲存格中輸入並執行下列命令，以檢視 Spark 為 JSON 檔案建立的結構描述︰</span><span class="sxs-lookup"><span data-stu-id="0914c-200">In the next cell, enter and run the following to view the schema that Spark created for the JSON files:</span></span>

   ```scala
   jsonData.printSchema
   ```

    <span data-ttu-id="0914c-201">每種類型的遙測結構描述皆不同。</span><span class="sxs-lookup"><span data-stu-id="0914c-201">The schema for each type of telemetry is different.</span></span> <span data-ttu-id="0914c-202">以下範例是針對 Web 要求產生的結構描述 (資料儲存在 `Requests` 子目錄中)：</span><span class="sxs-lookup"><span data-stu-id="0914c-202">The following example is the schema that is generated for web requests (data stored in the `Requests` subdirectory):</span></span>

        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)

8. <span data-ttu-id="0914c-203">使用下列命令將資料框架註冊為暫存資料表，並針對資料執行查詢︰</span><span class="sxs-lookup"><span data-stu-id="0914c-203">Use the following to register the dataframe as a temporary table and run a query against the data:</span></span>

   ```scala
   jsonData.registerTempTable("requests")
   var city = sqlContext.sql("select context.location.city from requests where context.location.city is not null limit 10").show()
   ```

    <span data-ttu-id="0914c-204">此查詢會傳回 context.location.city 不是 null 的前 20 筆記錄的 city 資訊。</span><span class="sxs-lookup"><span data-stu-id="0914c-204">This query returns the city information for the top 20 records where context.location.city is not null.</span></span>

   > [!NOTE]
   > <span data-ttu-id="0914c-205">context 結構會出現在 Application Insights 記錄的所有遙測中。</span><span class="sxs-lookup"><span data-stu-id="0914c-205">The context structure is present in all telemetry logged by Application Insights.</span></span> <span data-ttu-id="0914c-206">您的記錄中可能不會填入 city 元素。</span><span class="sxs-lookup"><span data-stu-id="0914c-206">The city element may not be populated in your logs.</span></span> <span data-ttu-id="0914c-207">使用結構描述找出您可以查詢可能包含您的記錄檔資料的其他元素。</span><span class="sxs-lookup"><span data-stu-id="0914c-207">Use the schema to identify other elements that you can query that may contain data for your logs.</span></span>
   >
   >

    <span data-ttu-id="0914c-208">此查詢會傳回類似以下文字的資訊：</span><span class="sxs-lookup"><span data-stu-id="0914c-208">This query returns information similar to the following text:</span></span>

        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## <a name="next-steps"></a><span data-ttu-id="0914c-209">後續步驟</span><span class="sxs-lookup"><span data-stu-id="0914c-209">Next steps</span></span>

<span data-ttu-id="0914c-210">如需在 Azure 中使用 Spark 處理資料和服務的範例，請參閱下列文件︰</span><span class="sxs-lookup"><span data-stu-id="0914c-210">For more examples of using Spark to work with data and services in Azure, see the following documents:</span></span>

* [<span data-ttu-id="0914c-211">Spark 和 BI：搭配 BI 工具來使用 HDInsight 中的 Spark 以執行互動式資料分析</span><span class="sxs-lookup"><span data-stu-id="0914c-211">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="0914c-212">Spark 和機器學習服務：使用 HDInsight 中的 Spark，利用 HVAC 資料來分析建築物溫度</span><span class="sxs-lookup"><span data-stu-id="0914c-212">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="0914c-213">Spark 和機器學習服務：使用 HDInsight 中的 Spark 來預測食品檢查結果</span><span class="sxs-lookup"><span data-stu-id="0914c-213">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="0914c-214">Spark 串流：使用 HDInsight 中的 Spark 來建置串流應用程式</span><span class="sxs-lookup"><span data-stu-id="0914c-214">Spark Streaming: Use Spark in HDInsight for building streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="0914c-215">使用 HDInsight 中的 Spark 進行網站記錄分析</span><span class="sxs-lookup"><span data-stu-id="0914c-215">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

<span data-ttu-id="0914c-216">如需建立和執行 Spark 應用程式的詳細資訊，請參閱下列文件︰</span><span class="sxs-lookup"><span data-stu-id="0914c-216">For information on creating and running Spark applications, see the following documents:</span></span>

* [<span data-ttu-id="0914c-217">使用 Scala 來建立獨立的應用程式</span><span class="sxs-lookup"><span data-stu-id="0914c-217">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="0914c-218">利用 Livy 在 Spark 叢集上遠端執行工作</span><span class="sxs-lookup"><span data-stu-id="0914c-218">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)
