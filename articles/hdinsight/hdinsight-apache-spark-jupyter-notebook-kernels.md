---
title: "Azure HDInsight 中 Spark 叢集上的 Jupyter Notebook 核心 | Microsoft Docs"
description: "了解可透過 Azure HDInsight 上 Spark 叢集提供 Jupyter Notebook 的 PySpark、PySpark3 和 Spark 核心。"
keywords: "spark 上的 jupyter notebook, jupyter spark"
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 6cfd1c1e7b22f5460b78687c815d149e6c6deac9
ms.sourcegitcommit: f537befafb079256fba0529ee554c034d73f36b0
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 07/11/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="d599d-104">Azure HDInsight 中 Spark 叢集上的 Jupyter Notebook 核心</span><span class="sxs-lookup"><span data-stu-id="d599d-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="d599d-105">HDInsight Spark 叢集提供的核心，可讓您用於 Spark 上的 Jupyter Notebook 以測試應用程式。</span><span class="sxs-lookup"><span data-stu-id="d599d-105">HDInsight Spark clusters provide kernels that you can use with the Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="d599d-106">核心是一個可執行並解譯程式碼的程式。</span><span class="sxs-lookup"><span data-stu-id="d599d-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="d599d-107">三個核心為︰</span><span class="sxs-lookup"><span data-stu-id="d599d-107">The three kernels are:</span></span>

- <span data-ttu-id="d599d-108">**PySpark** - 適用於以 Python2 撰寫的應用程式</span><span class="sxs-lookup"><span data-stu-id="d599d-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="d599d-109">**PySpark3** - 適用於以 Python3 撰寫的應用程式</span><span class="sxs-lookup"><span data-stu-id="d599d-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="d599d-110">**Spark** - 適用於以 Scala 撰寫的應用程式</span><span class="sxs-lookup"><span data-stu-id="d599d-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="d599d-111">在本文中，您將了解使用這些核心的方式及優點。</span><span class="sxs-lookup"><span data-stu-id="d599d-111">In this article, you learn how to use these kernels and the benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="d599d-112">必要條件</span><span class="sxs-lookup"><span data-stu-id="d599d-112">Prerequisites</span></span>

* <span data-ttu-id="d599d-113">HDInsight 中的 Apache Spark 叢集。</span><span class="sxs-lookup"><span data-stu-id="d599d-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="d599d-114">如需指示，請參閱 [在 Azure HDInsight 中建立 Apache Spark 叢集](hdinsight-apache-spark-jupyter-spark-sql.md)。</span><span class="sxs-lookup"><span data-stu-id="d599d-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="d599d-115">在 Spark HDInsight 上建立 Jupyter Notebook</span><span class="sxs-lookup"><span data-stu-id="d599d-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="d599d-116">從 [Azure 入口網站](https://portal.azure.com/)，開啟您的叢集。</span><span class="sxs-lookup"><span data-stu-id="d599d-116">From the [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="d599d-117">請參閱[列出和顯示叢集](hdinsight-administer-use-portal-linux.md#list-and-show-clusters)以取得指示。</span><span class="sxs-lookup"><span data-stu-id="d599d-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for the instructions.</span></span> <span data-ttu-id="d599d-118">叢集會在新的入口網站刀鋒視窗中開啟。</span><span class="sxs-lookup"><span data-stu-id="d599d-118">The cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="d599d-119">從 [快速連結] 區段，按一下 [叢集儀表板] 以開啟 [叢集儀表板] 刀鋒視窗。</span><span class="sxs-lookup"><span data-stu-id="d599d-119">From the **Quick links** section, click **Cluster dashboards** to open the **Cluster dashboards** blade.</span></span>  <span data-ttu-id="d599d-120">如果您沒有看見 [快速連結]，請按一下刀鋒視窗上左側功能表中的 [概觀]。</span><span class="sxs-lookup"><span data-stu-id="d599d-120">If you don't see **Quick Links**, click **Overview** from the left menu on the blade.</span></span>

    <span data-ttu-id="d599d-121">![Spark 上的 Jupyter Notebook](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Spark 上的 Jupyter Notebook")</span><span class="sxs-lookup"><span data-stu-id="d599d-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="d599d-122">按一下 [Jupyter Notebook]。</span><span class="sxs-lookup"><span data-stu-id="d599d-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="d599d-123">出現提示時，輸入叢集的系統管理員認證。</span><span class="sxs-lookup"><span data-stu-id="d599d-123">If prompted, enter the admin credentials for the cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="d599d-124">您也可以在瀏覽器中開啟下列 URL，來觸達 Spark 叢集上的 Jupyter Notebook。</span><span class="sxs-lookup"><span data-stu-id="d599d-124">You may also reach the Jupyter notebook on Spark cluster by opening the following URL in your browser.</span></span> <span data-ttu-id="d599d-125">使用您叢集的名稱取代 **CLUSTERNAME** ：</span><span class="sxs-lookup"><span data-stu-id="d599d-125">Replace **CLUSTERNAME** with the name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="d599d-126">按一下 [新增]，然後按一下 [Pyspark]、[PySpark3] 或 [Spark] 建立 Notebook。</span><span class="sxs-lookup"><span data-stu-id="d599d-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** to create a notebook.</span></span> <span data-ttu-id="d599d-127">使用適用於 Scala 應用程式的 Spark 核心、適用於 Python2 應用程式的 PySpark 核心，以及適用於 Python3 應用程式的 PySpark3 核心。</span><span class="sxs-lookup"><span data-stu-id="d599d-127">Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="d599d-128">![Spark 上的 Jupyter Notebook 核心](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Spark 上的 Jupyter Notebook 核心")</span><span class="sxs-lookup"><span data-stu-id="d599d-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="d599d-129">將以您選取的核心開啟 Notebook。</span><span class="sxs-lookup"><span data-stu-id="d599d-129">A notebook opens with the kernel you selected.</span></span>

## <a name="benefits-of-using-the-kernels"></a><span data-ttu-id="d599d-130">使用這些核心的優點</span><span class="sxs-lookup"><span data-stu-id="d599d-130">Benefits of using the kernels</span></span>

<span data-ttu-id="d599d-131">以下是在 Spark HDInsight 叢集上使用新的核心搭配 Jupyter Notebook 的幾個優點。</span><span class="sxs-lookup"><span data-stu-id="d599d-131">Here are a few benefits of using the new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="d599d-132">**預設內容**。</span><span class="sxs-lookup"><span data-stu-id="d599d-132">**Preset contexts**.</span></span> <span data-ttu-id="d599d-133">使用 **PySpark**、**Spark3** 或 **Spark**核心時，您不需要先明確地設定 Spark 或 Hive 內容，即可開始處理您的應用程式。</span><span class="sxs-lookup"><span data-stu-id="d599d-133">With  **PySpark**, **PySpark3**, or the **Spark** kernels, you do not need to set the Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="d599d-134">這些依預設都可用。</span><span class="sxs-lookup"><span data-stu-id="d599d-134">These are available by default.</span></span> <span data-ttu-id="d599d-135">這些內容包括：</span><span class="sxs-lookup"><span data-stu-id="d599d-135">These contexts are:</span></span>
   
   * <span data-ttu-id="d599d-136">**sc** - 代表 Spark 內容</span><span class="sxs-lookup"><span data-stu-id="d599d-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="d599d-137">**sqlContext** - 代表 Hive 內容</span><span class="sxs-lookup"><span data-stu-id="d599d-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="d599d-138">因此，您不需要執行如下的陳述式來設定這些內容：</span><span class="sxs-lookup"><span data-stu-id="d599d-138">So, you don't have to run statements like the following to set the contexts:</span></span>

        <span data-ttu-id="d599d-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="d599d-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="d599d-140">您可以直接在您的應用程式中使用現有的內容。</span><span class="sxs-lookup"><span data-stu-id="d599d-140">Instead, you can directly use the preset contexts in your application.</span></span>

- <span data-ttu-id="d599d-141">**Cell magic**。</span><span class="sxs-lookup"><span data-stu-id="d599d-141">**Cell magics**.</span></span> <span data-ttu-id="d599d-142">PySpark 核心提供一些預先定義的 “magic”，這是您可以使用 `%%` 呼叫的特殊命令 (例如 `%%MAGIC` <args>)。</span><span class="sxs-lookup"><span data-stu-id="d599d-142">The PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="d599d-143">magic 命令必須是程式碼儲存格中的第一個字，而且允許多行的內容。</span><span class="sxs-lookup"><span data-stu-id="d599d-143">The magic command must be the first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="d599d-144">magic 這個字應該是儲存格中的第一個字。</span><span class="sxs-lookup"><span data-stu-id="d599d-144">The magic word should be the first word in the cell.</span></span> <span data-ttu-id="d599d-145">在 magic 前面加入任何項目，甚至是註解，將會造成錯誤。</span><span class="sxs-lookup"><span data-stu-id="d599d-145">Adding anything before the magic, even comments, causes an error.</span></span>     <span data-ttu-id="d599d-146">如需 magic 的詳細資訊，請參閱 [這裡](http://ipython.readthedocs.org/en/stable/interactive/magics.html)。</span><span class="sxs-lookup"><span data-stu-id="d599d-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="d599d-147">下表列出透過核心而提供的不同 magic。</span><span class="sxs-lookup"><span data-stu-id="d599d-147">The following table lists the different magics available through the kernels.</span></span>

   | <span data-ttu-id="d599d-148">Magic</span><span class="sxs-lookup"><span data-stu-id="d599d-148">Magic</span></span> | <span data-ttu-id="d599d-149">範例</span><span class="sxs-lookup"><span data-stu-id="d599d-149">Example</span></span> | <span data-ttu-id="d599d-150">說明</span><span class="sxs-lookup"><span data-stu-id="d599d-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="d599d-151">help</span><span class="sxs-lookup"><span data-stu-id="d599d-151">help</span></span> |`%%help` |<span data-ttu-id="d599d-152">產生所有可用 magic 的表格，其中包含範例與說明</span><span class="sxs-lookup"><span data-stu-id="d599d-152">Generates a table of all the available magics with example and description</span></span> |
   | <span data-ttu-id="d599d-153">info</span><span class="sxs-lookup"><span data-stu-id="d599d-153">info</span></span> |`%%info` |<span data-ttu-id="d599d-154">輸出目前 Livy 端點的工作階段資訊</span><span class="sxs-lookup"><span data-stu-id="d599d-154">Outputs session information for the current Livy endpoint</span></span> |
   | <span data-ttu-id="d599d-155">設定</span><span class="sxs-lookup"><span data-stu-id="d599d-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="d599d-156">`{"executorMemory": "1000M"`,</span><span class="sxs-lookup"><span data-stu-id="d599d-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="d599d-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="d599d-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="d599d-158">設定用來建立工作階段的參數。</span><span class="sxs-lookup"><span data-stu-id="d599d-158">Configures the parameters for creating a session.</span></span> <span data-ttu-id="d599d-159">如果已建立工作階段，則強制旗標 (-f) 是必要的，可確保卸除並重新建立該工作階段。</span><span class="sxs-lookup"><span data-stu-id="d599d-159">The force flag (-f) is mandatory if a session has already been created, which ensures that the session is dropped and recreated.</span></span> <span data-ttu-id="d599d-160">如需有效參數的清單，請查看 [Livy 的 POST /sessions 要求本文](https://github.com/cloudera/livy#request-body) 。</span><span class="sxs-lookup"><span data-stu-id="d599d-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="d599d-161">參數必須以 JSON 字串傳遞，且必須在 magic 之後的下一行，如範例資料行中所示。</span><span class="sxs-lookup"><span data-stu-id="d599d-161">Parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.</span></span> |
   | <span data-ttu-id="d599d-162">sql</span><span class="sxs-lookup"><span data-stu-id="d599d-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="d599d-163">針對 sqlContext 執行 Hive 查詢。</span><span class="sxs-lookup"><span data-stu-id="d599d-163">Executes a Hive query against the sqlContext.</span></span> <span data-ttu-id="d599d-164">如果傳遞 `-o` 參數，則查詢的結果會當做 [Pandas](http://pandas.pydata.org/) 資料框架，保存在 %%local Python 內容中。</span><span class="sxs-lookup"><span data-stu-id="d599d-164">If the `-o` parameter is passed, the result of the query is persisted in the %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="d599d-165">local</span><span class="sxs-lookup"><span data-stu-id="d599d-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="d599d-166">接下來幾行的程式碼全部在本機執行。</span><span class="sxs-lookup"><span data-stu-id="d599d-166">All the code in subsequent lines is executed locally.</span></span> <span data-ttu-id="d599d-167">程式碼必須是有效的 Python2 程式碼，即使與您使用的核心無關也一樣。</span><span class="sxs-lookup"><span data-stu-id="d599d-167">Code must be valid Python2 code even irrespective of the kernel you are using.</span></span> <span data-ttu-id="d599d-168">因此，即使您在建立 Notebook 時選取 **PySpark3** 或 **Spark** 核心，如果您在資料格中使用核心 `%%local` magic，該資料格只能包含有效的 Python2 程式碼。</span><span class="sxs-lookup"><span data-stu-id="d599d-168">So, even if you selected **PySpark3** or **Spark** kernels while creating the notebook, if you use the `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="d599d-169">logs</span><span class="sxs-lookup"><span data-stu-id="d599d-169">logs</span></span> |`%%logs` |<span data-ttu-id="d599d-170">輸出目前 Livy 工作階段的記錄檔。</span><span class="sxs-lookup"><span data-stu-id="d599d-170">Outputs the logs for the current Livy session.</span></span> |
   | <span data-ttu-id="d599d-171">delete</span><span class="sxs-lookup"><span data-stu-id="d599d-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="d599d-172">刪除目前 Livy 端點的特定工作階段。</span><span class="sxs-lookup"><span data-stu-id="d599d-172">Deletes a specific session of the current Livy endpoint.</span></span> <span data-ttu-id="d599d-173">請注意，您無法刪除針對核心本身起始的工作階段。</span><span class="sxs-lookup"><span data-stu-id="d599d-173">Note that you cannot delete the session that is initiated for the kernel itself.</span></span> |
   | <span data-ttu-id="d599d-174">cleanup</span><span class="sxs-lookup"><span data-stu-id="d599d-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="d599d-175">刪除目前 Livy 端點的所有工作階段，包括此 Notebook 的工作階段。</span><span class="sxs-lookup"><span data-stu-id="d599d-175">Deletes all the sessions for the current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="d599d-176">force 旗標 -f 是必要的。</span><span class="sxs-lookup"><span data-stu-id="d599d-176">The force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="d599d-177">除了 PySpark 核心所新增的 Magic，您也可以使用[內建的 IPython Magic](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics) (包括 `%%sh`)。</span><span class="sxs-lookup"><span data-stu-id="d599d-177">In addition to the magics added by the PySpark kernel, you can also use the [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="d599d-178">您可以使用 `%%sh` Magic，在叢集前端節點上執行指令碼和程式碼區塊。</span><span class="sxs-lookup"><span data-stu-id="d599d-178">You can use the `%%sh` magic to run scripts and block of code on the cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="d599d-179">**自動視覺化**。</span><span class="sxs-lookup"><span data-stu-id="d599d-179">**Auto visualization**.</span></span> <span data-ttu-id="d599d-180">**Pyspark** 核心會自動將 Hive 和 SQL 查詢的輸出視覺化。</span><span class="sxs-lookup"><span data-stu-id="d599d-180">The **Pyspark** kernel automatically visualizes the output of Hive and SQL queries.</span></span> <span data-ttu-id="d599d-181">有數種不同類型的視覺效果供您選擇，包括資料表、圓形圖、線條、區域、長條圖。</span><span class="sxs-lookup"><span data-stu-id="d599d-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-the-sql-magic"></a><span data-ttu-id="d599d-182">%%sql magic 支援的參數</span><span class="sxs-lookup"><span data-stu-id="d599d-182">Parameters supported with the %%sql magic</span></span>
<span data-ttu-id="d599d-183">`%%sql` magic 支援不同的參數，可用來控制您執行查詢時收到的輸出類型。</span><span class="sxs-lookup"><span data-stu-id="d599d-183">The `%%sql` magic supports different parameters that you can use to control the kind of output that you receive when you run queries.</span></span> <span data-ttu-id="d599d-184">下表列出輸出。</span><span class="sxs-lookup"><span data-stu-id="d599d-184">The following table lists the output.</span></span>

| <span data-ttu-id="d599d-185">參數</span><span class="sxs-lookup"><span data-stu-id="d599d-185">Parameter</span></span> | <span data-ttu-id="d599d-186">範例</span><span class="sxs-lookup"><span data-stu-id="d599d-186">Example</span></span> | <span data-ttu-id="d599d-187">說明</span><span class="sxs-lookup"><span data-stu-id="d599d-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="d599d-188">-o</span><span class="sxs-lookup"><span data-stu-id="d599d-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="d599d-189">使用此參數，在 %%local Python 內容中保存查詢的結果，以做為 [Pandas](http://pandas.pydata.org/) 資料框架。</span><span class="sxs-lookup"><span data-stu-id="d599d-189">Use this parameter to persist the result of the query, in the %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="d599d-190">資料框架變數的名稱是您指定的變數名稱。</span><span class="sxs-lookup"><span data-stu-id="d599d-190">The name of the dataframe variable is the variable name you specify.</span></span> |
| <span data-ttu-id="d599d-191">-q</span><span class="sxs-lookup"><span data-stu-id="d599d-191">-q</span></span> |`-q` |<span data-ttu-id="d599d-192">使用此項關閉儲存格的視覺效果。</span><span class="sxs-lookup"><span data-stu-id="d599d-192">Use this to turn off visualizations for the cell.</span></span> <span data-ttu-id="d599d-193">如果您不想自動將儲存格內容視覺化，而且只想擷取它做為資料框架，則可使用 `-q -o <VARIABLE>`。</span><span class="sxs-lookup"><span data-stu-id="d599d-193">If you don't want to auto-visualize the content of a cell and just want to capture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="d599d-194">如果您想要關閉視覺化功能而不擷取結果 (例如，執行 SQL 查詢的 `CREATE TABLE` 陳述式)，請使用 `-q` 但不要指定 `-o` 引數。</span><span class="sxs-lookup"><span data-stu-id="d599d-194">If you want to turn off visualizations without capturing the results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="d599d-195">-m</span><span class="sxs-lookup"><span data-stu-id="d599d-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="d599d-196">其中 **METHOD** 是 **take** 或 **sample** (預設值是 **take**)。</span><span class="sxs-lookup"><span data-stu-id="d599d-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="d599d-197">如果方法是 **take**，核心會從 MAXROWS 指定的結果資料集頂端挑選項目 (如此表稍後所述)。</span><span class="sxs-lookup"><span data-stu-id="d599d-197">If the method is **take**, the kernel picks elements from the top of the result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="d599d-198">如果方法是 **sample**，核心會根據 `-r` 參數隨機取樣資料集的項目，如此表稍後所述。</span><span class="sxs-lookup"><span data-stu-id="d599d-198">If the method is **sample**, the kernel randomly samples elements of the data set according to `-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="d599d-199">-r</span><span class="sxs-lookup"><span data-stu-id="d599d-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="d599d-200">這裡的 **FRACTION** 是介於 0.0 到 1.0 之間的浮點數。</span><span class="sxs-lookup"><span data-stu-id="d599d-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="d599d-201">如果 SQL 查詢的範例方法是 `sample`，則核心會為您從結果集隨機取樣指定比例的項目。</span><span class="sxs-lookup"><span data-stu-id="d599d-201">If the sample method for the SQL query is `sample`, then the kernel randomly samples the specified fraction of the elements of the result set for you.</span></span> <span data-ttu-id="d599d-202">例如，如果您使用 `-m sample -r 0.01` 引數執行 SQL 查詢，則會隨機取樣 1% 的結果資料列。</span><span class="sxs-lookup"><span data-stu-id="d599d-202">For example, if you run a SQL query with the arguments `-m sample -r 0.01`, then 1% of the result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="d599d-203">**MAXROWS** 是整數值。</span><span class="sxs-lookup"><span data-stu-id="d599d-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="d599d-204">核心會將輸出資料列的數目限制為 **MAXROWS**。</span><span class="sxs-lookup"><span data-stu-id="d599d-204">The kernel limits the number of output rows to **MAXROWS**.</span></span> <span data-ttu-id="d599d-205">如果 **MAXROWS** 是負數 (例如 **-1**)，則結果集中的資料列數目不會受到限制。</span><span class="sxs-lookup"><span data-stu-id="d599d-205">If **MAXROWS** is a negative number such as **-1**, then the number of rows in the result set is not limited.</span></span> |

<span data-ttu-id="d599d-206">**範例：**</span><span class="sxs-lookup"><span data-stu-id="d599d-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="d599d-207">上面的陳述式會執行下列動作︰</span><span class="sxs-lookup"><span data-stu-id="d599d-207">The statement above does the following:</span></span>

* <span data-ttu-id="d599d-208">從 **hivesampletable**選取所有記錄。</span><span class="sxs-lookup"><span data-stu-id="d599d-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="d599d-209">因為我們使用 -q，所以它會關閉自動視覺效果。</span><span class="sxs-lookup"><span data-stu-id="d599d-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="d599d-210">因為我們使用 `-m sample -r 0.1 -n 500` ，所以它會從 hivesampletable 的資料列中隨機取樣 10%，並將結果集的大小限制為 500 個資料列。</span><span class="sxs-lookup"><span data-stu-id="d599d-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of the rows in the hivesampletable and limits the size of the result set to 500 rows.</span></span>
* <span data-ttu-id="d599d-211">最後，因為我們使用 `-o query2` ，所以它也會將輸出儲存成名為 **query2**的資料框架。</span><span class="sxs-lookup"><span data-stu-id="d599d-211">Finally, because we used `-o query2` it also saves the output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-the-new-kernels"></a><span data-ttu-id="d599d-212">使用新核心的考量</span><span class="sxs-lookup"><span data-stu-id="d599d-212">Considerations while using the new kernels</span></span>

<span data-ttu-id="d599d-213">無論您使用何種核心，讓 Notebook 持續執行會耗用叢集資源。</span><span class="sxs-lookup"><span data-stu-id="d599d-213">Whichever kernel you use, leaving the notebooks running consumes the cluster resources.</span></span>  <span data-ttu-id="d599d-214">針對這些核心，由於已預設內容，光是結束 Notebook 並不會終止內容，因此會繼續使用叢集資源。</span><span class="sxs-lookup"><span data-stu-id="d599d-214">With these kernels, because the contexts are preset, simply exiting the notebooks does not kill the context and hence the cluster resources continue to be in use.</span></span> <span data-ttu-id="d599d-215">理想的作法是當 Notebook 使用完畢時，從 Notebook 的 [檔案] 功能表中使用 [關閉並終止] 選項，這會刪除內容，然後結束 Notebook。</span><span class="sxs-lookup"><span data-stu-id="d599d-215">A good practice is to use the **Close and Halt** option from the notebook's **File** menu when you are finished using the notebook, which kills the context and then exits the notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="d599d-216">請舉例說明</span><span class="sxs-lookup"><span data-stu-id="d599d-216">Show me some examples</span></span>

<span data-ttu-id="d599d-217">當您開啟 Jupyter Notebook 時，您會在根層級看到兩個可用的資料夾。</span><span class="sxs-lookup"><span data-stu-id="d599d-217">When you open a Jupyter notebook, you see two folders available at the root level.</span></span>

* <span data-ttu-id="d599d-218">**PySpark** 資料夾含有使用新 **Python** 核心的範例 Notebook。</span><span class="sxs-lookup"><span data-stu-id="d599d-218">The **PySpark** folder has sample notebooks that use the new **Python** kernel.</span></span>
* <span data-ttu-id="d599d-219">**Scala** 資料夾含有使用新 **Spark** 核心的範例 Notebook。</span><span class="sxs-lookup"><span data-stu-id="d599d-219">The **Scala** folder has sample notebooks that use the new **Spark** kernel.</span></span>

<span data-ttu-id="d599d-220">您可以從 **PySpark** 或 **Spark** 資料夾開啟 **00 - [READ ME FIRST] Spark Magic Kernel Features** Notebook，以了解各種可用的 Magic。</span><span class="sxs-lookup"><span data-stu-id="d599d-220">You can open the **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from the **PySpark** or **Spark** folder to learn about the different magics available.</span></span> <span data-ttu-id="d599d-221">您也可以使用這兩個資料夾底下提供的其他 Notebook 範例，以了解如何搭配 HDInsight Spark 叢集使用 Jupyter Notebook 完成不同的案例。</span><span class="sxs-lookup"><span data-stu-id="d599d-221">You can also use the other sample notebooks available under the two folders to learn how to achieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-the-notebooks-stored"></a><span data-ttu-id="d599d-222">Notebook 會儲存在哪裡？</span><span class="sxs-lookup"><span data-stu-id="d599d-222">Where are the notebooks stored?</span></span>

<span data-ttu-id="d599d-223">Jupyter 筆記本會儲存到 **/HdiNotebooks** 資料夾下，與叢集相關聯的儲存體帳戶。</span><span class="sxs-lookup"><span data-stu-id="d599d-223">Jupyter notebooks are saved to the storage account associated with the cluster under the **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="d599d-224">您從 Jupyter 內建立的 Notebook、文字檔案和資料夾，都可從儲存體帳戶存取。</span><span class="sxs-lookup"><span data-stu-id="d599d-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from the storage account.</span></span>  <span data-ttu-id="d599d-225">例如，如果您使用 Jupyter 建立資料夾 **myfolder** 和 Notebook **myfolder/mynotebook.ipynb**，您可以在儲存體帳戶內從 `/HdiNotebooks/myfolder/mynotebook.ipynb` 存取該 Notebook。</span><span class="sxs-lookup"><span data-stu-id="d599d-225">For example, if you use Jupyter to create a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within the storage account.</span></span>  <span data-ttu-id="d599d-226">反之亦然，也就是說，如果您直接將 Notebook 上傳至儲存體帳戶的 `/HdiNotebooks/mynotebook1.ipynb`，則從 Jupyter 也能看到該 Notebook。</span><span class="sxs-lookup"><span data-stu-id="d599d-226">The reverse is also true, that is, if you upload a notebook directly to your storage account at `/HdiNotebooks/mynotebook1.ipynb`, the notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="d599d-227">即使在刪除叢集之後，Notebook 仍會保留在儲存體帳戶中。</span><span class="sxs-lookup"><span data-stu-id="d599d-227">Notebooks remain in the storage account even after the cluster is deleted.</span></span>

<span data-ttu-id="d599d-228">將 Notebook 儲存到儲存體帳戶的方式與 HDFS 相容。</span><span class="sxs-lookup"><span data-stu-id="d599d-228">The way notebooks are saved to the storage account is compatible with HDFS.</span></span> <span data-ttu-id="d599d-229">因此，如果對叢集執行 SSH，您可以使用檔案管理命令，如下列程式碼片段所示︰</span><span class="sxs-lookup"><span data-stu-id="d599d-229">So, if you SSH into the cluster you can use file management commands as shown in the following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at the root directory – everything in this directory is visible to Jupyter from the home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download the contents of the HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb to the root folder so it’s visible from Jupyter


<span data-ttu-id="d599d-230">萬一叢集有儲存體帳戶存取問題，Notebook 也會儲存在前端節點 `/var/lib/jupyter`上。</span><span class="sxs-lookup"><span data-stu-id="d599d-230">In case there are issues accessing the storage account for the cluster, the notebooks are also saved on the headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="d599d-231">支援的瀏覽器</span><span class="sxs-lookup"><span data-stu-id="d599d-231">Supported browser</span></span>

<span data-ttu-id="d599d-232">Google Chrome 上只支援 Spark HDInsight 叢集上的 Jupyter Notebook。</span><span class="sxs-lookup"><span data-stu-id="d599d-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="d599d-233">意見反應</span><span class="sxs-lookup"><span data-stu-id="d599d-233">Feedback</span></span>
<span data-ttu-id="d599d-234">新的核心已在發展階段，而且經過一段時間後將會成熟。</span><span class="sxs-lookup"><span data-stu-id="d599d-234">The new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="d599d-235">這或許也意味著，API 可能會隨著這些核心的成熟而改變。</span><span class="sxs-lookup"><span data-stu-id="d599d-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="d599d-236">您在使用這些新核心時如有任何意見，我們都非常樂於知道。</span><span class="sxs-lookup"><span data-stu-id="d599d-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="d599d-237">這對於這些核心最終版本的定調很有幫助。</span><span class="sxs-lookup"><span data-stu-id="d599d-237">This is useful in shaping the final release of these kernels.</span></span> <span data-ttu-id="d599d-238">您可以將您的評論/意見反應填寫在本文最後的 **評論** 一節底下。</span><span class="sxs-lookup"><span data-stu-id="d599d-238">You can leave your comments/feedback under the **Comments** section at the bottom of this article.</span></span>

## <span data-ttu-id="d599d-239"><a name="seealso"></a>另請參閱</span><span class="sxs-lookup"><span data-stu-id="d599d-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="d599d-240">概觀：Azure HDInsight 上的 Apache Spark</span><span class="sxs-lookup"><span data-stu-id="d599d-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="d599d-241">案例</span><span class="sxs-lookup"><span data-stu-id="d599d-241">Scenarios</span></span>
* [<span data-ttu-id="d599d-242">Spark 和 BI：在 HDInsight 中搭配使用 Spark 和 BI 工具執行互動式資料分析</span><span class="sxs-lookup"><span data-stu-id="d599d-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="d599d-243">Spark 和機器學習服務：使用 HDInsight 中的 Spark，利用 HVAC 資料來分析建築物溫度</span><span class="sxs-lookup"><span data-stu-id="d599d-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="d599d-244">Spark 和機器學習服務：使用 HDInsight 中的 Spark 來預測食品檢查結果</span><span class="sxs-lookup"><span data-stu-id="d599d-244">Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="d599d-245">Spark 串流：使用 HDInsight 中的 Spark 來建置即時串流應用程式</span><span class="sxs-lookup"><span data-stu-id="d599d-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="d599d-246">使用 HDInsight 中的 Spark 進行網站記錄分析</span><span class="sxs-lookup"><span data-stu-id="d599d-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="d599d-247">建立及執行應用程式</span><span class="sxs-lookup"><span data-stu-id="d599d-247">Create and run applications</span></span>
* [<span data-ttu-id="d599d-248">使用 Scala 建立獨立應用程式</span><span class="sxs-lookup"><span data-stu-id="d599d-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="d599d-249">利用 Livy 在 Spark 叢集上遠端執行作業</span><span class="sxs-lookup"><span data-stu-id="d599d-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="d599d-250">工具和擴充功能</span><span class="sxs-lookup"><span data-stu-id="d599d-250">Tools and extensions</span></span>
* [<span data-ttu-id="d599d-251">使用 IntelliJ IDEA 的 HDInsight Tools 外掛程式來建立和提交 Spark Scala 應用程式</span><span class="sxs-lookup"><span data-stu-id="d599d-251">Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="d599d-252">使用 IntelliJ IDEA 的 HDInsight Tools 外掛程式遠端偵錯 Spark 應用程式</span><span class="sxs-lookup"><span data-stu-id="d599d-252">Use HDInsight Tools Plugin for IntelliJ IDEA to debug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="d599d-253">利用 HDInsight 上的 Spark 叢集來使用 Zeppelin Notebook</span><span class="sxs-lookup"><span data-stu-id="d599d-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="d599d-254">搭配 Jupyter Notebook 使用外部套件</span><span class="sxs-lookup"><span data-stu-id="d599d-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="d599d-255">在電腦上安裝 Jupyter 並連接到 HDInsight Spark 叢集</span><span class="sxs-lookup"><span data-stu-id="d599d-255">Install Jupyter on your computer and connect to an HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="d599d-256">管理資源</span><span class="sxs-lookup"><span data-stu-id="d599d-256">Manage resources</span></span>
* [<span data-ttu-id="d599d-257">在 Azure HDInsight 中管理 Apache Spark 叢集的資源</span><span class="sxs-lookup"><span data-stu-id="d599d-257">Manage resources for the Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="d599d-258">追蹤和偵錯在 HDInsight 中的 Apache Spark 叢集上執行的作業</span><span class="sxs-lookup"><span data-stu-id="d599d-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
