---
title: "在 Azure HDInsight 叢集上的 Spark Jupyter 筆記本 aaaKernels |Microsoft 文件"
description: "深入了解適用於 Azure HDInsight 上的 Spark 叢集 Jupyter 筆記本的 hello PySpark、 PySpark3 和 Spark 核心。"
keywords: "spark 上的 jupyter notebook, jupyter spark"
services: hdinsight
documentationcenter: 
author: nitinme
manager: jhubbard
editor: cgronlun
tags: azure-portal
ms.assetid: 0719e503-ee6d-41ac-b37e-3d77db8b121b
ms.service: hdinsight
ms.custom: hdinsightactive,hdiseo17may2017
ms.workload: big-data
ms.tgt_pltfrm: na
ms.devlang: na
ms.topic: article
ms.date: 05/15/2017
ms.author: nitinme
ms.openlocfilehash: 560c944fe850c5753ac9fa90550b804f0c47d14c
ms.sourcegitcommit: 523283cc1b3c37c428e77850964dc1c33742c5f0
ms.translationtype: MT
ms.contentlocale: zh-TW
ms.lasthandoff: 10/06/2017
---
# <a name="kernels-for-jupyter-notebook-on-spark-clusters-in-azure-hdinsight"></a><span data-ttu-id="69964-104">Azure HDInsight 中 Spark 叢集上的 Jupyter Notebook 核心</span><span class="sxs-lookup"><span data-stu-id="69964-104">Kernels for Jupyter notebook on Spark clusters in Azure HDInsight</span></span> 

<span data-ttu-id="69964-105">HDInsight Spark 叢集提供可用於與 hello Jupyter 筆記本 Spark 上測試您的應用程式的核心。</span><span class="sxs-lookup"><span data-stu-id="69964-105">HDInsight Spark clusters provide kernels that you can use with hello Jupyter notebook on Spark for testing your applications.</span></span> <span data-ttu-id="69964-106">核心是一個可執行並解譯程式碼的程式。</span><span class="sxs-lookup"><span data-stu-id="69964-106">A kernel is a program that runs and interprets your code.</span></span> <span data-ttu-id="69964-107">hello 三個核心︰</span><span class="sxs-lookup"><span data-stu-id="69964-107">hello three kernels are:</span></span>

- <span data-ttu-id="69964-108">**PySpark** - 適用於以 Python2 撰寫的應用程式</span><span class="sxs-lookup"><span data-stu-id="69964-108">**PySpark** - for applications written in Python2</span></span>
- <span data-ttu-id="69964-109">**PySpark3** - 適用於以 Python3 撰寫的應用程式</span><span class="sxs-lookup"><span data-stu-id="69964-109">**PySpark3** - for applications written in Python3</span></span>
- <span data-ttu-id="69964-110">**Spark** - 適用於以 Scala 撰寫的應用程式</span><span class="sxs-lookup"><span data-stu-id="69964-110">**Spark** - for applications written in Scala</span></span>

<span data-ttu-id="69964-111">在本文中，您將學習如何 toouse 這些核心和使用它們的 hello 優點。</span><span class="sxs-lookup"><span data-stu-id="69964-111">In this article, you learn how toouse these kernels and hello benefits of using them.</span></span>

## <a name="prerequisites"></a><span data-ttu-id="69964-112">必要條件</span><span class="sxs-lookup"><span data-stu-id="69964-112">Prerequisites</span></span>

* <span data-ttu-id="69964-113">HDInsight 中的 Apache Spark 叢集。</span><span class="sxs-lookup"><span data-stu-id="69964-113">An Apache Spark cluster in HDInsight.</span></span> <span data-ttu-id="69964-114">如需指示，請參閱 [在 Azure HDInsight 中建立 Apache Spark 叢集](hdinsight-apache-spark-jupyter-spark-sql.md)。</span><span class="sxs-lookup"><span data-stu-id="69964-114">For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).</span></span>

## <a name="create-a-jupyter-notebook-on-spark-hdinsight"></a><span data-ttu-id="69964-115">在 Spark HDInsight 上建立 Jupyter Notebook</span><span class="sxs-lookup"><span data-stu-id="69964-115">Create a Jupyter notebook on Spark HDInsight</span></span>

1. <span data-ttu-id="69964-116">從 hello [Azure 入口網站](https://portal.azure.com/)，開啟您的叢集。</span><span class="sxs-lookup"><span data-stu-id="69964-116">From hello [Azure portal](https://portal.azure.com/), open your cluster.</span></span>  <span data-ttu-id="69964-117">請參閱[清單和顯示叢集](hdinsight-administer-use-portal-linux.md#list-and-show-clusters)hello 的指示。</span><span class="sxs-lookup"><span data-stu-id="69964-117">See [List and show clusters](hdinsight-administer-use-portal-linux.md#list-and-show-clusters) for hello instructions.</span></span> <span data-ttu-id="69964-118">hello 叢集的新入口網站的刀鋒視窗中開啟。</span><span class="sxs-lookup"><span data-stu-id="69964-118">hello cluster is opened in a new portal blade.</span></span>

2. <span data-ttu-id="69964-119">從 hello**快速連結**區段中，按一下**叢集儀表板**tooopen hello**叢集儀表板**刀鋒視窗。</span><span class="sxs-lookup"><span data-stu-id="69964-119">From hello **Quick links** section, click **Cluster dashboards** tooopen hello **Cluster dashboards** blade.</span></span>  <span data-ttu-id="69964-120">如果您沒有看到**快速連結**，按一下 [**概觀**hello hello] 刀鋒視窗的左側功能表中。</span><span class="sxs-lookup"><span data-stu-id="69964-120">If you don't see **Quick Links**, click **Overview** from hello left menu on hello blade.</span></span>

    <span data-ttu-id="69964-121">![Spark 上的 Jupyter Notebook](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Spark 上的 Jupyter Notebook")</span><span class="sxs-lookup"><span data-stu-id="69964-121">![Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/hdinsight-jupyter-notebook-on-spark.png "Jupyter notebook on Spark")</span></span> 

3. <span data-ttu-id="69964-122">按一下 [Jupyter Notebook]。</span><span class="sxs-lookup"><span data-stu-id="69964-122">Click **Jupyter Notebook**.</span></span> <span data-ttu-id="69964-123">如果出現提示，請輸入 hello 叢集 hello 系統管理員認證。</span><span class="sxs-lookup"><span data-stu-id="69964-123">If prompted, enter hello admin credentials for hello cluster.</span></span>
   
   > [!NOTE]
   > <span data-ttu-id="69964-124">您也可能會達到 hello Jupyter 筆記本，由下列 URL 在瀏覽器中開啟 hello 的 Spark 叢集上。</span><span class="sxs-lookup"><span data-stu-id="69964-124">You may also reach hello Jupyter notebook on Spark cluster by opening hello following URL in your browser.</span></span> <span data-ttu-id="69964-125">取代**CLUSTERNAME** hello 名稱，為您的叢集：</span><span class="sxs-lookup"><span data-stu-id="69964-125">Replace **CLUSTERNAME** with hello name of your cluster:</span></span>
   >
   > `https://CLUSTERNAME.azurehdinsight.net/jupyter`
   > 
   > 

3. <span data-ttu-id="69964-126">按一下**新增**，然後按一下 **Pyspark**， **PySpark3**，或**Spark** toocreate 筆記本。</span><span class="sxs-lookup"><span data-stu-id="69964-126">Click **New**, and then click either **Pyspark**, **PySpark3**, or **Spark** toocreate a notebook.</span></span> <span data-ttu-id="69964-127">使用 Scala 應用程式的 hello Spark 核心、 PySpark 核心 Python2 應用程式和 PySpark3 核心 Python3 應用程式。</span><span class="sxs-lookup"><span data-stu-id="69964-127">Use hello Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.</span></span>
   
    <span data-ttu-id="69964-128">![Spark 上的 Jupyter Notebook 核心](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Spark 上的 Jupyter Notebook 核心")</span><span class="sxs-lookup"><span data-stu-id="69964-128">![Kernels for Jupyter notebook on Spark](./media/hdinsight-apache-spark-jupyter-notebook-kernels/kernel-jupyter-notebook-on-spark.png "Kernels for Jupyter notebook on Spark")</span></span> 

4. <span data-ttu-id="69964-129">筆記本會開啟與您選取的 hello 核心。</span><span class="sxs-lookup"><span data-stu-id="69964-129">A notebook opens with hello kernel you selected.</span></span>

## <a name="benefits-of-using-hello-kernels"></a><span data-ttu-id="69964-130">使用 hello 核心的優點</span><span class="sxs-lookup"><span data-stu-id="69964-130">Benefits of using hello kernels</span></span>

<span data-ttu-id="69964-131">以下是使用 hello 新核心 Jupyter 筆記本 Spark HDInsight 叢集上的一些優點。</span><span class="sxs-lookup"><span data-stu-id="69964-131">Here are a few benefits of using hello new kernels with Jupyter notebook on Spark HDInsight clusters.</span></span>

- <span data-ttu-id="69964-132">**預設內容**。</span><span class="sxs-lookup"><span data-stu-id="69964-132">**Preset contexts**.</span></span> <span data-ttu-id="69964-133">與**PySpark**， **PySpark3**，或使用 hello **Spark**核心，您不需要 tooset hello Spark 或登錄區內容明確之前您開始使用您的應用程式。</span><span class="sxs-lookup"><span data-stu-id="69964-133">With  **PySpark**, **PySpark3**, or hello **Spark** kernels, you do not need tooset hello Spark or Hive contexts explicitly before you start working with your applications.</span></span> <span data-ttu-id="69964-134">這些依預設都可用。</span><span class="sxs-lookup"><span data-stu-id="69964-134">These are available by default.</span></span> <span data-ttu-id="69964-135">這些內容包括：</span><span class="sxs-lookup"><span data-stu-id="69964-135">These contexts are:</span></span>
   
   * <span data-ttu-id="69964-136">**sc** - 代表 Spark 內容</span><span class="sxs-lookup"><span data-stu-id="69964-136">**sc** - for Spark context</span></span>
   * <span data-ttu-id="69964-137">**sqlContext** - 代表 Hive 內容</span><span class="sxs-lookup"><span data-stu-id="69964-137">**sqlContext** - for Hive context</span></span>

    <span data-ttu-id="69964-138">因此，您不需要像 hello 遵循 tooset hello 內容 toorun 陳述式：</span><span class="sxs-lookup"><span data-stu-id="69964-138">So, you don't have toorun statements like hello following tooset hello contexts:</span></span>

        <span data-ttu-id="69964-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span><span class="sxs-lookup"><span data-stu-id="69964-139">sc = SparkContext('yarn-client')    sqlContext = HiveContext(sc)</span></span>

    <span data-ttu-id="69964-140">相反地，您可以直接使用 hello 預設應用程式中的內容。</span><span class="sxs-lookup"><span data-stu-id="69964-140">Instead, you can directly use hello preset contexts in your application.</span></span>

- <span data-ttu-id="69964-141">**Cell magic**。</span><span class="sxs-lookup"><span data-stu-id="69964-141">**Cell magics**.</span></span> <span data-ttu-id="69964-142">hello PySpark 核心提供一些預先定義 「 我們"，這是特殊的命令，您可以呼叫與`%%`(例如， `%%MAGIC` <args>)。</span><span class="sxs-lookup"><span data-stu-id="69964-142">hello PySpark kernel provides some predefined “magics”, which are special commands that you can call with `%%` (for example, `%%MAGIC` <args>).</span></span> <span data-ttu-id="69964-143">hello magic 命令必須在程式碼的儲存格的 hello 第一個字以及允許多行的內容。</span><span class="sxs-lookup"><span data-stu-id="69964-143">hello magic command must be hello first word in a code cell and allow for multiple lines of content.</span></span> <span data-ttu-id="69964-144">hello 非常重要，應該 hello hello 資料格中的第一個字。</span><span class="sxs-lookup"><span data-stu-id="69964-144">hello magic word should be hello first word in hello cell.</span></span> <span data-ttu-id="69964-145">將加入 hello magic，甚至是註解之前, 的任何項目會導致錯誤。</span><span class="sxs-lookup"><span data-stu-id="69964-145">Adding anything before hello magic, even comments, causes an error.</span></span>     <span data-ttu-id="69964-146">如需 magic 的詳細資訊，請參閱 [這裡](http://ipython.readthedocs.org/en/stable/interactive/magics.html)。</span><span class="sxs-lookup"><span data-stu-id="69964-146">For more information on magics, see [here](http://ipython.readthedocs.org/en/stable/interactive/magics.html).</span></span>
   
    <span data-ttu-id="69964-147">hello 下表列出可透過 hello 核心 hello 不同我們。</span><span class="sxs-lookup"><span data-stu-id="69964-147">hello following table lists hello different magics available through hello kernels.</span></span>

   | <span data-ttu-id="69964-148">Magic</span><span class="sxs-lookup"><span data-stu-id="69964-148">Magic</span></span> | <span data-ttu-id="69964-149">範例</span><span class="sxs-lookup"><span data-stu-id="69964-149">Example</span></span> | <span data-ttu-id="69964-150">說明</span><span class="sxs-lookup"><span data-stu-id="69964-150">Description</span></span> |
   | --- | --- | --- |
   | <span data-ttu-id="69964-151">help</span><span class="sxs-lookup"><span data-stu-id="69964-151">help</span></span> |`%%help` |<span data-ttu-id="69964-152">會產生所有 hello 可用我們使用範例和描述的資料表</span><span class="sxs-lookup"><span data-stu-id="69964-152">Generates a table of all hello available magics with example and description</span></span> |
   | <span data-ttu-id="69964-153">info</span><span class="sxs-lookup"><span data-stu-id="69964-153">info</span></span> |`%%info` |<span data-ttu-id="69964-154">Hello 目前晚總端點的輸出工作階段資訊</span><span class="sxs-lookup"><span data-stu-id="69964-154">Outputs session information for hello current Livy endpoint</span></span> |
   | <span data-ttu-id="69964-155">設定</span><span class="sxs-lookup"><span data-stu-id="69964-155">configure</span></span> |`%%configure -f`<br><span data-ttu-id="69964-156">`{"executorMemory": "1000M"`,</span><span class="sxs-lookup"><span data-stu-id="69964-156">`{"executorMemory": "1000M"`,</span></span><br><span data-ttu-id="69964-157">`"executorCores": 4`}</span><span class="sxs-lookup"><span data-stu-id="69964-157">`"executorCores": 4`}</span></span> |<span data-ttu-id="69964-158">設定 hello 參數建立工作階段。</span><span class="sxs-lookup"><span data-stu-id="69964-158">Configures hello parameters for creating a session.</span></span> <span data-ttu-id="69964-159">hello force 旗標 (-f) 是強制性，如果已建立工作階段，以確保該 hello 工作階段卸除並重新建立。</span><span class="sxs-lookup"><span data-stu-id="69964-159">hello force flag (-f) is mandatory if a session has already been created, which ensures that hello session is dropped and recreated.</span></span> <span data-ttu-id="69964-160">如需有效參數的清單，請查看 [Livy 的 POST /sessions 要求本文](https://github.com/cloudera/livy#request-body) 。</span><span class="sxs-lookup"><span data-stu-id="69964-160">Look at [Livy's POST /sessions Request Body](https://github.com/cloudera/livy#request-body) for a list of valid parameters.</span></span> <span data-ttu-id="69964-161">參數必須為 JSON 字串中傳遞且必須在 hello 下一行之後 hello magic hello 範例資料行中所示。</span><span class="sxs-lookup"><span data-stu-id="69964-161">Parameters must be passed in as a JSON string and must be on hello next line after hello magic, as shown in hello example column.</span></span> |
   | <span data-ttu-id="69964-162">sql</span><span class="sxs-lookup"><span data-stu-id="69964-162">sql</span></span> |`%%sql -o <variable name>`<br> `SHOW TABLES` |<span data-ttu-id="69964-163">執行 Hive 查詢針對 hello sqlContext。</span><span class="sxs-lookup"><span data-stu-id="69964-163">Executes a Hive query against hello sqlContext.</span></span> <span data-ttu-id="69964-164">如果 hello`-o`參數傳遞，hello hello 查詢結果會持續保留在 hello %%做為本機的 Python 內容[熊](http://pandas.pydata.org/)資料框架。</span><span class="sxs-lookup"><span data-stu-id="69964-164">If hello `-o` parameter is passed, hello result of hello query is persisted in hello %%local Python context as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> |
   | <span data-ttu-id="69964-165">local</span><span class="sxs-lookup"><span data-stu-id="69964-165">local</span></span> |`%%local`<br>`a=1` |<span data-ttu-id="69964-166">接下來幾行中的所有 hello 程式碼以本機方式都執行。</span><span class="sxs-lookup"><span data-stu-id="69964-166">All hello code in subsequent lines is executed locally.</span></span> <span data-ttu-id="69964-167">程式碼必須是有效的 Python2 程式碼，即使不論您使用的 hello 核心。</span><span class="sxs-lookup"><span data-stu-id="69964-167">Code must be valid Python2 code even irrespective of hello kernel you are using.</span></span> <span data-ttu-id="69964-168">因此，即使您選取**PySpark3**或**Spark**時建立 hello 筆記本中，如果您使用 hello 核心`%%local`magic 儲存格，該資料格必須只有有效 Python2 程式碼...</span><span class="sxs-lookup"><span data-stu-id="69964-168">So, even if you selected **PySpark3** or **Spark** kernels while creating hello notebook, if you use hello `%%local` magic in a cell, that cell must only have valid Python2 code..</span></span> |
   | <span data-ttu-id="69964-169">logs</span><span class="sxs-lookup"><span data-stu-id="69964-169">logs</span></span> |`%%logs` |<span data-ttu-id="69964-170">輸出 hello hello 目前晚總工作階段記錄檔。</span><span class="sxs-lookup"><span data-stu-id="69964-170">Outputs hello logs for hello current Livy session.</span></span> |
   | <span data-ttu-id="69964-171">delete</span><span class="sxs-lookup"><span data-stu-id="69964-171">delete</span></span> |`%%delete -f -s <session number>` |<span data-ttu-id="69964-172">刪除特定的工作階段的 hello 目前晚總端點。</span><span class="sxs-lookup"><span data-stu-id="69964-172">Deletes a specific session of hello current Livy endpoint.</span></span> <span data-ttu-id="69964-173">請注意，您無法刪除 hello 工作階段起始 hello 核心本身。</span><span class="sxs-lookup"><span data-stu-id="69964-173">Note that you cannot delete hello session that is initiated for hello kernel itself.</span></span> |
   | <span data-ttu-id="69964-174">cleanup</span><span class="sxs-lookup"><span data-stu-id="69964-174">cleanup</span></span> |`%%cleanup -f` |<span data-ttu-id="69964-175">刪除 hello 目前晚總端點使用，包括此筆記本工作階段的所有 hello 工作階段。</span><span class="sxs-lookup"><span data-stu-id="69964-175">Deletes all hello sessions for hello current Livy endpoint, including this notebook's session.</span></span> <span data-ttu-id="69964-176">hello force 旗標-f 是必要的。</span><span class="sxs-lookup"><span data-stu-id="69964-176">hello force flag -f is mandatory.</span></span> |

   > [!NOTE]
   > <span data-ttu-id="69964-177">此外 toohello 我們加入 hello PySpark 核心，您也可以使用 hello[內建的 IPython 魔法](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics)，包括`%%sh`。</span><span class="sxs-lookup"><span data-stu-id="69964-177">In addition toohello magics added by hello PySpark kernel, you can also use hello [built-in IPython magics](https://ipython.org/ipython-doc/3/interactive/magics.html#cell-magics), including `%%sh`.</span></span> <span data-ttu-id="69964-178">您可以使用 hello `%%sh` magic toorun 指令碼和 hello 叢集前端節點上的程式碼區塊。</span><span class="sxs-lookup"><span data-stu-id="69964-178">You can use hello `%%sh` magic toorun scripts and block of code on hello cluster headnode.</span></span>
   >
   >
2. <span data-ttu-id="69964-179">**自動視覺化**。</span><span class="sxs-lookup"><span data-stu-id="69964-179">**Auto visualization**.</span></span> <span data-ttu-id="69964-180">hello **Pyspark**核心自動視覺化 hello 的 Hive 和 SQL 查詢的輸出。</span><span class="sxs-lookup"><span data-stu-id="69964-180">hello **Pyspark** kernel automatically visualizes hello output of Hive and SQL queries.</span></span> <span data-ttu-id="69964-181">有數種不同類型的視覺效果供您選擇，包括資料表、圓形圖、線條、區域、長條圖。</span><span class="sxs-lookup"><span data-stu-id="69964-181">You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.</span></span>

## <a name="parameters-supported-with-hello-sql-magic"></a><span data-ttu-id="69964-182">支援以 hello 參數 %%sql 識別常數</span><span class="sxs-lookup"><span data-stu-id="69964-182">Parameters supported with hello %%sql magic</span></span>
<span data-ttu-id="69964-183">hello `%%sql` magic 支援不同的參數，您可以使用 toocontrol hello 種執行查詢時，您收到的輸出。</span><span class="sxs-lookup"><span data-stu-id="69964-183">hello `%%sql` magic supports different parameters that you can use toocontrol hello kind of output that you receive when you run queries.</span></span> <span data-ttu-id="69964-184">hello 下表列出 hello 輸出。</span><span class="sxs-lookup"><span data-stu-id="69964-184">hello following table lists hello output.</span></span>

| <span data-ttu-id="69964-185">參數</span><span class="sxs-lookup"><span data-stu-id="69964-185">Parameter</span></span> | <span data-ttu-id="69964-186">範例</span><span class="sxs-lookup"><span data-stu-id="69964-186">Example</span></span> | <span data-ttu-id="69964-187">說明</span><span class="sxs-lookup"><span data-stu-id="69964-187">Description</span></span> |
| --- | --- | --- |
| <span data-ttu-id="69964-188">-o</span><span class="sxs-lookup"><span data-stu-id="69964-188">-o</span></span> |`-o <VARIABLE NAME>` |<span data-ttu-id="69964-189">在 hello 中使用此參數 toopersist hello 查詢的結果 hello，%%本機 Python 內容，做為[熊](http://pandas.pydata.org/)資料框架。</span><span class="sxs-lookup"><span data-stu-id="69964-189">Use this parameter toopersist hello result of hello query, in hello %%local Python context, as a [Pandas](http://pandas.pydata.org/) dataframe.</span></span> <span data-ttu-id="69964-190">hello hello 資料框架變數名稱是您指定的 hello 變數名稱。</span><span class="sxs-lookup"><span data-stu-id="69964-190">hello name of hello dataframe variable is hello variable name you specify.</span></span> |
| <span data-ttu-id="69964-191">-q</span><span class="sxs-lookup"><span data-stu-id="69964-191">-q</span></span> |`-q` |<span data-ttu-id="69964-192">使用這個視覺效果關閉 tooturn hello 儲存格。</span><span class="sxs-lookup"><span data-stu-id="69964-192">Use this tooturn off visualizations for hello cell.</span></span> <span data-ttu-id="69964-193">如果您不想 tooauto-視覺化 hello 儲存格內容，且只想 toocapture 其成為資料框架，然後使用`-q -o <VARIABLE>`。</span><span class="sxs-lookup"><span data-stu-id="69964-193">If you don't want tooauto-visualize hello content of a cell and just want toocapture it as a dataframe, then use `-q -o <VARIABLE>`.</span></span> <span data-ttu-id="69964-194">如果您想 tooturn 關閉視覺效果，而不擷取 hello 結果 (例如，針對執行 SQL 查詢，例如`CREATE TABLE`陳述式)，使用`-q`但未指定`-o`引數。</span><span class="sxs-lookup"><span data-stu-id="69964-194">If you want tooturn off visualizations without capturing hello results (for example, for running a SQL query, like a `CREATE TABLE` statement), use `-q` without specifying a `-o` argument.</span></span> |
| <span data-ttu-id="69964-195">-m</span><span class="sxs-lookup"><span data-stu-id="69964-195">-m</span></span> |`-m <METHOD>` |<span data-ttu-id="69964-196">其中 **METHOD** 是 **take** 或 **sample** (預設值是 **take**)。</span><span class="sxs-lookup"><span data-stu-id="69964-196">Where **METHOD** is either **take** or **sample** (default is **take**).</span></span> <span data-ttu-id="69964-197">如果 hello 方法**採取**，hello 核心挑選 hello MAXROWS （稍後在此資料表中所述） 所指定的結果資料集的 hello 頂端的項目。</span><span class="sxs-lookup"><span data-stu-id="69964-197">If hello method is **take**, hello kernel picks elements from hello top of hello result data set specified by MAXROWS (described later in this table).</span></span> <span data-ttu-id="69964-198">如果 hello 方法**範例**，hello 核心會隨機取樣，根據太 hello 資料集的項目`-r`參數，此資料表中的一節所說明。</span><span class="sxs-lookup"><span data-stu-id="69964-198">If hello method is **sample**, hello kernel randomly samples elements of hello data set according too`-r` parameter, described next in this table.</span></span> |
| <span data-ttu-id="69964-199">-r</span><span class="sxs-lookup"><span data-stu-id="69964-199">-r</span></span> |`-r <FRACTION>` |<span data-ttu-id="69964-200">這裡的 **FRACTION** 是介於 0.0 到 1.0 之間的浮點數。</span><span class="sxs-lookup"><span data-stu-id="69964-200">Here **FRACTION** is a floating-point number between 0.0 and 1.0.</span></span> <span data-ttu-id="69964-201">如果 hello SQL 查詢的 hello 範例方法`sample`，然後 hello 核心會隨機取樣 hello 指定的部分設定為您的 hello 結果的 hello 項目。</span><span class="sxs-lookup"><span data-stu-id="69964-201">If hello sample method for hello SQL query is `sample`, then hello kernel randomly samples hello specified fraction of hello elements of hello result set for you.</span></span> <span data-ttu-id="69964-202">例如，如果您以 hello 引數執行 SQL 查詢`-m sample -r 0.01`，則會隨機取樣的 hello 結果資料列的 1%。</span><span class="sxs-lookup"><span data-stu-id="69964-202">For example, if you run a SQL query with hello arguments `-m sample -r 0.01`, then 1% of hello result rows are randomly sampled.</span></span> |
| -n |`-n <MAXROWS>` |<span data-ttu-id="69964-203">**MAXROWS** 是整數值。</span><span class="sxs-lookup"><span data-stu-id="69964-203">**MAXROWS** is an integer value.</span></span> <span data-ttu-id="69964-204">hello 核心限制 hello 輸出資料列數目太**MAXROWS**。</span><span class="sxs-lookup"><span data-stu-id="69964-204">hello kernel limits hello number of output rows too**MAXROWS**.</span></span> <span data-ttu-id="69964-205">如果**MAXROWS**是負數，例如**-1**，hello hello 結果集中的資料列數目沒有限制。</span><span class="sxs-lookup"><span data-stu-id="69964-205">If **MAXROWS** is a negative number such as **-1**, then hello number of rows in hello result set is not limited.</span></span> |

<span data-ttu-id="69964-206">**範例：**</span><span class="sxs-lookup"><span data-stu-id="69964-206">**Example:**</span></span>

    %%sql -q -m sample -r 0.1 -n 500 -o query2
    SELECT * FROM hivesampletable

<span data-ttu-id="69964-207">hello 上面的陳述式沒有 hello 遵循：</span><span class="sxs-lookup"><span data-stu-id="69964-207">hello statement above does hello following:</span></span>

* <span data-ttu-id="69964-208">從 **hivesampletable**選取所有記錄。</span><span class="sxs-lookup"><span data-stu-id="69964-208">Selects all records from **hivesampletable**.</span></span>
* <span data-ttu-id="69964-209">因為我們使用 -q，所以它會關閉自動視覺效果。</span><span class="sxs-lookup"><span data-stu-id="69964-209">Because we use -q, it turns off auto-visualization.</span></span>
* <span data-ttu-id="69964-210">因為我們使用`-m sample -r 0.1 -n 500`其隨機取樣的 10%的 hello hello hivesampletable 中的資料列，並限制 hello hello 結果集 too500 資料列的大小。</span><span class="sxs-lookup"><span data-stu-id="69964-210">Because we use `-m sample -r 0.1 -n 500` it randomly samples 10% of hello rows in hello hivesampletable and limits hello size of hello result set too500 rows.</span></span>
* <span data-ttu-id="69964-211">最後，因為我們使用`-o query2`它也會將 hello 輸出儲存至呼叫的資料框架**query2**。</span><span class="sxs-lookup"><span data-stu-id="69964-211">Finally, because we used `-o query2` it also saves hello output into a dataframe called **query2**.</span></span>

## <a name="considerations-while-using-hello-new-kernels"></a><span data-ttu-id="69964-212">使用 hello 新核心時的考量</span><span class="sxs-lookup"><span data-stu-id="69964-212">Considerations while using hello new kernels</span></span>

<span data-ttu-id="69964-213">您使用，無論核心離開 hello 筆記本執行會耗用 hello 叢集資源。</span><span class="sxs-lookup"><span data-stu-id="69964-213">Whichever kernel you use, leaving hello notebooks running consumes hello cluster resources.</span></span>  <span data-ttu-id="69964-214">這些核心，都有預設 hello 內容，因為只要結束 hello 筆記本並不會終止 hello 內容，因此 hello 叢集資源會繼續 toobe 使用中。</span><span class="sxs-lookup"><span data-stu-id="69964-214">With these kernels, because hello contexts are preset, simply exiting hello notebooks does not kill hello context and hence hello cluster resources continue toobe in use.</span></span> <span data-ttu-id="69964-215">最好的作法是 toouse hello**關閉而停止**選項從 hello 筆記本**檔案**使用 hello 筆記本，會清除 hello 內容完畢後再結束 hello 筆記本功能表。</span><span class="sxs-lookup"><span data-stu-id="69964-215">A good practice is toouse hello **Close and Halt** option from hello notebook's **File** menu when you are finished using hello notebook, which kills hello context and then exits hello notebook.</span></span>     

## <a name="show-me-some-examples"></a><span data-ttu-id="69964-216">請舉例說明</span><span class="sxs-lookup"><span data-stu-id="69964-216">Show me some examples</span></span>

<span data-ttu-id="69964-217">當您開啟 Jupyter 筆記本時，您會看到兩個資料夾可用 hello 根層級。</span><span class="sxs-lookup"><span data-stu-id="69964-217">When you open a Jupyter notebook, you see two folders available at hello root level.</span></span>

* <span data-ttu-id="69964-218">hello **PySpark**資料夾有範例筆記本該使用 hello new **Python**核心。</span><span class="sxs-lookup"><span data-stu-id="69964-218">hello **PySpark** folder has sample notebooks that use hello new **Python** kernel.</span></span>
* <span data-ttu-id="69964-219">hello **Scala**資料夾有範例筆記本該使用 hello new **Spark**核心。</span><span class="sxs-lookup"><span data-stu-id="69964-219">hello **Scala** folder has sample notebooks that use hello new **Spark** kernel.</span></span>

<span data-ttu-id="69964-220">您可以開啟 hello **00-[讀取我第一次] 的 Spark Magic 核心功能**hello 的筆記本**PySpark**或**Spark**資料夾 toolearn 有關 hello 不同魔法可用。</span><span class="sxs-lookup"><span data-stu-id="69964-220">You can open hello **00 - [READ ME FIRST] Spark Magic Kernel Features** notebook from hello **PySpark** or **Spark** folder toolearn about hello different magics available.</span></span> <span data-ttu-id="69964-221">您也可以使用如何 hello hello 兩個資料夾 toolearn 底下可用的其他範例筆記本 tooachieve Jupyter 筆記本使用 HDInsight Spark 叢集不同的案例。</span><span class="sxs-lookup"><span data-stu-id="69964-221">You can also use hello other sample notebooks available under hello two folders toolearn how tooachieve different scenarios using Jupyter notebooks with HDInsight Spark clusters.</span></span>

## <a name="where-are-hello-notebooks-stored"></a><span data-ttu-id="69964-222">Hello 筆記本儲存在哪裡？</span><span class="sxs-lookup"><span data-stu-id="69964-222">Where are hello notebooks stored?</span></span>

<span data-ttu-id="69964-223">Jupyter 筆記本會儲存 toohello 下 hello 的 hello 叢集相關聯的儲存體帳戶**/HdiNotebooks**資料夾。</span><span class="sxs-lookup"><span data-stu-id="69964-223">Jupyter notebooks are saved toohello storage account associated with hello cluster under hello **/HdiNotebooks** folder.</span></span>  <span data-ttu-id="69964-224">筆記本、 文字檔案，與您建立從 Jupyter 內的資料夾是可從 hello 儲存體帳戶存取。</span><span class="sxs-lookup"><span data-stu-id="69964-224">Notebooks, text files, and folders that you create from within Jupyter are accessible from hello storage account.</span></span>  <span data-ttu-id="69964-225">例如，如果您使用 Jupyter toocreate 資料夾**myfolder**和筆記型電腦**myfolder/mynotebook.ipynb**，您可以存取位於該筆記本`/HdiNotebooks/myfolder/mynotebook.ipynb`hello 儲存體帳戶內。</span><span class="sxs-lookup"><span data-stu-id="69964-225">For example, if you use Jupyter toocreate a folder **myfolder** and a notebook **myfolder/mynotebook.ipynb**, you can access that notebook at `/HdiNotebooks/myfolder/mynotebook.ipynb` within hello storage account.</span></span>  <span data-ttu-id="69964-226">hello 反向也是如此，也就是說，如果您上傳筆記本直接 tooyour 儲存體帳戶在`/HdiNotebooks/mynotebook1.ipynb`，也是可見的 Jupyter 筆記本 hello。</span><span class="sxs-lookup"><span data-stu-id="69964-226">hello reverse is also true, that is, if you upload a notebook directly tooyour storage account at `/HdiNotebooks/mynotebook1.ipynb`, hello notebook is visible from Jupyter as well.</span></span>  <span data-ttu-id="69964-227">即使在刪除 hello 叢集之後，才，筆記本會保留在 hello 儲存體帳戶。</span><span class="sxs-lookup"><span data-stu-id="69964-227">Notebooks remain in hello storage account even after hello cluster is deleted.</span></span>

<span data-ttu-id="69964-228">筆記本會儲存 toohello 儲存體帳戶的 hello 方法是與 HDFS 相容。</span><span class="sxs-lookup"><span data-stu-id="69964-228">hello way notebooks are saved toohello storage account is compatible with HDFS.</span></span> <span data-ttu-id="69964-229">因此，如果您到 hello 叢集中，您可以使用 SSH 檔案管理命令 hello 下列程式碼片段所示：</span><span class="sxs-lookup"><span data-stu-id="69964-229">So, if you SSH into hello cluster you can use file management commands as shown in hello following snippet:</span></span>

    hdfs dfs -ls /HdiNotebooks                               # List everything at hello root directory – everything in this directory is visible tooJupyter from hello home page
    hdfs dfs –copyToLocal /HdiNotebooks                    # Download hello contents of hello HdiNotebooks folder
    hdfs dfs –copyFromLocal example.ipynb /HdiNotebooks   # Upload a notebook example.ipynb toohello root folder so it’s visible from Jupyter


<span data-ttu-id="69964-230">萬一有存取 hello hello 叢集的儲存體帳戶的問題，請 hello 筆記本也會儲存在 hello 叢集前端節點上`/var/lib/jupyter`。</span><span class="sxs-lookup"><span data-stu-id="69964-230">In case there are issues accessing hello storage account for hello cluster, hello notebooks are also saved on hello headnode `/var/lib/jupyter`.</span></span>

## <a name="supported-browser"></a><span data-ttu-id="69964-231">支援的瀏覽器</span><span class="sxs-lookup"><span data-stu-id="69964-231">Supported browser</span></span>

<span data-ttu-id="69964-232">Google Chrome 上只支援 Spark HDInsight 叢集上的 Jupyter Notebook。</span><span class="sxs-lookup"><span data-stu-id="69964-232">Jupyter notebooks on Spark HDInsight clusters are supported only on Google Chrome.</span></span>

## <a name="feedback"></a><span data-ttu-id="69964-233">意見反應</span><span class="sxs-lookup"><span data-stu-id="69964-233">Feedback</span></span>
<span data-ttu-id="69964-234">hello 新核心中發展階段，並將成人經過一段時間。</span><span class="sxs-lookup"><span data-stu-id="69964-234">hello new kernels are in evolving stage and will mature over time.</span></span> <span data-ttu-id="69964-235">這或許也意味著，API 可能會隨著這些核心的成熟而改變。</span><span class="sxs-lookup"><span data-stu-id="69964-235">This could also mean that APIs could change as these kernels mature.</span></span> <span data-ttu-id="69964-236">您在使用這些新核心時如有任何意見，我們都非常樂於知道。</span><span class="sxs-lookup"><span data-stu-id="69964-236">We would appreciate any feedback that you have while using these new kernels.</span></span> <span data-ttu-id="69964-237">這是在發展這些核心 hello 最後發行版本中很有用。</span><span class="sxs-lookup"><span data-stu-id="69964-237">This is useful in shaping hello final release of these kernels.</span></span> <span data-ttu-id="69964-238">您可以讓您的註解/的意見反應在 hello**註解**在 hello 這篇文章底部的區段。</span><span class="sxs-lookup"><span data-stu-id="69964-238">You can leave your comments/feedback under hello **Comments** section at hello bottom of this article.</span></span>

## <span data-ttu-id="69964-239"><a name="seealso"></a>另請參閱</span><span class="sxs-lookup"><span data-stu-id="69964-239"><a name="seealso"></a>See also</span></span>
* [<span data-ttu-id="69964-240">概觀：Azure HDInsight 上的 Apache Spark</span><span class="sxs-lookup"><span data-stu-id="69964-240">Overview: Apache Spark on Azure HDInsight</span></span>](hdinsight-apache-spark-overview.md)

### <a name="scenarios"></a><span data-ttu-id="69964-241">案例</span><span class="sxs-lookup"><span data-stu-id="69964-241">Scenarios</span></span>
* [<span data-ttu-id="69964-242">Spark 和 BI：在 HDInsight 中搭配使用 Spark 和 BI 工具執行互動式資料分析</span><span class="sxs-lookup"><span data-stu-id="69964-242">Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools</span></span>](hdinsight-apache-spark-use-bi-tools.md)
* [<span data-ttu-id="69964-243">Spark 和機器學習服務：使用 HDInsight 中的 Spark，利用 HVAC 資料來分析建築物溫度</span><span class="sxs-lookup"><span data-stu-id="69964-243">Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data</span></span>](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [<span data-ttu-id="69964-244">機器學習的 Spark： 使用 HDInsight toopredict 食物檢查結果中的 Spark</span><span class="sxs-lookup"><span data-stu-id="69964-244">Spark with Machine Learning: Use Spark in HDInsight toopredict food inspection results</span></span>](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [<span data-ttu-id="69964-245">Spark 串流：使用 HDInsight 中的 Spark 來建置即時串流應用程式</span><span class="sxs-lookup"><span data-stu-id="69964-245">Spark Streaming: Use Spark in HDInsight for building real-time streaming applications</span></span>](hdinsight-apache-spark-eventhub-streaming.md)
* [<span data-ttu-id="69964-246">使用 HDInsight 中的 Spark 進行網站記錄分析</span><span class="sxs-lookup"><span data-stu-id="69964-246">Website log analysis using Spark in HDInsight</span></span>](hdinsight-apache-spark-custom-library-website-log-analysis.md)

### <a name="create-and-run-applications"></a><span data-ttu-id="69964-247">建立及執行應用程式</span><span class="sxs-lookup"><span data-stu-id="69964-247">Create and run applications</span></span>
* [<span data-ttu-id="69964-248">使用 Scala 建立獨立應用程式</span><span class="sxs-lookup"><span data-stu-id="69964-248">Create a standalone application using Scala</span></span>](hdinsight-apache-spark-create-standalone-application.md)
* [<span data-ttu-id="69964-249">利用 Livy 在 Spark 叢集上遠端執行作業</span><span class="sxs-lookup"><span data-stu-id="69964-249">Run jobs remotely on a Spark cluster using Livy</span></span>](hdinsight-apache-spark-livy-rest-interface.md)

### <a name="tools-and-extensions"></a><span data-ttu-id="69964-250">工具和擴充功能</span><span class="sxs-lookup"><span data-stu-id="69964-250">Tools and extensions</span></span>
* [<span data-ttu-id="69964-251">HDInsight 工具外掛程式用於 IntelliJ 概念 toocreate 並提交 Spark Scala 應用程式</span><span class="sxs-lookup"><span data-stu-id="69964-251">Use HDInsight Tools Plugin for IntelliJ IDEA toocreate and submit Spark Scala applications</span></span>](hdinsight-apache-spark-intellij-tool-plugin.md)
* [<span data-ttu-id="69964-252">從遠端使用 HDInsight Tools 外掛程式 IntelliJ 概念 toodebug Spark 應用程式</span><span class="sxs-lookup"><span data-stu-id="69964-252">Use HDInsight Tools Plugin for IntelliJ IDEA toodebug Spark applications remotely</span></span>](hdinsight-apache-spark-intellij-tool-plugin-debug-jobs-remotely.md)
* [<span data-ttu-id="69964-253">利用 HDInsight 上的 Spark 叢集來使用 Zeppelin Notebook</span><span class="sxs-lookup"><span data-stu-id="69964-253">Use Zeppelin notebooks with a Spark cluster on HDInsight</span></span>](hdinsight-apache-spark-zeppelin-notebook.md)
* [<span data-ttu-id="69964-254">搭配 Jupyter Notebook 使用外部套件</span><span class="sxs-lookup"><span data-stu-id="69964-254">Use external packages with Jupyter notebooks</span></span>](hdinsight-apache-spark-jupyter-notebook-use-external-packages.md)
* [<span data-ttu-id="69964-255">在您的電腦上安裝 Jupyter 並連接 tooan HDInsight Spark 叢集</span><span class="sxs-lookup"><span data-stu-id="69964-255">Install Jupyter on your computer and connect tooan HDInsight Spark cluster</span></span>](hdinsight-apache-spark-jupyter-notebook-install-locally.md)

### <a name="manage-resources"></a><span data-ttu-id="69964-256">管理資源</span><span class="sxs-lookup"><span data-stu-id="69964-256">Manage resources</span></span>
* [<span data-ttu-id="69964-257">管理 Azure HDInsight 中的 hello Apache Spark 叢集的資源</span><span class="sxs-lookup"><span data-stu-id="69964-257">Manage resources for hello Apache Spark cluster in Azure HDInsight</span></span>](hdinsight-apache-spark-resource-manager.md)
* [<span data-ttu-id="69964-258">追蹤和偵錯在 HDInsight 中的 Apache Spark 叢集上執行的作業</span><span class="sxs-lookup"><span data-stu-id="69964-258">Track and debug jobs running on an Apache Spark cluster in HDInsight</span></span>](hdinsight-apache-spark-job-debugging.md)
